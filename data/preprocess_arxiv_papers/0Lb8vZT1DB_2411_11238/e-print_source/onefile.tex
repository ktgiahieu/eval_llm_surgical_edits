\documentclass[11pt]{article}



\usepackage[letterpaper,margin=1in]{geometry}

\oddsidemargin=-0.1in \evensidemargin=-0.1in \topmargin=-.5in
\textheight=9in \textwidth=6.5in
\parindent=18pt

\usepackage{amsthm,amsmath,amssymb,amsfonts,amssymb,mathtools}
\usepackage{color}
\usepackage{xcolor}
\usepackage{empheq}
\usepackage{dsfont}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{subfig}
\usepackage[T1]{fontenc}
% \usepackage[protrusion=true,expansion=true]{microtype}  

%%uncomment for speedup compile
%\pdfcompresslevel=0
%\pdfobjcompresslevel=0

%%%%%%%%tikz stuff
\usepackage{tikz}
\usetikzlibrary{patterns}
\usetikzlibrary{arrows,shapes,automata,backgrounds,petri,positioning}
\usetikzlibrary{shadows}
\usetikzlibrary{calc}
\usetikzlibrary{spy}
\usepackage{pgf,pgfplots}
\usetikzlibrary{angles, quotes}
\usepackage{pgfmath,pgffor}

%
\usepackage{framed}
%Other
\usepackage{multirow}
\usepackage{algorithm2e}
\usepackage[noend]{algpseudocode}
\SetAlCapSkip{1em}
%alg-caption-setup
% \SetAlCapFnt{\normalfont\small}% Set caption font
% \SetAlCapNameFnt{\unskip\itshape\small}% Set caption name font



\usepackage[colorlinks,citecolor=blue,linkcolor=magenta,bookmarks=true]{hyperref}

\usepackage[nameinlink]{cleveref}
\crefname{ineq}{Inequality}{Inequality}
\creflabelformat{ineq}{#2{\upshape(#1)}#3}
\crefname{sub}{Subsection}{Subsection}
\creflabelformat{Subsection}{#2{\upshape(#1)}#3}
\crefname{sdp}{SDP}{SDP}
\creflabelformat{sdp}{#2{\upshape(#1)}#3}
\crefname{lp}{LP}{LP}
\creflabelformat{lp}{#2{\upshape(#1)}#3}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%MACROS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%  Theorems
\newtheorem{theorem}{Theorem}[section]
\newtheorem{question}[theorem]{Question}
\newtheorem{conj}{Conjecture}
\newtheorem{cond}[theorem]{Condition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{informal theorem}[theorem]{Theorem (informal statement)}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{obs}[theorem]{Observation}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{infproposition}[theorem]{(Informal) Proposition}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{definition}[theorem]{Definition}
\newcommand{\eqdef}{\stackrel{{\mathrm {\footnotesize def}}}{=}}
% \newcommand{\eqdef}{\coloneqq}
\crefname{question}{question}{questions}
%\def\colorful{1}
\def\colorful{0}

\ifnum\colorful=1
\newcommand{\new}[1]{{\color{red} #1}}
\newcommand{\green}[1]{{\color{green} #1}}
\newcommand{\yellow}[1]{{\color{yellow} #1}}
\newcommand{\newblue}[1]{{\color{blue} #1}}
\else
\newcommand{\new}[1]{{#1}}
\newcommand{\blue}[1]{{#1}}
\newcommand{\newblue}[1]{{#1}}
\newcommand{\yellow}[1]{{#1}}
\newcommand{\green}[1]{{#1}}
\fi


\newcommand{\wperp}{(\vec w^*)^{\perp_\vec w}}
\newcommand{\lp}{\left}
\newcommand{\rp}{\right}
\newcommand\norm[1]{\left\| #1 \right\|}
\newcommand\snorm[2]{\left\| #2 \right\|_{#1}}
\renewcommand\vec[1]{\mathbf{#1}}
\DeclareMathOperator*{\pr}{\mathbf{Pr}}
\DeclareMathOperator*{\E}{\mathbf{E}}
\newcommand{\proj}{\mathrm{proj}}
\newcommand{\me}{\mathrm{e}}
\newcommand{\mrm}{\mathrm}
\def\d{\mathrm{d}}
\newcommand{\normal}{\mathcal{N}}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\influ}{Inf}
\DeclareMathOperator*{\he}{He}



\newcommand{\sample}[2]{#1^{(#2)}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\Sp}{\mathbb{S}}
\newcommand{\bH}{\mathbf{H}}


\newcommand{\err}{\mathrm{err}}
\newcommand{\LTF}{\mathcal{H}}
\newcommand{\B}{\mathbb{B}}



\newcommand{\p}{\mathbf{P}}
\newcommand{\q}{\mathbf{Q}}
\newcommand{\h}{\mathbf{H}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\s}{\mathbb{S}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\eps}{\epsilon}
\newcommand{\dtv}{d_{\mathrm TV}}
\newcommand{\poly}{\mathrm{poly}}
\newcommand{\polylog}{\mathrm{polylog}}
\newcommand{\var}{\mathbf{Var}}
\newcommand{\cov}{\mathbf{Cov}}


\newcommand{\sgn}{\mathrm{sign}}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\calN}{{\cal N}}
\newcommand{\calL}{{\cal L}}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\Ex}{\mathop{{\bf E}\/}}
\newcommand{\opt}{\mathrm{OPT}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\LR}{\mathrm{LeakyRelu}}
\newcommand{\Ind}{\mathds{1}}
\newcommand{\1}{\Ind}
\newcommand{\matr}{\vec}
\newcommand{\littleint}{\mathop{\textstyle \int}}
\newcommand{\littlesum}{\mathop{\textstyle \sum}}
\newcommand{\littleprod}{\mathop{\textstyle \prod}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bi}{\mathbf{i}}
\newcommand{\bj}{\mathbf{j}}
\newcommand{\tp}{\tilde p}
\newcommand{\gaus}{\mathcal{N}}
\newcommand{\ind}{\mathbf{1}}
\newcommand{\nul}{\mathrm{null}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\wt}{\widetilde}
\newcommand{\wh}{\widehat}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\tD}{\tilde D}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\ltf}{\mathrm{LTF}}

\newcommand{\ltwo}[1]{\left\lVert#1\right\rVert_2}
\newcommand{\miscl}{\err_{0-1}^{\D}}
\newcommand{\wstar}{\bw^{\ast}}
\newcommand{\x}{\vec x}
\DeclareMathOperator\erf{erf}




\usepackage{enumitem}



\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors




\title{Reliable Learning of Halfspaces under Gaussian Marginals}


\author{
Ilias Diakonikolas\thanks{Supported in part by NSF Medium Award CCF-2107079 and an H.I. Romnes Faculty Fellowship.
}\\
University of Wisconsin-Madison\\
{\tt ilias@cs.wisc.edu}\\
\and
Lisheng Ren\thanks{Supported in part by NSF Medium Award CCF-2107079.}\\
University of Wisconsin-Madison\\
{\tt lren29@wisc.edu}\\
\and
Nikos Zarifis\thanks{Supported in part by NSF Medium Award CCF-2107079.}\\
University of Wisconsin-Madison\\
{\tt zarifis@wisc.edu}\\
}
\begin{document}


\maketitle


\begin{abstract}
We study the problem of PAC learning halfspaces in the 
reliable agnostic model of Kalai et al. (2012).
The reliable PAC model  
captures learning scenarios where one type of error is 
costlier than the others. Our main positive result is a 
new algorithm for reliable learning 
of Gaussian halfspaces on 
$\R^d$ with sample and computational complexity 
$$d^{O(\log (\min\{1/\alpha, 1/\eps\}))}\min (2^{\log(1/\eps)^{O(\log (1/\alpha))}},2^{\poly(1/\eps)})\;,$$ 
where $\eps$ is the excess error and $\alpha$ 
is the bias of the optimal halfspace. We complement our upper bound with 
a Statistical Query lower bound 
suggesting that the $d^{\Omega(\log (1/\alpha))}$ dependence is best possible. 
Conceptually, our results imply a strong computational separation 
between reliable agnostic learning and standard agnostic 
learning of halfspaces in the Gaussian setting.
\end{abstract}

\setcounter{page}{0}
\thispagestyle{empty}
\newpage


\section{Introduction} \label{sec:intro}

Halfspaces (or Linear Threshold Functions) is the class of functions 
$f:\R^d\to\{\pm 1\}$ of the form $f(\bx)=\sign(\langle\bw,\bx\rangle-t)$, where $\bw\in \R^d$ is called the weight vector and $t$ is called the threshold.
The problem of learning halfspaces is one of the classical and most well-studied
problems in machine learning---going back to the Perceptron algorithm \cite{Ros58}---and has had great impact on many other 
influential techniques, including SVMs \cite{Vap98} and AdaBoost \cite{FS97}. 

Here we focus on learning halfspaces from random labeled
examples. The computational complexity of this task crucially depends on the choice
of the underlying model. For example, in the realizable PAC model (i.e., with clean labels), 
the problem is known to be efficiently solvable (see, e.g.,~\cite{MT:94}) 
via a reduction to linear programming. Unfortunately, this method is quite fragile 
and breaks down in the presence of noisy labels. 
In the noisy setting, the computational complexity of the problem depends on the choice of 
noise model and distributional assumptions. In this work, we study the problem of distribution-specific PAC learning of halfspaces,  with respect to Gaussian marginals, in the reliable agnostic model of~\cite{KKM2012}.  
Formally, we have the following definition.


\begin{definition}[(Positive) Reliable Learning of Gaussian Halfspaces]
\label{def:reliable-learning}
Let \new{$\mathcal{H}_d$} be the class of halfspaces on $\R^d$.
Given $0<\eps<1$ and i.i.d.\ samples $(\bx,y)$ from a distribution $D$ 
supported on $\R^d\times\{\pm 1\}$,  
where the marginal $D_{\bx}$ is the standard Gaussian $\gaus({\bf 0},\bI)$,
we say that an algorithm reliably learns \new{$\mathcal{H}_d$} 
to error $\eps$ if the algorithm with high probability 
outputs a hypothesis $h:\R^d\to \{\pm 1\}$ such that 
$\pr_{(\x,y)\sim D}[h(\x)= 1\land y= -1]\leq \eps$ and 
$\pr_{(\x,y)\sim D}[h(\x)= -1\land y=1]\leq \opt+\eps$,
where 
$
\opt\eqdef 
\min_{f\in \mathcal{G}(D)} \pr_{(\x,y)\sim D}[f(\x)= -1\new{\land} y= \new{1}]$, 
with \[ \mathcal{G}(D)=\{f\in \new{\mathcal{H}_d}: 
\pr_{(\x,y)\sim D}[f(\x)= 1\new{\land} y= \new{-1}]=0\}\cup \{f(\bx)=\new{-1}\} \;.\]
We say that $f=\argmin_{f\in \mathcal{G}(D)} \pr_{(\x,y)\sim D}[f(\x)= -1\new{\land} y= \new{1}]$ is the optimal halfspace on distribution $D$
and
for the conditions above that hypothesis $h$ satisfies, we say
that $h$ \new{is} $\eps$-reliable with respect to \new{$\mathcal{H}_d$} 
on distribution $D$.
\end{definition}

In other words, a (positive) reliable learner makes almost 
no false positive errors, while also maintaining the best 
possible false negative error---as compared to any hypothesis 
with no false positive errors.
Note that if there is no hypothesis (in the target class) 
that makes no false positive errors, 
then essentially we have no requirement for the false negative 
error of the returned hypothesis.
The algorithm can then simply return the $-1$ constant 
hypothesis.


The reliable agnostic PAC model was introduced by~\cite{KKM2012} 
as a one-sided analogue
of the classical agnostic model~\cite{Haussler:92, KSS:94}, and 
has since been extensively
studied in a range of works; see, e.g.,~\cite{KKM2012,KT14, GKKT17, DJ19, GKKM20, KK21}. 
The underlying motivation for this definition comes from learning situations 
where mistakes of one type (e.g., false positive) should be avoided at all costs. 
Typical examples include spam detection---where incorrectly detecting spam emails 
is much less costly than mislabeling an important email 
as spam---and detecting network failures---where 
false negatives may be particularly harmful. 
Such scenarios motivate 
the study of reliable learning, which can be viewed as minimizing a loss function 
for different costs for false positive and false negative errors (see, e.g., \cite{Dom99, elk01}). In a historical context, reliable learning is related 
to the Neyman-Pearson criterion~\cite{NP33} in hypothesis testing, which 
shows that the optimal strategy to minimize one type of error 
subject to another type of error being bounded is to threshold 
the likelihood ratio function. More recently, reliable learning 
has been shown~\cite{KK21} to be equivalent to the PQ-learning 
model~\cite{GKKM20}---a recently introduced learning model 
motivated by covariance shift.




The algorithmic task of agnostic reliable learning can be quite challenging. 
While reliable learning can be viewed as minimizing a loss function with different cost for false positive and false negative error, in general, such a loss function will result in a non-convex optimization problem. As pointed out in~\cite{KKM2012}, 
distribution-specific reliable learning can be efficiently reduced to distribution-specific agnostic learning (such reduction preserves the marginal distribution). 
\new{A natural question, serving as a motivation for this work, is whether reliable learning is qualitatively easier computationally---for natural concept classes and halfspaces in particular.}

Before we state our contributions, we briefly summarize 
prior work on agnostically learning Gaussian halfspaces. 
Recall that, in agnostic learning, the goal is to output a hypothesis with error $\opt+\eps$, where $\opt$ 
is the optimal 0-1 error within the target class. 
Prior work has essentially characterized the computational complexity of agnostically learning Gaussian halfspaces. Specifically, it is known that the $L_1$ polynomial regression algorithm of~\cite{KKMS:08}
is an agnostic learner with complexity $d^{O(1/\eps^2)}$~\cite{DGJ+09, DKN10}. Moreover, there is strong evidence that this complexity upper bound is tight, both in the Statistical Query (SQ) model~\cite{DKZ20, GGK20, DKPZ21} and under plausible 
cryptographic assumptions~\cite{Diakonikolas2023, Tiegel2022}. 
\new{It is worth noting that the aforementioned hardness results hold even for the subclass of homogeneous halfspaces.}



\new{Given the aforementioned reduction of reliable learning to agnostic learning~\cite{KKM2012}, one can use $L_1$-regression as a reliable agnostic learner for Gaussian halfspaces, leading to complexity $d^{O(1/\eps^2)}$. Prior to this work, this was the best (and only known) reliable halfspace learner. Given the fundamental nature of this problem, it is natural to ask if one 
can do better for reliable learning.
\begin{center}
{\em Is it possible to develop faster algorithms for reliably learning Gaussian halfspaces \\ 
compared to agnostic learning?}
\end{center}
}
\noindent In this work, we provide an affirmative answer to this question.


To formally state our main result, we need the notion of the bias of a Boolean function.
\begin{definition}[Bias of Boolean Function] \label{def:bias} 
We define the bias $\alpha \in [0, 1/2]$ of 
$h:\R^d\to \{\pm 1\}$ under the Gaussian distribution 
as $\alpha = \alpha(h) \eqdef \min(\pr_{\bx\sim \gaus_d}[h(\bx)=1],\pr_{\bx\sim \gaus_d}[h(\bx)=-1])$.
\end{definition}



The main result of this paper is encapsulated in the following theorem:
\begin{theorem} [Main Result]
\label{thm:algor-and-lb}
    Let $D$ be a joint distribution of $(\bx,y)$ 
    supported on $\R^d\times \{ \pm 1\}$ with marginal $D_{\bx}=\gaus_d$
    and let $\alpha$ be the bias of the optimal halfspace on distribution $D$ (with respect to \Cref{def:reliable-learning}).
    There is an algorithm that 
    uses $N=d^{O(\log (\min\{1/\alpha,1/\eps\}))}\min\left (2^{\log(1/\eps)^{O(\log (1/\alpha))}},2^{\poly(1/\eps)}\right )$ many samples 
    from $D$, runs in
    $\poly(N,d)$ time, and with high probability returns a hypothesis 
    $h(\bx)=\sign(\langle \bw,\bx\rangle-t)$ that is $\eps$-reliable with respect to 
    $\mathcal{H}_d$. Moreover, for $\eps<\alpha/2$, 
    any SQ algorithm for the problem requires complexity  
    $d^{\Omega(\log(1/\alpha))}$.
\end{theorem}
For more detailed theorem statements of our upper and lower bounds, 
see \Cref{app:alg} for the algorithmic result and \Cref{app:lb} for the SQ hardness result. 


\Cref{thm:algor-and-lb} gives a significantly more efficient
algorithm (in terms of both sample complexity and computational complexity) 
for reliably learning Gaussian halfspaces, compared to agnostic learning.
Specifically, as long as $\alpha>0$ is a universal constant, the overall complexity is 
polynomial in $d$ and quasi-polynomial in $1/\eps$---as opposed to $d^{\poly(1/\eps)}$ 
in the agnostic setting. While the complexity of our algorithm 
(namely, the multiplicative factor that is independent of $d$) 
increases  as the optimal halfspace becomes more biased, 
it is always bounded above by the complexity of agnostic learning. 


Finally, we note that our algorithm also applies to the fully reliable learning model~\cite{KKM2012}, 
since one can easily reduce the fully reliable learning to positive reliable  
and negative reliable learning (as observed in~\cite{KKM2012}).



\subsection{Overview of Techniques} \label{ssec:techniques}
Instead of directly considering the optimal halfspace for a 
distribution $D$ (as defined in \Cref{def:reliable-learning}), 
we introduce the following definition as a relaxation.
\begin{definition}[Reliability Condition] \label{def:rel-cond}
    We say that a distribution $D$ supported on $X\times \{\pm 1\}$ satisfies the reliability condition with respect to $f:X\mapsto \{\pm 1\}$ if 
    $\pr_{(\bx,y)\sim D}[f(\bx)=+1 \land y=-1]=0\; .$
\end{definition}

Notice that $h$ being $\eps$-reliable on distribution $D$ is equivalent to $h$
having better false negative error compared with any $f$ such that $D$ satisfies 
the reliability condition with respect to $f$ 
(instead of compared with the optimal halfspace).
At the same time, given any fixed $f$, such a definition allows the adversary to 
arbitrarily corrupt negative labels, thus allowing for more manageable analysis.

\medskip

We start with a brief overview of our SQ lower bound construction,  
followed by the high-level idea enabling our nearly matching 
algorithm. 

\paragraph{SQ Lower Bound}
As follows from \Cref{def:rel-cond} and the subsequent 
discussion, the adversary in reliable learning can arbitrarily 
corrupt the negative labels. We leverage this idea to construct an instance 
where the corruption level suffices to match many moments, 
meaning that labels $y$ are uncorrelated with any polynomial of degree 
at most $\log(1/\alpha)$. To prove this, we construct an (infinite) 
feasibility Linear Program (LP) with the following properties: if the LP is 
feasible, there exists an instance for which no low-degree polynomial correlates 
\new{with the labels}; see~\Cref{lem:polynomial}. To show the existence of such an instance, 
we leverage a technique from \cite{DKPZ21} that exploits (an infinite generalization of) 
LP duality. Specifically, we show that it is possible to add noise 
to the labels whose \new{uncorrupted value is negative}
so that all low-degree moments are uncorrelated with the observed labels $y$.  
This implies that no algorithm that relies on low-degree polynomials
can succeed for reliable learning. This allows us to show that 
SQ algorithms fail if they do not use queries 
with tolerance at most $1/d^{\Omega(\log(1/\alpha))}$ 
or exponentially many queries.

\paragraph{Reliable Learning Algorithm}
As discussed in the previous paragraph, 
an adversary can arbitrarily corrupt the negative labels 
which can make various algorithmic approaches fail. 
In more detail, the adversary can corrupt $\Omega(\log(1/\alpha))$ moments; that is, 
any SQ algorithm needs to rely 
on higher-order moment information. 
One of the main difficulties of this setting is that 
there may exist weight vectors $\vec w, \vec w'$ 
with $\|\vec w-\vec w'\|_2\geq \Omega(1)$ 
while at the same time the 0-1 error of $\vec w$ and $\vec w'$ 
is nearly the same. 
This might suggest that no approach can verify 
that the algorithm decreases the angle between 
the current weight vector and the optimal vector. 
To overcome this obstacle, we develop an algorithm that performs
a random walk over a ``low-dimensional'' subspace. To establish the correctness of our algorithm, we prove 
that at some point during its execution, 
the algorithm will find a weight vector 
sufficiently close to the optimal vector.

To show that such a low-dimensional subspace exists, 
we proceed as follows:
We first prove 
that there exists a non-trivial polynomial $p$ 
of degree $O(\log(1/\alpha))$ 
that correlates 
with the negative labels (by the term nontrivial, we are referring 
to the requirement that 
we cannot use the constant polynomial, as this trivially correlates with the labels); 
see \Cref{clm:sgn-matching-polynomial}. 
This implies that the low-degree \new{moment} tensors, 
i.e., $\E_{(\x,y)\sim \D}[\1\{y=-1\}\x^{\otimes k}]$ 
for some $k$ between $1$ and $O(\log(1/\alpha))$, 
correlate non-trivially with $(\wstar)^{\otimes k}$, 
where $\wstar$ is the target weight vector. 
Thus, we can use these moment tensors to construct a low-dimensional subspace. 
We leverage the structure of the problem, namely 
that the (observed) negative labels are uncorrupted, 
to show that the correlation is in fact almost constant. 
As a consequence, this allows us to construct a subspace 
whose dimension depends only on the degree of the polynomial $p$ 
(which is $O(\log(1/\alpha))$) 
and the desired excess error $\eps$.

We then proceed as follows: 
in each round, as long as the current solution $\vec w$ is not optimal, 
i.e., there are negative samples on the region $\{\x\in \R^d:\vec w\cdot\x -t\geq 0\}$, 
our algorithm conditions on a thin strip, 
projects the points $\x$ on the orthogonal complement of $\vec w$, 
and reapplies the above structure result. 
This leads (with constant probability) 
to an increase in the correlation between $\vec w$ and $\wstar$. 
Assuming that the correlation is increased by $\beta$ in each round 
with probability at least $1/3$, we roughly need at most $1/\beta$ successful updates. 
Therefore, if we run our algorithm for $3^{1/\beta}$ steps, 
it is guaranteed that with probability at least $1/3$
we will find a vector almost parallel to $\vec w^{\ast}$.


\paragraph{Prior Algorithmic Techniques}
Roughly speaking, prior techniques for reliable learning 
relied on some variant of $L_1$-regression. 
In that sense, our algorithmic approach departs from a direct 
polynomial approximation. Concretely, for distribution-free reliable learning, 
the algorithm from \cite{KT14} uses a one-sided variant of $L_1$ regression---where instead of approximating 
the target function in $L_1$ norm, 
they use the hinge loss instead.
For distribution-specific (and Gaussian in particular) reliable 
learning of halfspaces, the only previous approach  uses 
the reduction of \cite{KKM2012} to 
agnostic learning.
While our algorithm also leverages polynomial approximation, 
our approach employs significantly different ideas and 
we believe it provides a novel perspective for this problem.


\paragraph{Technical Comparison with Prior Work}
Our algorithm shares similarities with the algorithm of \cite{Diakonikolas2022b} for learning Gaussian halfspaces with Massart noise (a weaker semi-random noise model).  
Both algorithms perform a random walk in order to converge to the target halfpace. 
Having said so, our algorithm is fundamentally different than theirs for the following reasons. 
The algorithm of \cite{Diakonikolas2022b} partitions $\R^d$ into sufficiently 
angles and searches in each one of them for a direction 
to update the current hypothesis at random. 
Our algorithm instead conditions on $y=-1$, 
which are the points that are uncorrupted, 
and uses the guarantee (that we establish) that the first 
$\Omega(\log(1/\alpha))$ 
moments of the distribution (conditioned on $y=-1$) 
correlate with the unknown optimal hypothesis. 
This leads to an algorithm with significantly faster runtime. 

Our SQ lower bound leverages techniques from \cite{DKPZ21}. 
Essentially, we formulate a linear program to construct a noise function that 
makes all low-degree polynomials uncorrelated 
with the labels $y$. 
This condition suffices to show that no SQ algorithm can solve the problem 
without relying on higher moments. To prove the existence of such a noise function, 
we use (a generalization of) LP duality of an infinite LP, 
which provides the necessary conditions for the existence of such a function. 

\subsection{Related Work}
This work is part of the broader research program of characterizing 
the efficient learnability of natural concept classes with 
respect to challenging noise models. The complexity of this general 
learning task depends on the underlying class, the 
distributional assumptions and the choice of noise model. A long line of prior 
work has made substantial progress in this direction for the 
class of halfspaces under Gaussians (and other natural 
distributions), and for both semi-random~\cite{AwasthiBHU15, DKTZ20,ZSA20, DKTZ20b, DKKTZ20,DKKTZ21b, Diakonikolas2022b} 
and adversarial noise~\cite{KKMS:08, KOS:08, KLS:09jmlr, ABL17, DKS18a,DKTZ20c,DKKTZ21,DKTZ22}.
An arguably surprising conceptual implication of our results is 
that the complexity of reliable learning for Gaussian halfspaces 
is qualitatively closer to the semi-random case. 


\subsection{Preliminaries} \label{sec:prelims} 

We use lowercase boldface letters for vectors and capitalized boldface letters for matrices and tensors.
We use $\langle \bx,\by \rangle $ for the inner product between $\bx,\by\in \R^d$.
For $\bx,\bs\in \R^d$,
we use $\proj_\bs(\bx)\eqdef\frac{\langle\bx,\bs\rangle\bs}{\|\bs\|^2_2}$ for the projection of $\bx$ on 
the $\bs$ direction. Similarly, 
we use $\proj_{\perp\bs}(\bx)=\bx-\proj_\bs(\bx)$ for the projection of $\bx$ on 
the orthogonal complement of $\bs$.
Additionally, 
let $\bx^{V}\in \R^{\dim(V)}$ be the projection of $\bx$ on the subspace $V$ and reparameterized
on $\R^{\dim(V)}$. 
More precisely, let $\bB_{V}\in \R^{d\times \dim(V)}$ be the matrix whose 
columns form an (arbitrary) orthonormal
basis for the subspace $V$, and let $\bx^{V}\eqdef(\bB_{V})^\intercal \bx$.
For $p \geq 1$ and $\bx\in \R^d$, 
we use $\|\bx\|_p\eqdef\left (\sum_{i=1}^n |\bx_i|^p\right )^{1/p}$ 
to denote the $\ell_p$-norm of $\bx$.
For a matrix or tensor $\bT$, we denote by $\|\bT\|_F$ the Frobenius norm of $\bT$.



We use $\gaus_d$ to denote the standard normal distribution $\gaus(\mathbf{0},\bI)$,
where $\mathbf{0}$ is the $d$-dimensional zero vector and $\bI$ is the $d\times d$ identity matrix.
We use $\bx \sim D$ to denote a random variable with distribution $D$.
For a random variable $\bx$ (resp. a distribution $D$), we use $P_\bx$
(resp. $P_D$) to denote the probability density function or probability 
mass function of the random variable $\bx$ 
(resp. distribution $D$).
We also use $\Phi:\R\mapsto [0,1]$ to denote the cdf function of $\gaus_1$.
We use $\ind$ to denote the indicator function. 



For a boolean function $h:\R^d\to \{\pm 1\}$ and a distribution $D$ supported on $\R^d \times \{\pm 1\}$,
we use $R_+(h;D)\eqdef \pr_{(\bx,y)\sim D}[h(\bx)=1\land y\neq 1]$ 
(resp. $R_-(h;D)\eqdef \pr_{(\bx,y)\sim D}[h(\bx)=-1\land y\neq -1]$)
to denote the false positive (resp. false negative) 0-1 error.
























\section{Algorithm for Reliably Learning Gaussian Halfspaces} \label{sec:alg}

In this section, we describe and analyze our algorithm establishing \Cref{thm:algor-and-lb}.
Due to space limitations, some proofs have been deferred to \Cref{app:alg}. 
For convenience, we will assume that $\alpha$ (the bias of the optimal halfspace) 
is known to the algorithm and that the excess error satisfies $\eps\leq \alpha/3$.
These assumptions are without loss of generality for the following reasons:
First, one can efficiently reduce the unknown $\alpha$ case to the case that $\alpha$ is known, 
by guessing the value of $\alpha$. Second, if $\eps$ \new{is large}, 
there is a straightforward algorithm for the problem 
(simply output the best constant hypothesis). 

\medskip

\noindent {\bf Notation:} We use the notation $\mathcal{H}_d^\alpha$ for 
the set of all LTFs on $\R^d$ whose bias is equal to $\alpha$.
Given the above assumptions, it suffices for us to give a reliable learning algorithm 
for $\mathcal{H}_d^\alpha$ instead of $\mathcal{H}_d$.

\medskip

The high-level idea of the algorithm is as follows.
Without loss of generality, we assume that there exists 
an $\alpha$-biased halfspace that correctly classifies 
all the points with label $y=-1$---since otherwise 
the algorithm can just return the hypothesis $h(\x) \equiv -1$.

Let $f(\bx)=\sign(\langle \bw^*,\bx\rangle -t^*)$ be the optimal halfspace 
and let $\bw$ be our current guess of $\bw^*$.
Assuming that $\bw^*$ is not sufficiently close to $\bw$ 
and the hypothesis that classifies all the points as negative is not optimal, 
we show that there exists a \new{low-degree} polynomial of the form $p(\langle \bw,\bx\rangle)$ 
with correlation at least $2^{-O({t^*}^2)}\eps$ with the negative labels.
By leveraging this structural result, we use a spectral algorithm 
to find a direction $\bv$ that is non-trivially correlated with 
$\proj_{\perp \bw} (\bw^*)$ with at least some constant probability.

Unfortunately, it is not easy to verify whether 
$\langle \bv,\proj_{\perp \bw} (\bw^*)\rangle$ is non-trivial.
However, conditioned on the algorithm always getting a $\bv$ that has good correlation, 
we show that it only takes at most $\log(1/\eps)^{O({t^*}^2)}$ steps 
to get sufficiently close to $\bw^*$.
Therefore, repeating this process $2^{\log(1/\eps)^{O({t^*}^2)}}$ 
many times will eventually find an accurate approximation to $\bw^*$.






The structure of this section is as follows:
In \Cref{ssec:good-direction}, we give our algorithm for finding 
a good direction. \Cref{ssec:rw} describes and analyzes 
our random walk procedure. 

\subsection{Finding a Non-Trivial Direction} \label{ssec:good-direction}

Here we present an algorithm that finds a direction that correlates non-trivially 
with the unknown optimal vector. We first show that there exists a zero-mean 
\new{$O(\log(1/\alpha))$-degree}
polynomial 
that sign-matches the optimal hypothesis. Furthermore, using the fact that the 
optimal hypothesis always correctly guesses the sign of the negative points, this 
gives us that the polynomial correlates with the negative (clean) points. 
Using this intuition, our algorithm estimates the first $O(\log(1/\alpha))$ moments 
of the distribution $D_\x$ conditioned on $y=-1$.
This guarantees that at least one moment correlates 
with the optimal hypothesis, 
as a linear combination of the moments generates the sign-matching polynomial. 
Then, by taking a random vector that lies in the high-influence directions 
\new{(which form a low-dimensional subspace)}, we 
guarantee that with constant probability, this vector correlates well 
with the unknown optimal vector.
The main result of the section is the following.


\begin{proposition} \label{lem:finding-nontrivial-direction}
    Let $D$ be a joint distribution of $(\bx,y)$ supported on $\R^d\times \{ \pm 1\}$ with 
    marginal $D_{\bx}=\gaus_d$ and $\eps\in (0,1)$.
    Suppose $D$ satisfies the reliability condition with respect to 
    $f(\bx)=\sign(\langle \bw^*,\bx\rangle-t^*)$ with $t^*=O\left (\sqrt{\log(1/\eps)}\right )$ and 
    $\pr_{(\bx,y)\sim D} [y=-1]\geq \eps$. 
    Then there is an algorithm that draws $N=d^{O({t^*}^2)}/\eps^2$ samples, 
    has \new{$\poly(N)$} runtime, and with probability at least $\Omega(1)$ 
    returns a unit vector $\bv$ such that 
    $\langle \bv,\bw^*\rangle\geq\max(\log(1/\eps)^{-O({t^*}^2)},\eps^{O(1)})$.
\end{proposition}
    

We start by showing that for any distribution satisfying the reliability condition 
with respect to some $f(\bx)=\sign(\langle\bw^*,\bx\rangle-t^*)$, 
there exists a degree-$O({t^*}^2)$ zero-mean polynomial 
of the form $p(\langle\bw^*,\bx\rangle)$ that has correlation 
at least $2^{-O({t^*}^2)}\pr_{(\bx,y)\sim D}[y=-1]$ with the labels.

\begin{lemma} [Correlation with an Orthonormal Polynomial]\label{lem:polynomial-correlation}
Let $D$ be a joint distribution of $(\bx,y)$ supported on $\R^d\times \{ \pm 1\}$ with 
marginal $D_{\bx}=\gaus_d$.
Suppose $D$ satisfies the reliability condition with respect to 
$f(\bx)=\sign(\langle \bw^*,\bx\rangle-t^*)$. 
Then there exists a polynomial $p:\R\mapsto \R$ of degree at most 
$k=O({t^*}^2+1)$ such that 
$\E_{z\sim {\gaus_1}}[p(z)]=0$, $\E_{z\sim {\gaus_1}}[p^2(z)]=1$ 
and 
$\E_{(\bx,y)\sim D}[y \, p(\langle\bw^*,\bx\rangle)]=2^{-O({t^*}^2)}\pr_{(\bx,y)\sim D}[y=-1]$.
\end{lemma}
\new{
\begin{proof} [Proof Sketch of \Cref{lem:polynomial-correlation}]
Note that since $p$ is a zero-mean polynomial with respect to $D_\x$, 
we have that
$\E_{(\bx,y)\sim D}[y\, p(\langle\bw^*,\bx\rangle)]=-2\E_{(\bx,y)\sim D}[\ind(y=-1)\, p(\langle\bw^*,\bx\rangle)]\; .$
Then, using the fact that the distribution $D$ satisfies the reliability condition,
the statement boils down to showing that for any $\eps$-mass 
inside the interval $[t^*,\infty]$,
the expectation of $p$ on this $\epsilon$ mass is at least $2^{-O({t^*}^2)}$.
To prove this statement, it suffices to construct a polynomial $p$ 
that is non-negative on $[t^*,\infty]$ and such that the $\eps/2$-tail of $p$ 
in that interval is at least $2^{-O({t^*}^2)}$.
To achieve this, we show that the sign-matching polynomial used 
in \cite{Diakonikolas2022b} meets our purpose.
\end{proof}
}

Our algorithm uses the following normalized Hermite tensor.

\begin{definition}[Hermite Tensor]\label{def:Hermite-tensor}
For $k\in \N$ and $\bx\in\R^d$, we define the $k$-th Hermite tensor as
\[
(\bH_k(\bx))_{i_1,i_2,\ldots,i_k}=\frac{1}{\sqrt{k!}}\sum_{\substack{\text{Partitions $P$ of $[k]$}\\ \text{into sets of size 1 and 2}}}\bigotimes_{\{a,b\}\in P}(-\bI_{i_a,i_b})\bigotimes_{\{c\}\in P}\bx_{i_c}\; .
\]
\end{definition}

\noindent Given that there is such a polynomial, we show that if we take a flattened version 
of the Chow-parameter tensors (which turns them into matrices) 
and look at the space spanned by their top singular vectors, 
then a \new{non-trivial} fraction of $\bw^*$ must lie inside this subspace. 
We prove the following lemma, which is similar to Lemma 5.10 in \cite{Diakonikolas2022b}. 
\new{See \Cref{app:alg} for the proof.}

\begin{lemma} \label{lem:chow-subspace}
Let $D$ be the joint distribution of $(\bx,y)$ supported on $\R^d\times \{ \pm 1\}$ with 
 marginal $D_{\bx}=\gaus_d$.
Let $p:\R\mapsto\R$ be a univariate, mean zero and unit variance polynomial 
of degree $k$ such that 
for some unit vector $\mathbf{v^*}\in \R^d$ it holds 
$\E_{(\bx,y)\sim D}[\ind(y=-1) p(\langle\mathbf{v^*,\bx} \rangle)]\geq \tau$ 
for some $\tau\in (0,1]$. 
Let $\bT'^m$ be an approximation of the order-$m$ Chow-parameter 
tensor $\bT^m=\E_{(\bx,y)\sim D}[\ind(y=-1){\bf H}_m(\bx)]$ such that 
$\|\bT'^m-\bT^m\|_F\leq \tau/(4\sqrt{k})$. 
Denote by $V_m$ the subspace spanned by the left singular vectors 
of flattened $\bT'^m$ whose 
singular values are greater than $\tau/(4\sqrt{k})$. 
Moreover, denote by $V$ the union of $V_1,\cdots,V_k$. Then, for 
$\gamma=\pr_{(\bx,y)\sim D}[y=-1]$, we have that 
\begin{enumerate}[leftmargin=*]
	\item $\dim(V)= O(\gamma^2 \log (1/\gamma)^k k^2/\tau^2+1)$, and \label{item:p1}
	\item $\|\proj_V(\mathbf{v^*})\|_2= \Omega\left (\tau/\left (\sqrt{k}\gamma \log (1/\gamma)^{k/2}\right )\right )$.
\end{enumerate}
\end{lemma}


By \Cref{lem:chow-subspace}, taking a random unit vector $\bv$ in $V$ will give us 
$\langle \bv,\bv^*\rangle\geq\|\proj_V(\bv^*)\|/\sqrt{\dim(V)}$.
We are ready to prove \Cref{lem:finding-nontrivial-direction}.
For the algorithm pseudocode, see \Cref{app:alg}.




\begin{proof} [Proof Sketch of \Cref{lem:finding-nontrivial-direction} ]
The idea is that the empirical estimate $\bT'^m$ obtained using \\
$d^{O({\max\{{t^*}^2,1\}})}\log(1/\delta)/\eps^2$ many samples will satisfy 
$\|\bT'^m-\bT^m\|_F\leq \tau/(4\sqrt{k})$ with high probability. 
Then, by \Cref{lem:polynomial-correlation} and \Cref{lem:chow-subspace}, 
if we take $\bv$ to be a random \new{unit} vector in $V$, with constant probability 
we will have $\langle \bv ,\bw^*\rangle=\log(1/\eps)^{-O({t^*}^2)}$.
For $\langle\bv,\bw^*\rangle=\eps^{O(1)}$, the proof relies on a different version of \Cref{lem:chow-subspace}.
\end{proof}



\subsection{Random Walk to Update Current Guess} \label{ssec:rw}


We now describe how we use \Cref{lem:finding-nontrivial-direction} 
to construct an algorithm for our learning problem.
Let $\bw$ be the current guess for $\bw^*$. 
For convenience, we assume that $\langle\bw,\bw^*\rangle=\Omega(1/\sqrt{d})$. 
Let $D'$ be the distribution of 
$\bx^{\perp \bw}$ conditioned on $\bx\in B$, 
where $B=\{\bx\in \R^d :\langle\bw,\bx\rangle-t^*\geq 0\}$.
We next show that the $\bx$-marginals of $D'$ are standard Gaussian 
and that $D'$ \new{satisfies} the reliability condition with respect to the halfspace 
$h(\bx)=\sign(\langle \bw',\bx\rangle-t')$ with $\bw'={\bw^*}^{\perp \bw}$ and $|t'|\leq |t^*|$.

\begin{lemma} \label{lem:reduction_satisfies_reliability}
    Let $D$ be the joint distribution of $(\bx,y)$ supported on $\R^d\times\{\pm 1\}$ with  marginal $D_{\bx}=\gaus_d$ and $\eps\in (0,1)$. 
    Suppose $D$ satisfies the reliability condition with respect to $f(\bx)=\sign(\langle\bw^*,\bx\rangle-t^*)$.
    Suppose that $h(\bx)=\sign(\langle\bw,\bx\rangle-t)$ with $\langle \bw,\bw^*\rangle> 0$ and $t-t^*\in [0,\eps/100]$ does not satisfy $R_h^+(D)\leq \eps/2$.
    Let $B=\{\bx\in \R^d :\langle\bw,\bx\rangle-t\geq 0\}$ and $D'$ be the distribution of 
    $(\bx',y)=(\bx^{\perp \bw},y)$ given $\bx\in B$, then
    \begin{enumerate}[leftmargin=*]
        \item $D'$ has marginal distribution $D_{\bx'}=\gaus_{d-1}$,\label{prop:1}
        \item $D'$ satisfies the reliability condition with respect to $h'(\bx)=\sign(\langle \bw',\bx\rangle-t')$, where $\bw'={\bw^*}^{\perp \bw}/\|{\bw^*}^{\perp \bw}\|_2$ and $|t'|\leq |t^*|$, and\label{prop:2}
        \item $\pr_{(\bx',y)\sim D'} [y=-1]\geq \eps/2$.    \label{prop:3}
    \end{enumerate}
\end{lemma}

\begin{proof} [Proof Sketch of \Cref{lem:reduction_satisfies_reliability}]
\Cref{prop:1} follows by definition.
For \Cref{prop:2}, we consider two cases: $t^*>0$; and $t^*\leq 0$.
For the case $t^*\leq 0$, we prove that the distribution $D'$ 
satisfies the reliability condition with respect to 
$h'(\bx)=\sign(\langle\bw',\bx\rangle)$, 
where $\bw'={\bw^*}^{\perp \bw}/\|{\bw^*}^{\perp \bw}\|_2$.
For the case $t^*> 0$, we prove that the distribution $D'$ satisfies 
the reliability condition with respect to $h'(\bx)=\sign(\langle\bw',\bx\rangle-t')$, 
where $\bw'={\bw^*}^{\perp \bw}/\|{\bw^*}^{\perp \bw}\|_2$ and $t'=t^*$.
Then \Cref{prop:3} follows from the fact that $h$ does not satisfy $R_h^+(D)\leq \eps/2$.
\end{proof}



By \Cref{lem:reduction_satisfies_reliability}, 
given any current guess $\bw$ such that 
$h(\bx)=\sign(\langle \bw,\bx \rangle-t^*)$
does not satisfy $R_+(h;D)\leq \eps/2$, the corresponding distribution $D'$ satisfies the reliability condition with respect to $h'$ and 
$\pr_{(\bx,y)\sim D'}[y=-1]\geq \eps/2$.
Therefore, $D'$ satisfies the assumptions of \Cref{lem:finding-nontrivial-direction}.
So, if we apply the algorithm in \Cref{lem:finding-nontrivial-direction}, 
we will with probability at least $\Omega(1)$ 
get a unit vector $\bv$ such that $\langle \bv,\bw\rangle=0$
and $\langle \bv,\bw^*\rangle=\log(1/\eps)^{-O({t^*}^2)}$. 

The following fact shows that by updating our current guess 
in the direction of $\bv$ 
with appropriate step size, we can get an updated guess 
with increased correlation with $\bw^*$.   

\begin{fact} [Correlation Improvement, Lemma 5.13 in \cite{Diakonikolas2022b}] \label{lem:coorelation-improvement}
Fix unit vectors $\bv^*,\bv\in \R^d$. Let $\bu\in \R^d$ such that $\langle \bu,\bv^*\rangle\geq c$, $\langle \bu,\bv\rangle=0$ and 
$\|\bu\|_2\leq 1$ with $c>0$. Then, for $\bv'=\frac{\bv+\lambda \bu}{\|\bv+\lambda \bu\|_2}$, with $\lambda=c/2$, we have that $\langle \bv',\bv^*\rangle\geq \langle \bv,\bv^*\rangle+\lambda^2/2$.
\end{fact}

Notice that because $\|\proj_{\perp\bw}(\bw^*)\|_2$ is unknown, 
we cannot always choose the optimal step size $\lambda$.
Instead, we will use the same $\lambda$ to do sufficiently many update steps 
such that 
after that many updates, we are certain 
that $\|\proj_{\perp\bw}(\bw^*)\|_2\leq 3\lambda$.
We then take the new step size $\lambda_{\text{update}}=\lambda/2$ 
and repeat this process, until $\bw$ and $\bw^*$ are sufficiently close to each other.

We are now ready to describe our algorithm (\Cref{alg:random-walk-reliable}) 
and prove \new{its correctness}. 


Notice that given that we know the bias $\alpha$, $t$ must be either $-\Phi^{-1}(\alpha)$ or $\Phi^{-1}(\alpha)$.
For convenience, we assume that $t=\Phi^{-1}(\alpha)$.
To account for the case that $t=-\Phi^{-1}(\alpha)$, we can simply run \Cref{alg:random-walk-reliable} twice and pick the output halfspace with the smallest $t$ value (or even run a different efficient algorithm, since $t\leq 0$ as explained in \Cref{app:alg}).
For convenience, we also assume 
$\min\left (2^{\log(1/\eps)^{O(\log (1/\alpha))}},2^{\poly(1/\eps)}\right )=2^{\log(1/\eps)^{O(\log (1/\alpha))}}$.
To account for the other case, we can simply initialize the step size $\zeta=\eps^c$ for a sufficiently large constant $c$ instead of 
$\zeta=\log(1/\eps)^{-c{t^*}^2}$.

A more detailed version of \Cref{alg:random-walk-reliable} is deferred to \Cref{app:alg}.

\begin{algorithm}
    \caption{Reliably Learning General Halfspaces with Gaussian Marginals.}
    \label{alg:random-walk-reliable}
    \centering\fbox{\parbox{5.7in}{
        \textbf{Input:} $\eps\in (0,1)$, $\alpha\in (0,1/2)$ and samples access to a joint distribution $D$ of $(\bx,y)$ supported on $\R^d\times \{\pm 1\}$ with $\bx$-marginal $D_{\bx}=\gaus_d$. 
        
        \textbf{Output:} $h(\bx)=\sgn(\langle \bw,\bx\rangle-t)$ that is $\eps$-reliable with respect to the class $\mathcal{H}_d^\alpha$.

        \begin{enumerate} [leftmargin=8mm]
            \item  \label{step1}
            Check if $\pr_{(\bx,y)\sim D} [y=-1]\leq \eps/2$ (with sufficiently small constant failure probability). If so, return the $+1$ constant hypothesis.
            Set the initial step size $\zeta=\log(1/\eps)^{-c{t^*}^2}$, where $c$ is a sufficiently large universal constant and $t=\Phi^{-1}(\alpha)$.  
            
            \item  \label{step2}
            Initialize $\bw$ to be a random unit vector in $\R^d$.
            Let the update step size $\lambda=\zeta$ and repeat the following process until 
            $\lambda\leq \eps/100$. 
            \begin{enumerate} 
                \item \label{step2a}
                Use samples from $D$ to check if the hypothesis $h(\bx)=\sign(\langle \bw,\bx\rangle-t)$ satisfies $R_+(h;D)\leq \epsilon/2$.
                If so, go to Step \eqref{step3}.
                
                \item  \label{step2b}
                With $1/2$ probability, let $\bw=-\bw$. Let $B=\{\bx\in \R^d :\langle\bw,\bx\rangle-t\geq 0\}$, and let $D'$ be the distribution of $(\bx^{\perp \bw},y)$ for $(\bx,y)\sim D$ given $\bx\in B$.
                Use the algorithm of \Cref{lem:finding-nontrivial-direction} on $D'$ to find a unit vector $\bv$ such that $\langle \bv,\bw\rangle=0$ and
                $\left \langle \bv, \frac{\proj_{\perp \bw}(\bw^*)}{\|\proj_{\perp \bw}(\bw^*)\|_2}\right \rangle\geq \zeta$.
                Then update $\bw$ as follows: 
                $
                \bw_{\mathrm{update}}=\frac{\bw+\lambda\bv}{\|\bw+\lambda\bv\|_2}\; .
                $
                

                \item \label{step2c}
                Repeat Steps \eqref{step2a} and \eqref{step2b} $c/\zeta^2$ times, where $c$ is a sufficiently large universal constant, with the same step size $\lambda$.
                After that, update the new step size as $\lambda_{\mathrm{update}}=\lambda/2$.
                
            \end{enumerate}  
            \item  \label{step3} Check if $h(\bx)=\sign(\langle \bw,\bx\rangle-t)$ satisfies $R_+(h;D)\leq \eps/2$. If so, return $h$ and terminate.
            Repeat Step \eqref{step2}
            $2^{1/\zeta^c}$ many times where $c$ is a sufficiently large constant. 
        \end{enumerate}
        }}
\end{algorithm}



 
\begin{proof} [Proof Sketch of the algorithmic part of \Cref{thm:algor-and-lb}]
    Let 
    $f(\bx)=\sign(\langle\bw^*,\bx\rangle-t^*)$ be the optimal halfspace with $\alpha$ bias.
    We need to show that with high probability \Cref{alg:random-walk-reliable} 
    returns a hypothesis $h(\bx)=\sign(\langle\bw,\bx\rangle-t)$ 
    such that $R_+(h;D)\leq \eps$ and $R_-(h;D)\leq R_-(f;D)+\eps$.

    To do so, it suffices to show that $R_+(h;D)\leq \eps$; 
    given $R_+(h;D)\leq \eps$, $R_-(h;D)\leq R_-(f;D)+\eps$ follows from our choice of $t$. 
    For convenience, we can assume $h$ never satisfies $R_+(h;D)\leq \eps/2$ 
    in Step \eqref{step2a}
    (otherwise, we are done).
    We can also assume that the subroutine in \Cref{lem:finding-nontrivial-direction} 
    always succeeds since the algorithm repeats Step \eqref{step2}
    sufficiently many times. Given the above conditions, using
    \Cref{lem:coorelation-improvement}, one can show that 
    each time after $c/\zeta^2$ many updates in Step \eqref{step2b}, 
    we must have $\|\proj_{\perp \bw}\bw^*\|_2\leq 3\lambda$. 
    Therefore, when we have $\lambda\leq \eps/100$, \new{then} $ 
    \|\proj_{\perp \bw}\bw^*\|_2\leq 3\eps/100$, which implies $R_+(h;D)\leq \eps/2$.
\end{proof}






























\section{Nearly Matching SQ Lower Bound} \label{sec:lb}
In this section, 
we establish the SQ hardness result of \Cref{thm:algor-and-lb}.
Due to space limitations, some proofs have been deferred to~\Cref{app:lb}.


\paragraph{Proof Overview} 
To establish our SQ lower bound for reliable learning, 
we first prove an SQ lower bound for a natural decision 
version of reliably learning $\alpha$-biased LTFs.
We define the following decision problem over distributions.

\begin{definition} [Decision Problem over Distributions]
Let $D$ be a fixed distribution and $\cal D$ be a distribution family. 
We denote by $\mathcal{B}(\D,D)$ the decision problem in which 
the input distribution $D'$ is promised to satisfy either (a) $D'=D$ or (b) $D'\in \D$,
and the goal is to distinguish the two cases with high probability.
\end{definition}
We show that given SQ access to a joint distribution $D$ of $(\bx,y)$ 
supported on $\R^d\times \{\pm 1\}$
with marginal $D_\bx=\gaus({\bf 0},\bI)$,
it is hard to solve the problem $\mathcal{B}(\D,D)$ with the following distributions.
\begin{enumerate}[leftmargin=*]

\item [(a)] Null hypothesis: \new{$D$ is the distribution so that }$y=1$ with probability $1/2$ independent of $\bx$.

\item [(b)] Alternative hypothesis: 
$D\in \D$, where $\D$ is a family of distributions such that 
for any distribution $D\in \D$, 
there exists an $\alpha$-biased LTF $f$ such that $R_+(f;D)=0$.
\end{enumerate}


In order to construct such a family of distributions $\D$, 
we start by constructing a joint distribution $D'$ of $(z,y)$ over $\R\times\{\pm 1\}$ 
such that the marginal distribution of $z$ is $\gaus_1$ and 
the conditional distributions $z\mid y=1$ and $z\mid y=-1$ both match many moments 
with the standard Gaussian $\gaus_1$.
Moreover, there is $\alpha$ probability mass on the positive side of the marginal distribution of $z$ that
is purely associated with $y=1$ (i.e., $\E_{(z,y)\sim D}[y\mid z\geq c]=1$ where $c=\Phi^{-1}(1-\alpha)$). 
We then embed this distribution $D$ along a hidden direction inside 
the joint distribution of $(\bx,y)$ on $\R^d\times \{\pm 1\}$ to construct 
a family of hard-to-distinguish distributions using the ``hidden-direction'' framework developed in \cite{DKS17-sq} 
and enhanced in \cite{DKPZ21,DKRS23}.

\new{We can now proceed with the details of the proof.}
We start by defining the pairwise correlation between distributions.

\begin{definition} [Pairwise Correlation]
The pairwise correlation of two distributions with
pdfs $D_1,D_2:\R^d\mapsto \R_+$ 
with respect to a distribution with density $D:\R^d\mapsto \R_+$, 
where the support of $D$ contains the support of $D_1$ and $D_2$, 
is defined as $\chi_D(D_1,D_2)\eqdef \int_{\R^d}D_1(\x)D_2(\x)/D(\x)d\x-1$. 
Furthermore, the $\chi$-squared divergence of $D_1$ to $D$ is defined as
$\chi^2(D_1,D)\eqdef \chi_D(D_1,D_1)$.
\end{definition}

In particular, the framework in \cite{DKS17-sq} allows us to construct a family of $2^{d^{\Omega(1)}}$ distributions on $\R^d\times \{\pm 1\}$
whose pairwise correlation is $d^{-\Omega(n)}$ where $n$ is the number of matching moments 
$D'$ has with the standard Gaussian.
Then, using standard SQ dimension techniques, this gives an SQ lower bound 
for the distinguishing problem. After that, we reduce the distinguishing problem
to the problem of reliably learning $\alpha$-biased LTFs 
under Gaussian marginals with additive error $\epsilon<\alpha/3$. 


In order to construct such a distribution $D'$ of $(z,y)$ supported on $\R\times\{\pm 1\}$, 
we reparameterize $\E_{(z,y)\sim D'}[y|z]$ as $g(z)$. 
For a function $g:\R\to\R$, we use $\|g\|_p=\E_{t\sim \gaus_1}[|g(t)|^p]^{1/p}$ 
for its $L_p$ norm.  
We let $L^1(R)$ denote the set of all functions $g:\R\to\R$ that have finite $L_1$-norm.

We use linear programming over one-dimensional functions in $L^1(\R)$ space 
to establish the existence of such a function. 
Specifically, we show the following:

\begin{lemma} \label{lem:polynomial}
For any sufficiently large $n\in\N$, there exists a function $g:\R\mapsto [-1,+1]$ 
such that $g$ satisfies the following properties:
\begin{itemize}[leftmargin=*]
	\item[(i)] $g(z)=1$ for all $z\geq c$, where $c=\Phi^{-1}(1-3^{-2n}/4)$, and
	
	\item[(ii)] $\E_{t\sim \gaus_1}[g(z)z^k]=0$ for all $k\in [n]$. 
\end{itemize}
\end{lemma}

\begin{proof} [Proof Sketch of \Cref{lem:polynomial}]
We let $P_n$ denote the set of all polynomials $p:\R\to \R$ of degree at most $n$ and let
$L^1_+(\R)$ denote the set of all nonnegative functions in $L^1(\R)$.
Then, using linear programming, we will get the following primal:
\begin{alignat*}{7}
    &\text{find} \quad& g\in L^1(\R)\\
    &\text{such that}\quad& \E_{z\sim \gaus_1}[p(z)g(z)] &= 0\; ,& \forall p&\in P_n\\
    &      \quad&\E_{z\sim \gaus_1}[g(z)h(z)\1\{t\geq c\}] & \geq \|h(z) \1\{z\geq c\}\|_1\; , & \quad \forall h&\in L_+^1(\R)                \\
    &      \quad&\E_{z\sim \gaus_1}[g(z)H(z)]&\leq \|H\|_1\; ,   &\quad \forall H&\in L^1(\R)
\end{alignat*}
Then, using (infinite-dimensional) LP duality, 
we get that the above primal is feasible if and only if 
there is no polynomial of degree $n$ such that 
$\E_{t\sim \gaus_1}[|p(t)|\mathds{1}(t\leq c)]<\E_{t\sim \gaus_1}[p(t)\mathds{1}(t\geq c)]$.

 Using Gaussian hypercontractivity \new{(see \cite{Bog:98, nelson1973free})}, one can show that for every polynomial $p$ and $c\in \R$, it holds 
\[\E_{z\sim \gaus_1}[|p(z)|]
	<2\cdot 3^n\E_{z\sim \gaus_1}[|p(z)|]\left( \pr_{z\sim \gaus_1}[z\geq c]\right )^{1/2}\;.\]
From our choice of the parameter $c$, we have that $\pr_{z\sim \gaus_1}[z\geq c]\leq 3^{-2n}/4$; thus, $\E_{z\sim \gaus_1}[|p(z)|]<\E_{z\sim \gaus_1}[|p(z)|]$, which is a
contradiction.
Therefore, such a polynomial $p$ cannot exist. 
\end{proof}

	
We have proven the existence of the function $g$ in \Cref{lem:polynomial}.
Now, we construct a joint distribution of $(z,y)$ on $\R\times\{\pm 1\}$ such that $\E[y|z]$ is exactly $g$, as we discussed in the proof outline.
For a joint distribution $D$ of $(x,y)$ supported on $X \times \{\pm 1\}$, 
we will use $D_+$ to denote the conditional distribution of $x$ given $y=1$; 
and $D_-$ for the distribution of $x$ given $y=-1$.

\begin{lemma} \label{lem:distribution}
For any sufficiently large $n\in \N$, there exists a distribution $D$ on $\R\times\{\pm 1\}$ such that 
for $(z,y)\sim D$: 
\begin{itemize}[leftmargin=7mm]
	\item [(i)] the marginal distribution $D_z=\gaus_1$;
	
	\item [(ii)]$\E_{(z,y)\sim D}[y|z=z']=1$ for all $z'\geq \Phi^{-1}(1-3^{-2n}/4)$; 

	\item [(iii)]$\E_{(z,y)\sim D}[y]=0$ and 
	$\E_{(z,y)\sim D}[z^k]=\E_{(z,y)\sim D}[z^k\mid y=1]=\E_{(z,y)\sim D}[z^k\mid y=-1]$  for all $k\in [n]$;

	\item [(iv)]$\chi^2(D_+,\gaus_{1}), \chi^2(D_-,\gaus_{1})=O(1)$.
\end{itemize}
\end{lemma}

\begin{proof} [Proof Sketch for \Cref{lem:distribution}]
    The properties here directly follow from the properties of $g$.
\end{proof}



Using the framework introduced in \cite{DKS17-sq, DKPZ21}, 
we can construct a set of alternative hypothesis distributions 
$\D=\{D_\bv:\bv\in V\}$ on $\R^d\times \{\pm 1\}$, 
where $V$ is a set of exponentially many pairwise nearly-orthogonal vectors 
and the marginal distribution of each $D_{\bv}$ on direction $\bv$ is the distribution 
$D$ in \Cref{lem:distribution}.  
This effectively embeds the distribution $D$ in a hidden direction $\bv$.
The size of the family $\D$ is exponential in $d$, 
and the distributions in it have small pairwise correlations. 
The details of $\D$ are deferred to \Cref{app:lb}.
Now, we are ready to give a proof sketch for the SQ hardness 
part of our main theorem \Cref{thm:algor-and-lb}.

\begin{proof} [Proof Sketch of the SQ hardness part of \Cref{thm:algor-and-lb}]
Let $\D$ be the set of distributions discussed above. 
We also let $D_\nul$ be the joint distribution of $(\x,y)$ such that 
$\x\sim \gaus_d$ and $y\sim \mathrm{Bern}(1/2)$ independent of $\x$. 
Then, using standard SQ dimension techniques, one can show that 
any SQ algorithm that solves $\mathcal{B}(\D,D_\nul)$ requires either 
queries of tolerance at most $d^{-\Omega(\log\frac{1}{\alpha})}$ or 
makes at least $2^{d^{\Omega(1)}}$ queries.
By reducing the decision problem $\mathcal{B}(\D,D_\nul)$ 
to reliably learning $\alpha$-biased LTFs 
with $\epsilon<\alpha/3$ accuracy, we get the lower bound part 
of the statement in \Cref{thm:algor-and-lb}.
\end{proof}









\newpage

\section{Conclusions and Open Problems} \label{sec:concl}
In this paper, we study the problem of learning halfspaces under 
Gaussian marginals in the reliable learning model. Our main 
contribution is the design of the first efficient learner for this 
task whose complexity beats the complexity of agnostically 
learning this concept class. Moreover, we provide rigorous evidence, via an SQ lower bound, that no 
fully-polynomial time algorithm exists for general halfspaces. The 
obvious open question is whether the dependence on $\eps$ in the 
complexity of our algorithm  can be improved. Specifically, is it 
possible to design a reliable learner with complexity 
$d^{O(\log(1/\alpha))} \poly(1/\epsilon)$? Is it possible to 
obtain similarly efficient reliable 
learners under more general marginal distributions (e.g., strongly 
log-concave or discrete distributions)?
More broadly, it would be interesting to characterize the 
computational separation between (distribution-specific)  
reliable and agnostic learning for other 
natural concept classes. 


	
\bibliographystyle{alpha}
\bibliography{clean2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\appendix


\section*{Appendix}

\paragraph{Organization} The supplementary material is structured as follows:
\Cref{app:prelims} includes additional preliminaries. 
\Cref{app:alg} includes the omitted proofs from \Cref{sec:alg}. 
Finally, \Cref{app:lb} includes the omitted proofs from \Cref{sec:lb}.


\section{Additional Preliminaries} \label{app:prelims}

We use $\mathbb{S}^{d-1}$ to denote the $d$-dimensional unit sphere, i.e., $\mathbb{S}^{d-1}\eqdef\{\x\in\R^d:\|\x\|_2=1 \}$.

\paragraph{Basics of the SQ Model}
In order to give an SQ lower bound for reliable learning, we first present the basics of the SQ model. 
We define the pairwise correlation, which is used for the SQ lower bound.

\begin{definition} [Pairwise Correlation]
The pairwise correlation of two distributions with probability density function $D_1, D_2:\R^d\mapsto \R_+$ 
with respect to a distribution with density $D:\R^d\mapsto \R_+$, 
where the support of $D$ contains the support of $D_1$ and $D_2$, 
is defined as $\chi_D(D_1,D_2)\eqdef \int_{\R^d}D_1(\x)D_2(\x)/D(\x)d\x-1$. 
Furthermore, the $\chi$-squared divergence of $D_1$ to $D$ is defined as
$\chi^2(D_1,D)\eqdef \chi_D(D_1,D_1)$.
\end{definition}

We require the following definitions and lemmata from~\cite{FeldmanGRVX17} 
connecting pairwise correlation and SQ lower bounds.

\begin{definition}
We say that a set of $s$ distribution $\D=\{D_1,\cdots,D_s\}$ over $\R^d$ is 
$(\gamma,\beta)$-correlated relative to a distribution $D$ if $\chi_D(D_i,D_j)\leq  \gamma$ for all $i\neq j$, 
and $\chi_D(D_i,D_j)\leq  \beta$ for $i=j$.
\end{definition}

\begin{definition} [Statistical Query Dimension]
For $\beta,\gamma>0$, a decision problem $\mathcal{B}(\D,D)$, where $D$ is a fixed distribution and 
$\D$ is a family of distribution, let $s$ be the maximum integer such that
there exists a finite set of distributions $\D_D\subseteq\D$ such that 
$\D_D$ is $(\gamma,\beta)$-correlated relative to $D$ and $|\D_D|\geq s$. 
The Statistical Query dimension with pairwise correlations $(\gamma,\beta)$ of $\mathcal{B}$ is defined to be $s$, 
and denoted by $s=\mathrm{SD}(\mathcal{B},\gamma,\beta)$.
\end{definition}

\begin{lemma}\label{lem:sq-lb}
Let $\mathcal{B}(\D,D)$ be a decision problem, where $D$ is 
the reference distribution and $\D$ is a class of distribution. For $\gamma,\beta>0$, 
let $s=\mathrm{SD}(\mathcal{B},\gamma,\beta)$. 
For any $\gamma'>0$, any SQ algorithm for $\mathcal{B}$ requires queries of tolerance 
at most $\sqrt{\gamma+\gamma'}$ or makes at least $s\gamma'/(\beta-\gamma)$ queries.
\end{lemma}


\paragraph{Basics of Hermite Polynomials}
We require the following definitions used in our work. 

\begin{definition}[Normalized Hermite Polynomial]\label{def:Hermite-poly}
For $k\in\N$,
we define the $k$-th \emph{probabilist's} Hermite polynomials
$\mathrm{\textit{He}}_k:\R\to \R$
as
$\mathrm{\textit{He}}_k(t)=(-1)^k e^{t^2/2}\cdot\frac{\d^k}{\d t^k}e^{-t^2/2}$.
We define the $k$-th \emph{normalized} Hermite polynomial 
$h_k:\R\to \R$
as
$h_k(t)=\mathrm{\textit{He}}_k(t)/\sqrt{k!}$.
\end{definition}

Furthermore, we use multivariate Hermite polynomials in the form of Hermite tensors (as the entries in the Hermite tensors are re-scaled multivariate Hermite polynomials).
We define the \emph{Hermite tensor} as follows.
\begin{definition}[Hermite Tensor]\label{def:Hermite-tensor-app}
For $k\in \N$ and $\bx\in\R^d$, we define the $k$-th Hermite tensor as
\[
(\bH_k(\bx))_{i_1,i_2,\ldots,i_k}=\frac{1}{\sqrt{k!}}\sum_{\substack{\text{Partitions $P$ of $[k]$}\\ \text{into sets of size 1 and 2}}}\bigotimes_{\{a,b\}\in P}(-\bI_{i_a,i_b})\bigotimes_{\{c\}\in P}\bx_{i_c}\; .
\]
\end{definition}

We also point out that fully reliable learning (see \cite{KKM2012} for the definition) reduces to one-sided reliable learning (\Cref{def:reliable-learning}). The proof is 
implicit in the proof of Theorem 3 in \cite{KKM2012} and we include it here for completeness. 
\begin{fact}
    For a joint distribution $D$ of $(\x,y)$ supported on $X\times \{\pm 1\}$,
    let $h_+:X\to \{\pm 1\}$ (resp. $h_-:X\to \{\pm 1\}$) be a hypothesis that is $\eps/4$-reliable with respect to the concept class $C$ on distribution $D$ for positive labels (resp. for negative labels).
    Let hypothesis $h$ be defined as $h(\bx)=1$ if $h_+(\bx)=h_-(\bx)=1$, $h(\bx)=-1$ if $h_+(\bx)= h_-(\bx)=-1$ and $h(\bx)=?$ otherwise.
    Then $h$ is $\eps$ close to the best fully reliable sandwich hypothesis for $C$ on distribution $D$.
\end{fact}
\begin{proof} [Proof Sketch]
    We first show that $R_+(h;D), R_-(h;D)\leq \eps$.
    Notice that $$R_+(h;D)=\pr_{(\bx,y)\sim D}[h(\bx)=1\land y=-1]\leq \pr_{(\bx,y)\sim D}[h_+(\bx)=1\land y=-1]\leq \eps \;.$$
    The same holds for negative labels.

    To show that $\pr_{(\bx,y)\sim D}[h(\bx)=?]$ is $\eps$-suboptimal,
    assume (for the purpose of contradiction) that 
    there is an $h'$ such that 
    $R_+(h';D), R_-(h';D)\leq \eps$ and $\pr_{(\bx,y)\sim D}[h'(\bx)=?]<\pr_{(\bx,y)\sim D}[h(\bx)=?]-\eps$.
    Notice that 
    \begin{align*} 
      &\pr_{(\bx,y)\sim D}[h(\bx)=?]\\
    = &\pr_{(\bx,y)\sim D}[h_+(\bx)=1\land h_-(\bx)=-1]+\pr_{(\bx,y)\sim D}[h_+(\bx)=-1\land h_-(\bx)=1]\\
    \leq & 1-\pr_{(\bx,y)\sim D}[h_+(\bx)=1]-\pr_{(\bx,y)\sim D}[h_-(\bx)=-1]+\pr_{(\bx,y)\sim D}[h_+(\bx)=1\land h_-(\bx)=-1]\\
    \leq & 1-\pr_{(\bx,y)\sim D}[h_+(\bx)=1]-\pr_{(\bx,y)\sim D}[h_-(\bx)=-1]+\eps/2\; . 
    \end{align*}
    Thus,
    \begin{equation} \label{eq:full}
        \pr_{(\bx,y)\sim D}[h_+(\bx)=1]+\pr_{(\bx,y)\sim D}[h_-(\bx)=-1]\leq 
        1-\pr_{(\bx,y)\sim D}[h(\bx)=?]+\eps/2\; .
    \end{equation}
    Then, define $h_+':X\to \{\pm 1 \} $ as $h_+'(\bx)=1$ if $h'(\bx)=1$ and $h_+'(\bx)=-1$ otherwise ($h_-'$ is defined similarly).
    We get $\pr_{(\bx,y)\sim D}[h'_+(\bx)=1]+\pr_{(\bx,y)\sim D}[h'_-(\bx)=-1]=1-\pr_{(\bx,y)\sim D}[h'(\bx)=?]$.
    Using \Cref{eq:full} and $\pr_{(\bx,y)\sim D}[h'(\bx)=?]<\pr_{(\bx,y)\sim D}[h(\bx)=?]-\eps$, we get 
    \[
    \pr_{(\bx,y)\sim D}[h_+(\bx)=1]-\pr_{(\bx,y)\sim D}[h_-(\bx)=-1]< \pr_{(\bx,y)\sim D}[h'_+(\bx)=1]-\pr_{(\bx,y)\sim D}[h'_-(\bx)=-1]-\eps/2\; .
    \]
    Therefore, either $\pr_{(\bx,y)\sim D}[h_+(\bx)=1]<\pr_{(\bx,y)\sim D}[h'_+(\bx)=1]-\eps/4$ or $$\pr_{(\bx,y)\sim D}[h_-(\bx)=-1]<\pr_{(\bx,y)\sim D}[h'_-(\bx)=-1]-\eps/4 \;.$$
    Thus, either $h_+$ or $h_-$ is not $\eps/4$-suboptimal. This gives a contradiction.
\end{proof}


\section{Omitted Proofs from \Cref{sec:alg}} \label{app:alg}

We define the bias of a function 
$h:\R^d\to \{\pm 1\}$ as $\min(\pr_{\bx\sim \gaus_d}[h(\bx)=1],\pr_{\bx\sim \gaus_d}[h(\bx)=-1])$,
and define $\mathcal{H}_d^\alpha$ to be the set of all LTFs whose bias is $\alpha$.
Let $f=\argmin_{c\in \mathcal{H}_d^\alpha\; \land\; R_+(f;D)=0} R_-(f;D)$ be the optimal reliable hypothesis and $\alpha$ be the bias of $f$.

\subsection{Reliably Learning Halfspaces with Gaussian Marginals when $\eps\geq 2\alpha$}

We show that for the problem in \Cref{thm:algor-and-lb}, in the case of $\eps\geq 2\alpha$, the algorithm can just return one of the constant hypotheses. Let $h_+$ (resp. $h_-$) be the $+1$ (resp. $-1$) constant hypothesis.
\begin{fact}
For the problem defined in \Cref{thm:algor-and-lb},
the case where $\eps\geq 2\alpha$ can be efficiently solved
by returning $h_+$ if $R_+(h_+;D)\leq 3\eps/4$ and returning $h_-$ otherwise. 
\end{fact}
\begin{proof}
When the algorithm returns $h_+$, it is easy to see that $h_+$ satisfies the reliable learning requirement in \Cref{def:reliable-learning}. So we only consider the case that the algorithm returns $h_-$.

Given the algorithm returns $h_-$, either 
there is an $\alpha$-biased LTF $f$ such that $R_+(f;D)=0$ or
none of the $\alpha$-biased LTFs $f$ satisfies $R_+(f;D)=0$.
For the case that none of the $\alpha$-biased LTFs $f$ satisfies $R_+(f;D)=0$, it is easy to see $h_-$ is a valid answer from the definition of \Cref{def:reliable-learning}.
For the case that there is an $\alpha$-biased LTF $f$ such that $R_+(f,D)=0$,
since the algorithm did not return $h_+$,
it must be the case that $\pr_{(\bx,y)\sim D}[y=-1]>3\eps/4>\alpha$.
Therefore, we have $\pr_{\bx\sim \gaus_d} [f(\bx)=-1]=1-\alpha$.
Thus, we get $R_+(h_-,D)=0$ and $R_-(h_-,D)\leq R_-(f,D)+\alpha$.
This shows $h_-$ is $\eps$ reliable with respect to all $\alpha$ biased LTFs.
This completes the proof.
\end{proof}



\subsection{Reliably Learning Halfspaces with Gaussian Marginals: Case of Unknown $\alpha$}

Here we establish our main result:

\begin{theorem}[Main Algorithmic Result] \label{main-algorithm-known-alpha}
  Let $\eps\in (0,1/2)$ and let $D$ be a joint distribution of $(\bx,y)$ supported on $\R^d\times \{ \pm 1\}$ 
  with marginal $D_{\bx}=\gaus_d$. 
  Let $f=\argmin_{f\in \mathcal{H}_d\; \land\; R_+(f;D)=0} R_-(f;D)$ be the optimal halfspace and $\alpha$ be its bias which is unknown.  
  Then there is an algorithm that 
  uses $N=d^{O(\log (\min\{1/\alpha,1/\eps\}))}\min\left (2^{\log(1/\eps)^{O(\log (1/\alpha))}},2^{\poly(1/\eps)}\right )$ many samples 
  from $D$, runs in
  $\poly(N,d,1/\eps)$ time and with high probability returns a hypothesis 
  $h(\bx)=\sign(\langle \bw,\bx\rangle-t)$ that is $\eps$-reliable with respect to the class of LTFs.    
\end{theorem}

To prove \Cref{main-algorithm-known-alpha}, we need to use the algorithm in the following lemma as a black box. The lemma shows that if the optimal halfspace $f$ satisfies $\pr_{\bx\sim \gaus_d} [f(\bx)=-1]\leq 1/2$,
then the problem can be solved efficiently.

\begin{lemma} \label{prp:easy-case}
    Let $D$ be the joint distribution of $(\bx,y)$ supported on $\R^d\times \{ \pm 1\}$ with 
    marginal $D_{\bx}=\gaus_d$ and $\eps\in (0,1)$.
    Suppose the optimal halfspace $f=\argmin_{f\in \mathcal{H}_d\; \land\; R_+(f;D)=0} R_-(f;D)$ satisfies $\pr_{\bx\sim \gaus_d} [f(\bx)=-1]\leq 1/2$. Then there is an algorithm that reliably learns LTFs on $D$ using $N=O(d/\eps^2)$ many samples and $\poly(N, d)$ running time.
\end{lemma}
\begin{proof}
        The algorithm is the following: 
    \begin{enumerate}[leftmargin=*]
        \item \label{step:sampling} First check if $\pr_{(\bx,y)\sim D)}[y=-1]\leq \eps$ with sufficiently small constant failure probability. If so, return the +1 constant hypothesis.
        \item Otherwise, draw $m=(d/\eps)^c$ many samples $S=\{(\bx_1,y_1),\cdots , (\bx_m,y_m)\}$ conditioned on $y=-1$.
        By Step \ref{step:sampling}, the sampling efficiency here is $\Omega(\eps)$ with high probability.
        \item Solve the following semidefinite program for $\bw'$ and $t'$:
        \begin{alignat*}{7} \label{alg:convex_program}
            &\text{minimize} & t'\\
            &\text{such that}\quad& \langle \bw',\bx\rangle-t'\leq & 0\; ,& \quad\forall (\bx,y)\in S\\\
            &                \quad&                 \|\bw'\|_2\leq & 1 
        \end{alignat*}
        Return the hypothesis $h(\bx)=\sign(\langle \bw,\bx\rangle-t)$, where $\bw=\bw'/\|\bw'\|_2$ and $t=t'/\|\bw'\|_2$.
    \end{enumerate}

    Let $f(\bx)=\sign(\langle \bw^*,\bx\rangle-t^*)$ be the optimal hypothesis.
    We prove that $h(\bx)=\sign(\langle\bw,\bx\rangle-t)$ is such that $R_+(h;D)\leq \eps$ and $R_-(h;D)\leq R_-(f;D)+\eps$.
    Since $h$ is consistent with all the negative samples, we have $R_+(h;D)\leq \eps$.
    From the assumption that 
    $\pr_{\bx\sim \gaus_d} [f(\bx)=-1]\leq 1/2$, 
    we have $t^*\leq 0$.
    Notice that $\bw^*$ and $t^*$ is a feasible solution to the above program; therefore,
    $t'\leq t^*\leq 0$ and we must have $t=t'/\|\bw'\|_2\leq t'\leq t^*$,
    which implies that $\pr_{(\bx,y)\sim D}[h(\bx)=1]\geq \pr_{(\bx,y)\sim D}[f(\bx)=1]$.
    Therefore,
    \begin{align*}
        R_-(h;D) &=\pr_{(\bx,y)\sim D}[y=1]-\pr_{(\bx,y)\sim D}[y=1\land h(\bx)=1]\\
        &=\pr_{(\bx,y)\sim D}[y=1]-\pr_{(\bx,y)\sim D}[h(\bx)=1]+R_+(h;D)\\
        &\leq \pr_{(\bx,y)\sim D}[y=1]-\pr_{(\bx,y)\sim D}[f(\bx)=1]+\eps\\
        &= R_-(f;D)+\eps\; .
    \end{align*}
    This completes the proof.
\end{proof}

We now prove \Cref{main-algorithm-known-alpha}.

\begin{proof} [Proof of \Cref{main-algorithm-known-alpha}]
    The algorithm is the following:
    \begin{enumerate}[leftmargin=*]
        \item Run the algorithm in \Cref{prp:easy-case} with $\eps'=\eps/2$. If the output hypothesis $h$ satisfies $R_+(h;D)\leq \eps$ with a sufficiently small constant failure probability, then output $h$ and terminate.
        \item Set $\alpha=1/2-\eps/100$ and run the algorithm in \Cref{thm:main-algorithm-known-alpha} with $\eps'=\eps/2$. If the output hypothesis $h$ satisfies $R_+(h;D)\leq \eps$ with a sufficiently small constant failure probability, then output $h$ and terminate.
        Otherwise, update $\alpha$ as $\alpha-\eps/100$. 
        Repeat Step 2 until the algorithm terminates.
    \end{enumerate}    
    Let $f=\argmin_{f\in \mathcal{H}_d\; \land\; R_+(f;D)=0} R_-(f;D)$ be the optimal halfspace and $\alpha_f$ be its bias which is unknown. 
    Suppose the algorithm terminates. Let $\alpha$ be the bias of the output hypothesis $h$. 
    It is easy to see that $R_+(h;D)\leq \eps$ with high probability; therefore, it only remains to show 
    $R_-(h;D)\leq R_-(f;D)+\eps$.
    Notice that if $\pr_{\bx\sim \gaus_d} [f(\bx)=-1]\leq 1/2$, the algorithm will terminate in Step 1.
    Therefore, without loss of generality, we assume $\pr_{\bx\sim \gaus_d} [f(\bx)=-1]\geq 1/2$. 
    Then, by \Cref{thm:main-algorithm-known-alpha}, it is easy to see that $\alpha\geq \alpha_f-\eps/100$
    since given any $\alpha\leq \alpha_f$, Step 2 is guaranteed to terminate.
    Therefore, we have \[
    \pr_{\bx\sim \gaus_d} [h(\bx)=-1]\leq 1-\alpha 
    \leq 1-\alpha_f+\eps/100=\pr_{\bx\sim \gaus_d} [f(\bx)=-1]+\eps/100\; .
    \]
    Thus,
    \begin{align*}
        R_-(h;D) &=\pr_{(\bx,y)\sim D}[y=1]-\pr_{(\bx,y)\sim D}[y=1\land h(\bx)=1]\\
        &=\pr_{(\bx,y)\sim D}[y=1]-\pr_{(\bx,y)\sim D}[h(\bx)=1]+R_+(h;D)\\
        &\leq \pr_{(\bx,y)\sim D}[y=1]-\pr_{(\bx,y)\sim D}[f(\bx)=1]+\eps\\
        &= R_-(f;D)+\eps\; .
    \end{align*}
    This completes the proof.    
\end{proof}

\subsection{Reliably Learning Halfspaces with Gaussian Marginals for the Case $\eps\leq \alpha/2$}

For convenience, we assume that $\alpha$ is known. This can be assumed without loss of generality as we discussed in \Cref{main-algorithm-known-alpha}. 

We show the following: 

\begin{theorem} \label{thm:main-algorithm-known-alpha}
  Let $\eps\in (0,1/2), \alpha\in(0,1/2]$ and let $D$ be a joint distribution of $(\bx,y)$ supported on $\R^d\times \{ \pm 1\}$ 
    with marginal $D_{\bx}=\gaus_d$. Assume that there is a halfspace in $\mathcal{H}_d^\alpha$ that is reliable with respect to $D$. Then, there is an algorithm that 
    uses $N=d^{O(\log (\min\{1/\alpha,1/\eps\}))}\min\left (2^{\log(1/\eps)^{O(\log (1/\alpha))}},2^{\poly(1/\eps)}\right )$ many samples 
    from $D$, runs in
    $\poly(N,d,1/\eps)$ time and with high probability returns a hypothesis 
    $h(\bx)=\sign(\langle \bw,\bx\rangle-t)$ that is $\eps$-reliable with respect to the class of 
    $\mathcal{H}_d^\alpha$.
\end{theorem}



\noindent Below we provide the omitted proofs from \Cref{sec:alg}.

\subsection{Proof of \Cref{lem:polynomial-correlation}}
Notice that for any polynomial $p$ such that $\E_{z\sim {\gaus_1}}[p(z)]=0$,
we have 
\[\E_{(\bx,y)\sim D}[y\, p(\langle\bw^*,\bx\rangle)]=-2\E_{(\bx,y)\sim D}[\ind(y=-1)\, p(\langle\bw^*,\bx\rangle)]\; .\]
Therefore, it suffices for us to show that there is a zero mean and unit variance polynomial $p$ such that
\[\E_{(\bx,y)\sim D}[\ind(y=-1)\, p(\langle\bw^*,\bx\rangle)]=2^{-O({t^*}^2)}\pr_{(\bx,y)\sim D}[y=-1]\; .\]
Here we will consider the sign-matching polynomial from \cite{Diakonikolas2022b}, which satisfies the following properties:
\begin{claim} \label{clm:sgn-matching-polynomial}
Let $b\in \R$ and $b\geq 4$. There exists a zero mean and unit variance polynomial $p:\R\to\R$ of degree $k=\Theta(b^2)$ such that
\begin{enumerate}[leftmargin=*]
    \item The sign of $p(z)$ matches $\sgn(z-b)$, i.e. $\sgn(p(z))=\sgn(z-b)$.
    \item For any $z\leq b/2$, we have $|p(z)|=2^{-O(k)}$.
\end{enumerate}
\end{claim}
\begin{proof}
    We consider the polynomial $p$ defined as:
    \[\tp(z)=q(z)-\frac{q(b)}{r(b)}r(z)\; ,\]
    where $q(z)=z^{3k}$, $r(z)=z^{2k}-(2k-1)!!$ and $k$ is a sufficiently large odd integer 
    such that $2|b|\leq \sqrt{k}\leq 4\max(|b|,1)$.
    We then take $p(z)=\tp(z)/\sqrt{\E_{u\sim \gaus_1}[\tp^2(u)]}$. 
    
    For convenience, we first note that from Stirling's approximation for $m\in \Z^+$, 
    \[(m/2)^m\leq (2m-1)!!\leq (2m)^m\; .\]
    To prove that $\sign(p(z))=\sign(z-b)$, we first show that $\frac{q(b)}{r(b)}\leq 0$. 
    Since $q(b)\geq 0$, we just need to show $r(b)\leq 0$. 
    Notice $r(b)=b^{2k}-(2k-1)!!$, since $2|b|\leq \sqrt{k}$ and $(2k-1)!!\geq {(k/2)}^k$, 
    thus $r(b)\leq (\sqrt{k}/2)^{2k}-(k/2)^{k}\leq 0$. 
    Therefore, considering $q(z)=z^{3k}$ and $r(z)=z^{2k}-(2k-1)!!$,
    \[\tp(z)=q(z)-\frac{q(b)}{r(b)}r(z)\] 
    must be monotone increasing for $z\geq b$ and
    $p(z)$ must also be monotone increasing for $z\geq b$.
    Notice that $p(b)=0$; therefore, $p(z)>0$ for $z>b$.
    To show $p(z)<0$ for $z<-b$, we prove it for the cases $z\in [-b,+b)$ and $z<-b$.
    For $z\in [-b,+b)$, notice that due to $\frac{q(b)}{r(b)}\leq 0$ and the definition of $q(z)$ and $r(z)$, 
    \[\tp(z)=q(z)-\frac{q(b)}{r(b)}r(z)\leq q(b)-\frac{q(b)}{r(b)}r(b)< p(b)=0\; .\]
    Then, for $z\leq -b$, we show that $p(z)$ is monotone increasing.
    Notice that the derivative $\tp'(z)=kz^{2k-1}(3z^k-2\frac{q(b)}{r(b)})$, 
    for $z\leq -b<0$, this is positive if and only if $3z^k-2\frac{q(b)}{r(b)}<0$.
    If we can show $b^{k}\geq -\frac{2}{3}q(b)/r(b)$, 
    then it is immediate that for any $z\leq -b$,
    $3z^k-2\frac{q(b)}{r(b)}<0$.
    The condition $b^{k}\geq -\frac{2}{3}q(b)/r(b)$ can be further simplified to
    $(2k-1)!!/b^{2k}\geq 5/3$.
    Then considering $2b<\sqrt{k}$ and $(2k-1)!!\geq (k/2)^k$, we have
    \[(2k-1)!!/b^{2k}\geq (k/2)^k/(\sqrt{k}/2)^{2k}\geq 2^k\; ,\]
    thus the condition holds.
    Therefore, $\tp'(z)>0$ for $z<-b$ and $p(z)$ must be monotone increasing for $z<-b$. 
    Then combined with the condition $p(-b)<0$, which is implied from the previous case ($z\in [-b,b]$), we have $p(z)<0$ for $z<-b$.  

    For the Property 2, we show that $p(z)=2^{-\Theta(k)}$ for $z\in [0,b/2]$, $z\in [-b/2,0]$, $z\in [-b,-b/2]$ and $z\in [-\infty,-b]$ separately.
    For $z\in [0,b/2]$ notice that $p(z)$ is convex for $z\in [0,b]$; therefore, it suffices to show $p(0)=2^{-\Theta(k)}$. 
    Note that $p(0)=\frac{q(b)}{r(b)}(2k-1)!!/\sqrt{\E_{u\sim \gaus_1}\Tilde{p}^2(u)}$,
    and we have $\left |\frac{q(b)}{r(b)}\right |\geq b^{3k}/\max(b^{2k},(2k-1)!!)\geq \Omega(k)^{k/2}$.
    Therefore, since $\E_{u\sim \gaus_1}[\tp^2(u)]\leq (O(k))^{3k}$, we have
    $p(0)\geq 2^{-O(k)}$. This completes the proof of the case $z\in [0,b/2]$.
    For the case $z\in [-b/2,0]$,
    it immediately follows from the fact that $|p(-z)|\geq |p(z)|$ by the definition.
    For the case $z\in [-b,-b/2]$, we have $p(z)\leq (p(b)-(b/2)^{3k})/\sqrt{\E_{u\sim \gaus_1}\Tilde{p}^2(u)}\leq -2^{-O(k)}$.
    Then, the case $z\in [-\infty, -b]$ immediately follows from the fact that $p(z)$ is monotone increasing in this interval.
    This completes the proof of Property 2.    
\end{proof}
Given \Cref{clm:sgn-matching-polynomial}, we let $p$ be the polynomial in \Cref{clm:sgn-matching-polynomial} with $b=-\max(2|t^*|,4)$. Then, given that $D$ satisfies the reliability condition with respect to $f$, it is easy to see the polynomial $p(\langle \bw^*,\bx\rangle)$ satisfies the requirements. This completes the proof of \Cref{lem:polynomial-correlation}.

\subsection{Proof of \Cref{lem:chow-subspace}}
We start by bounding the Frobenius norm of $\bT^m$, i.e., $\|\bT^m\|_F$. Notice that by taking $p:\R\to \R$ to be any degree-$m$  polynomial such that $\E_{\bx\sim \gaus_d}[p(\bx)]=0$ and $\E_{\bx\sim \gaus_d}[p(\bx)^2]=1$, then
$\|\bT^m\|_F=\max_{p} \E_{(\bx,y)\sim D}[\ind (y=-1)p(\bx)]$.
We leverage the following standard fact about the concentration of polynomials lemma known as Bonami-Beckner inequality or simply Gaussian hypercontractivity.
\begin{fact}[Hypercontractivity Concentration Inequality  \cite{Bog:98,nelson1973free}]\label{fct:hypercontractivity_concentration} 
Consider any $m$-degree, unit variance polynomial $p:\R^d\to \R$ with respect to $\gaus_d$. Then, for any $\lambda\geq 0$
\[
\pr_{\bx\sim \gaus_d} \left [\left |p(\bx)-\E_{\bu} [p(\bu)]\right |\geq \lambda\right ]\leq e^2 e^{-{\left (c\lambda^2\right )}^{1/m}}\; ,
\]
where $c$ is an absolute constant.
\end{fact}
Using \Cref{fct:hypercontractivity_concentration}, we have that for any zero mean and unit variance polynomial $p$,
\begin{align*}
\E_{(\bx,y)\sim D}[\ind(y=-1)p(\bx)]&\leq \E_{(\bx,y)\sim D}[\ind(y=-1)|p(\bx)|]\leq \int_{0}^\infty
\min\left (\gamma, e^2 e^{-{\left (c\lambda^2\right )}^{1/m}}\right )d\lambda\\
&=O\left (\gamma \log (1/\gamma)^{m/2}\right )\; .
\end{align*}
Therefore, $\|\bT^m\|_F=  O\left (\gamma \log (1/\gamma)^{m/2}\right )$.

To prove the Property 1, since $\|\bT'^m\|_F\leq \|\bT^m\|_F+\tau/(4\sqrt{k})=O\left (\gamma \log (1/\gamma)^{m/2}\right )+\tau/(4\sqrt{k})$, there can be at most $\|\bT^m\|_F^2/\left (\tau/\sqrt{k}\right )^2=O(\gamma^2 \log (1/\gamma)^m k/\tau^2+1)$ many singular vectors with singular values greater than $\tau/(4\sqrt{k})$.
Therefore, $\dim(V)$ is at most $O(\gamma^2 \log (1/\gamma)^m k/\tau^2+1)$.

For the Property 2, suppose $\|\proj_V(\mathbf{v^*})\|_2= o\left (\tau/\left (\sqrt{k}\gamma \log (1/\gamma)^{k/2}\right )\right )$, then we have for any $m$,
\begin{align*}
\|{\bv^*}^{\top}\bT'^m\|_2
&\leq \|{\bv^*}^{\top}\bT^m\|_2+\tau/(4\sqrt{k})\\
&\leq \|\bT^m\|_F\|\proj_{V} (\bv^*)\|_2
+\tau/(4\sqrt{k})\|\proj_{\perp V} (\bv^*)\|_2+\tau/(4\sqrt{k})\leq (5/8)\tau/\sqrt{k}\; . 
\end{align*}
However, since $\E_{(\bx,y)\sim D}[\ind(y=-1) p(\langle\mathbf{v^*,x} \rangle)]=\sum_{m=1}^k a_i{\bv^*}^{\top}\bT^m{\bv^*}^{\otimes {m-1}}\geq \tau$ for some $a_1^2+\cdots+a_k^2=1$,
we know there must be an $m$ such that 
\[
\left |{\bv^*}^{\top}\bT^m{\bv^*}^{\otimes {m-1}}\right |\geq \tau/\sqrt{k}\; .
\]
Therefore, we can write 
\[
\|{\bv^*}^{\top}\bT'^m\|_2\geq \|{\bv^*}^{\top}\bT^m\|_2-\tau/(4\sqrt{k})\geq \tau/\sqrt{k}-\tau/(4\sqrt{k})=(3/4)\tau/\sqrt{k}\; ,
\]
which contradicts $\|{\bv^*}^{\top}\bT'^m\|_2\leq (5/8)\tau/\sqrt{k}$. This completes the proof.


\subsection{Proof of \Cref{lem:finding-nontrivial-direction}}

The pseudocode for the algorithm 
establishing~\Cref{lem:finding-nontrivial-direction} is given 
in \Cref{alg:get-correlational-vector}.

\medskip 

\begin{algorithm}
    \caption{Finding a Direction with High Correlation.}
    \label{alg:get-correlational-vector}
    \centering\fbox{\parbox{5.8in}{
        \textbf{Input:} Sample access to a joint distribution $D$ of $(\bx,y)$ supported on $\R^d\times \{\pm 1\}$ with the marginal $D_{\bx}=\gaus_d$. Let $\eps,\delta\in(0,1)$ and suppose $D$ satisfies the reliability condition with respect to an LTF 
        $f(\bx)=\sign(\langle\bw^*,\bx\rangle-t^*)$ with $t^*=O\left (\sqrt{\log(1/\eps)}\right )$ and $\pr_{(\bx,y)\sim D} [y=-1]\geq \eps$.
        
        \textbf{Output:} With probability at least $\Omega(1)$, the algorithm outputs a unit vector 
        $\bv$ such that 
        $\langle\bv,\bw^*\rangle=\max\left (\log(1/\eps)^{-O({t^*}^2)},\eps^{O(1)}\right )$ .
        \begin{enumerate} [leftmargin=1cm]
            \item [1. ] 
            Let $c_1$ be a sufficiently large universal constant and $c_2$ be a sufficiently large universal constant depending on $c_1$. 
            Let $S$ be a set of $N=d^{c_2{\max({t^*}^2,1)}}\log(1/\delta)/\eps^2$ many samples from $D$.
            For $m=1,\cdots ,c_1{t^*}^2$, with 1/2 probability, take 
            \[ \bT'^m=\E_{(\bx,y)\sim_u S}[\ind(y=-1)\bH^m(\bx)]\; ,\]
            to be the empirical Chow-tensor on negative samples.
            Otherwise, take 
            \[ \bT'^m=\E_{(\bx,y)\sim_u S}[y\bH^m(\bx)]\; ,\]
            to be the empirical Chow-tensor on all samples.
            Let $\gamma$ be the empirical estimation of $\pr_{(\bx,y)\sim D}[y=-1]$ with error at most $\eps/100$ with a sufficiently small constant failure probability.
            \item [2. ] Take $\tilde \bT'^m\in \R^{d\times{d^{m-1}}}$ to be the flattened version of $\bT'^m$, and let $V_m$ be the subspace spanned by the left singular vectors of $\tilde \bT'^m$ whose singular values are greater than 
            $2^{-c{t^*}^2}\gamma$ where $c$ is a sufficiently large constant.
            Let $V$ be the union of all $V_m$ and output $\bv$ to be a random unit vector chosen from $V$.
        \end{enumerate}
        }}
\end{algorithm}

\noindent We can now prove \Cref{lem:finding-nontrivial-direction}.

\begin{proof} [Proof of \Cref{lem:finding-nontrivial-direction}]
We first introduce the following fact about learning Chow tensors.
    \begin{fact} \label{fct:chow_parameter_approximation}
        Fix $m\in \Z_+$, and $\eps,\delta\in (0,1)$. Let $D$ be a distribution on $\R^d\times [\pm 1]$ with standard normal marginals. There is an algorithm that with $N=d^{O(m)}\log(1/\delta)/\eps^2$ samples and $\poly(d,N)$ runtime, outputs an approximation $\bT'^m$ of the order-$m$ Chow-parameter tensor $\bT^m=\E_{(\bx,y)\sim D}[y\bH^{m}(\bx)]$ such that
        with probability $1-\delta$, it holds $\|\bT'^m-\bT^m\|_F\leq \eps$.
    \end{fact}
    \begin{proof}
        Let $\bT'^m$ be the empirical estimation of $\bT^m$ using $N=d^{cm}\log(1/\delta)/\eps^2$ for sufficiently large universal constant $c$.
        We start by showing $\E_{(\bx,y)\sim D}\left [\|y\bH^m(\bx)\|_F^2\right]\leq d^m$.
        Notice that,
        \[
        \E_{(\bx,y)\sim D}\left [\|y\bH^m(\bx)\|_F^2\right]
        =\E_{(\bx,y)\sim D}\left [y^2\|\bH^m(\bx)\|_F^2\right]
        \leq \E_{(\bx,y)\sim D}\left [\|\bH^m(\bx)\|_F^2\right]=\E_{\bx\sim \R^d}\left [\|\bH^m(\bx)\|_F^2\right]\; .
        \]
        Notice that each entry of $\bH^m(\bx)$ must be some $\alpha h(\bx)$ where $\alpha\in [0,1]$ and $h(\bx)$ is a unit variance Hermite polynomial.
        Therefore, $\E_{(\bx,y)\sim D}\left [ \|\bH^m(\bx)\|_F^2 \right ]\leq d^m$ and thus
        $\E_{(\bx,y)\sim D}\left [\|y\bH^m(\bx)\|_F^2\right]\leq d^m$.

        Using the above, we have 
        \[
        \E_{(\bx,y)\sim D}\left [\|y\bH^m(\bx)-\bT^m\|_F^2\right]=\E_{(\bx,y)\sim D}\left [\|y\bH^m(\bx)\|_F^2\right]-\|\bT^m\|_F^2\leq  d^m\; .
        \]
        Then, applying Chebyshev's inequality gives $\|\bT'^m-\bT^m\|_F\leq \eps$.      
    \end{proof}

    We prove \Cref{lem:finding-nontrivial-direction} for two cases: 
    (a) $\max(\log(1/\eps)^{-O({t^*}^2)},\eps^{O(1)})=\log(1/\eps)^{-O({t^*}^2)}$; and (b) $\max(\log(1/\eps)^{-O({t^*}^2)},\eps^{O(1)})=\eps^{O(1)}$.
    We first prove Case (a) as the other case is similar. 
    For Case (a), it suffices for us to prove that $\langle \bv,\bw^*\rangle=\log(1/\eps)^{-O({t^*}^2)}$ happens with $\Omega(1)$ probability given the algorithm chooses
    $\bT'^m=\E_{(\bx,y)\sim_u S}[\ind(y=-1)\bH^m(\bx)]$
    (which happens with 1/2 probability).
    Let $k=c_1{t^*}^2$ and $\tau/(4\sqrt{k})=2^{-c{t^*}^2}\gamma$.
    By \Cref{fct:chow_parameter_approximation}, we have $\|\bT'^m-\bT^m\|_F\leq \tau/(4\sqrt{k})$.
    Then from \Cref{lem:polynomial-correlation} and \Cref{lem:chow-subspace}, 
    if we take $\bv$ to be a random vector in $V$, we will with constant probability have,
    \begin{align*}        
    \langle \bv ,\bw^*\rangle &= \frac{\Omega\left (\tau/\left (\sqrt{k}\gamma \log (1/\gamma)^{k/2}\right )\right )}{\dim(V)^{1/2}}=\Omega\left (\tau^2/\left (\log(1/\gamma)^kk^{3/2}\gamma^2\right )\right ) \\
    &=\Omega\left (\log(1/\gamma)^{-O({t^*}^2)}{k}^{-3/2}\right )\; .
    \end{align*}
    Since $\pr_{(\bx,y)\sim D} [y=-1]\geq \eps$, we have
    $\gamma=\Omega(\eps)$. 
    Also, considering $k=O(\max\{{t^*}^2,1\})=O\left (\log(1/\eps)\right )$, we get
    $\langle \bv ,\bw^*\rangle=\log(1/\eps)^{-O({t^*}^2)}$.

    For Case (b), the proof remains the same except we take $\bT'^m=\E_{(\bx,y)\sim_u S}[y\bH^m(\bx)]$ and \new{use fact below instead of \Cref{lem:chow-subspace}.}
    \begin{fact} [Lemma 5.10 in \cite{Diakonikolas2022b}] \label{lem:chow-subspace-variant}
    Let $D$ be the joint distribution of $(\bx,y)$ supported on $\R^d\times \{ \pm 1\}$ with 
    the marginal $D_{\bx}=\gaus_d$.
    Let $p:\R\mapsto\R$ be a univariate, mean zero, unit variance polynomial of degree $k$ such that 
    for some unit vector $\mathbf{v^*}\in \R^d$ it holds 
    $\E_{(\bx,y)\sim D}[y p(\langle\mathbf{v^*,\bx} \rangle)]\geq \tau$ for some $\tau\in (0,1]$. 
    Let $\bT'^m$ be an approximation of the order-$m$ Chow-parameter tensor $\bT^m=\E_{(\bx,y)\sim D}[y{\bf H}_m(\bx)]$ such that 
    $\|\bT'^m-\bT^m\|_F\leq \tau/(4\sqrt{k})$. 
    Denote by $V_m$ the subspace spanned by the left singular vectors of flattened $\bT'^m$ whose 
    singular values are greater than $\tau/(4\sqrt{k})$. 
    Moreover, denote by $V$ the union of $V_1,\cdots,V_k$. Then we have that 
    \begin{enumerate}[leftmargin=*]
	   \item $\dim(V)\leq 4k/\tau^2$, and 
	   \item $\|\proj_V(\mathbf{v^*})\|_2\geq \tau/\left (4\sqrt{k}\right )$.
    \end{enumerate}
    \end{fact}   
    Then the same argument will give $\langle \bv ,\bw^*\rangle=\eps^{O(1)}$.
    The above described algorithm uses at most $N=d^{O({t^*}^2)}/\eps^2$ samples and $\poly\left (N,1/\eps\right )$ runtime. This completes the proof of \Cref{lem:finding-nontrivial-direction}.
\end{proof}


\subsection{Proof of \Cref{lem:reduction_satisfies_reliability}}
     \Cref{prop:1} follows from the definition of $D'$ and the fact that $D$ has marginal distribution $D_\bx=\gaus_d$.

    For proving \Cref{prop:2},
    we consider two cases: The first case is when $t^*>0$ and the second one when $t^*\leq 0$.
    For the case $t^*\leq 0$, we prove that the distribution $D'$ satisfies the reliability condition with respect to $h'(\bx)=\sign(\langle\bw',\bx\rangle)$ where $\bw'={\bw^*}^{\perp \bw}/\|{\bw^*}^{\perp \bw}\|_2$.
    Notice that for any $(\bx,y)$ such that $\bx\in B$ and $h'(\bx^{\perp \bw})=1$, we have 
    \begin{align*}
    f(\bx)&=\sign(\langle \bw^*,\bx\rangle-t^*)\\
    &=\sign(\langle\proj_{\perp \bw}(\bw^*),\bx\rangle+\langle \proj_{\bw}(\bw^*),\bx\rangle-t^*)\\
    &=\sign(\langle {\bw^*}^{\perp \bw},{\bx}^{\perp \bw}\rangle+\langle \proj_{\bw}(\bw^*),\bx\rangle-t^*)\; ,
    \end{align*}
    where $\langle {\bw^*}^{\perp \bw},{\bx}^{\perp \bw}\rangle\geq 0$ since $h'(\bx)=\sign(\langle {\bw^*}^{\perp \bw},{\bx}^{\perp \bw}\rangle/\|{\bw^*}^{\perp \bw}\|_2)\geq 0$.
    Then we have
    \begin{align}    
    f(\bx)&\geq \sign(\langle \proj_{\bw}(\bw^*),\bx\rangle-t^*) \nonumber\\
    &= \sign(\langle \proj_{\bw}(\bw^*)/\|\proj_{\bw}(\bw^*)\|_2,\bx\rangle-t^*/\|\proj_{\bw}(\bw^*)\|_2 \nonumber\\
    &\geq  \sign(\langle \bw,\bx\rangle-t^*) \label{eq:step-3}\\
    &\geq  \sign(\langle \bw,\bx\rangle-t) \nonumber\\
    &=h(\bx)= +1 \nonumber\; ,
    \end{align}
    where Inequality \eqref{eq:step-3} follows from the fact that 
    $\langle \bw,\bw^*\rangle> 0$, $\|\proj_{\bw}(\bw^*)\|_2\in (0,1]$ and $t^*\leq 0$.
    Since $D$ satisfies the reliability condition with respect to $f$, we have that it must be the case
    $y=+1$. Therefore, $D'$ satisfies the reliability condition with respect to $h'$.

    For the case $t^*> 0$, we prove that the distribution $D'$ satisfies the reliability condition with respect to $h'(\bx)=\sign(\langle\bw',\bx\rangle-t')$ where $\bw'={\bw^*}^{\perp \bw}/\|{\bw^*}^{\perp \bw}\|_2$ and $t'=t^*$.
    Notice  that for any $(\bx,y)$ such that $\bx\in B$ and $h'(\bx^{\perp \bw})=1$, we have 
    \begin{align*}
    f(\bx)&=\sign(\langle \bw^*,\bx\rangle-t^*)\\
    &=\sign(\langle\proj_{\perp \bw}\bw^*,\bx\rangle+\langle \proj_{\bw}\bw^*,\bx\rangle-t^*)\\
    &=\sign\left (\|{\bw^*}^{\perp \bw}\|_2\langle \bw',\bx\rangle-\|{\bw^*}^{\perp \bw}\|_2t^*+\langle \proj_{\bw}\bw^*,\bx\rangle-(1-\|{\bw^*}^{\perp \bw}\|_2)t^*\right )\; ,
    \end{align*}
    where $\|{\bw^*}^{\perp \bw}\|_2\langle \bw',\bx\rangle-\|{\bw^*}^{\perp \bw}\|_2t^*\geq 0$
    since $h'(\bx)=+1$.
    Then we have
    \begin{align*}
    f(\bx)&\geq \sign(\langle \proj_{\bw}\bw^*,\bx\rangle-(1-\|{\bw^*}^{\perp \bw}\|_2)t^*)\\
    &=\sign\left (\sqrt{1-\|{\bw^*}^{\perp \bw}\|_2^2}\; \langle \bw,\bx\rangle-(1-\|{\bw^*}^{\perp \bw}\|_2)t^*\right )\\
    &\geq \sign\left (\sqrt{1-\|{\bw^*}^{\perp \bw}\|_2^2}\; (\langle \bw,\bx\rangle-t^*)\right )= h(\bx)=1\; ,
    \end{align*}
    where the second equation follows from $\langle \bw,\bw^*\rangle> 0$ and the third one follows from $t^*> 0$. 
    Since $D$ satisfies the reliability condition with respect to $f$, we have that it must be the case
    $y=+1$. Therefore, $D'$ satisfies the reliability condition with respect to $h'$.


    For proving \Cref{prop:3}, notice that $R_+(h;D)\geq \eps/2$ implies 
    \[
    \pr_{(\bx',y)\sim D'} [y=-1]=R_+(h;D)/\pr_{(\bx,y)\sim D}(\bx\in B)\geq \eps/2\; .
    \]
    This completes the proof.    

\subsection{Proof of \Cref{thm:main-algorithm-known-alpha}}
We first give the algorithm in \Cref{thm:main-algorithm-known-alpha}, which is a more detailed version of \Cref{alg:random-walk-reliable}.

\medskip

\begin{algorithm} [H]
    \caption{Reliably Learning General Halfspaces with Gaussian Marginals (detailed version of \Cref{alg:random-walk-reliable}).}
    \label{alg:random-walk-reliable-detail}
    \centering\fbox{\parbox{5.8in}{
        \textbf{Input:} $\eps\in (0,1)$, $\alpha\in (0,1/2)$ and sample access to a joint distribution $D$ of $(\bx,y)$ supported on $\R^d\times \{\pm 1\}$ with $\bx$-marginal $D_{\bx}=\gaus_d$. 
        
        \textbf{Output:} $h(\bx)=\sgn(\langle \bw,\bx\rangle-t)$ that is $\eps$-reliable with respect to the class $\mathcal{H}_d^\alpha$.

        \begin{enumerate} [leftmargin=8mm]
            \item  \label{step1-app}
            Check if $\pr_{(\bx,y)\sim D} [y=-1]\leq \eps/2$ (with sufficiently small constant failure probability). If so, return the $+1$ constant hypothesis.
            If the parameters satisfy
            \[
            \min\left (2^{\log(1/\eps)^{O(\log (1/\alpha))}},2^{\poly(1/\eps)}\right )=2^{\log(1/\eps)^{O(\log (1/\alpha))}}\; ,
            \]
            then set the correlation parameter $\zeta=\log(1/\eps)^{-c{t^*}^2}$, where $c$ is a sufficiently large universal constant.            
            Otherwise, set $\zeta=\eps^{c}$ for a sufficiently large universal constant $c$. 
            For $t=-\Phi^{-1}(\alpha)$ and $t=\Phi^{-1}(\alpha)$, do Steps \eqref{step2-app} and \eqref{step3-app}.  
            
            \item  \label{step2-app}
            Initialize $\bw$ to be a random unit vector in $\R^d$.
            Let the update step size $\lambda=\zeta$ and repeat the following process until 
            $\lambda\leq \eps/100$. 
            \begin{enumerate} 
                \item \label{step2a-app}
                Use samples from $D$ to check if the hypothesis $h(\bx)=\sign(\langle \bw,\bx\rangle-t)$ satisfies $R_+(h;D)\leq \epsilon/2$.
                If so, go to Step \eqref{step3-app}.
                
                \item  \label{step2b-app}
                With $1/2$ probability, let $\bw=-\bw$. Let $B=\{\bx\in \R^d :\langle\bw,\bx\rangle-t\geq 0\}$, and let $D'$ be the distribution of $(\bx^{\perp \bw},y)$ for $(\bx,y)\sim D$ given $\bx\in B$.
                Use the algorithm of \Cref{lem:finding-nontrivial-direction} on $D'$ to find a unit vector $\bv$ such that $\langle \bv,\bw\rangle=0$ and
                $\left \langle \bv, \frac{\proj_{\perp \bw}(\bw^*)}{\|\proj_{\perp \bw}(\bw^*)\|_2}\right \rangle\geq \zeta$.
                Then, update $\bw$ as follows: 
                $
                \bw_{\mathrm{update}}=\frac{\bw+\lambda\bv}{\|\bw+\lambda\bv\|_2}\; .
                $
                

                \item \label{step2c-app}
                Repeat Steps \eqref{step2a-app} and \eqref{step2b-app} $c/\zeta^2$ times, where $c$ is a sufficiently large universal constant, with the same step size $\lambda$.
                After that, update the new step size as $\lambda_{\mathrm{update}}=\lambda/2$.
                
            \end{enumerate}  
            \item  \label{step3-app} Check if $h(\bx)=\sign(\langle \bw,\bx\rangle-t)$ satisfies $R_+(h;D)\leq \eps/2$. If so, \new{add} it to the set $S$.
            For each choice of value $t$ in Step \eqref{step1-app}, repeat Step \eqref{step2-app}
            $2^{1/\zeta^c}$ many times where $c$ is a sufficiently large constant. 
            
            \item Let $S$ be the set in Step \eqref{step3-app}.
            Return the hypothesis $h(\bx)=\sign(\langle \bw,\bx\rangle-t)$, 
            where $(\bw,t)=\argmin_{(\bw,t)\in S} t$. Return the $-1$ constant hypothesis if $S$ is empty.
        \end{enumerate}
        }}
\end{algorithm}



\begin{proof} [Proof of \Cref{thm:main-algorithm-known-alpha}]
    Without loss of generality, we assume $\eps> \alpha$ for the following reason.
    Suppose $\alpha\leq \eps$, then we can simply return one of the constant hypotheses that is closest to $f$. 
    Let 
    $f(\bx)=\sign(\langle\bw^*,\bx\rangle-t^*)$ be the optimal LTF with $\alpha$ bias, namely, 
    \[
    f=\argmin_{f\in \mathcal{H}_d^\alpha\land R_+(h;D)=0} R_-(f;D)\; .
    \] 
    We prove that with high probability, \Cref{alg:random-walk-reliable-detail} returns a hypothesis $h(\bx)=\sign(\langle\bw,\bx\rangle-t)$ such that $R_+(h;D)\leq \eps$ and $R_-(h;D)\leq R_-(f;D)+\eps$.
    Notice that at some point, we will run Step \eqref{step2-app} with $t=t^*$.
    We show that given $t=t^*$, Step \eqref{step2-app} will with probability $2^{-\poly(1/\zeta)}$ 
    returns a $h$ with $R_+(h;D)\leq \eps/2$.
    For convenience, we assume $h$ never satisfies $R_+(h;D)\leq \eps/2$ in Step \eqref{step2a-app} (otherwise, we are done).
    Furthermore, we assume the subroutine algorithm in \Cref{lem:finding-nontrivial-direction}
    used in Step \eqref{step2b-app} always succeed, and we always have $\langle \bw,\bw^*\rangle\geq 0$, since both happen with constant probability
    and we are running the update at most $O(\log(1/\eps)/\zeta^2)=\poly(1/\zeta)$ many times.
    
    Let $\eta=\|\proj_{\perp \bw}\bw^*\|_2$.
    Now, we will prove by induction that each time after $c/\zeta^2$ many updates in step \eqref{step2b-app},
    we always have $\|\proj_{\perp \bw}\bw^*\|_2\leq 3\lambda$ (without loss of generality, we assume $\lambda$ is always at most a sufficiently small constant).
    Notice that by \Cref{lem:reduction_satisfies_reliability} and \Cref{lem:finding-nontrivial-direction}, we have that at each update, given the subroutine algorithm in \Cref{lem:finding-nontrivial-direction} succeeds,
    the update direction $\bv$ always satisfies $\left \langle \bv, \frac{\proj_{\perp \bw}(\bw^*)}{\eta}\right \rangle\geq \zeta$, which implies $\left \langle \bv, \proj_{\perp \bw}(\bw^*)\right \rangle\geq \eta\zeta$. 
    Then we have $\langle\bw,\bw^*\rangle=\sqrt{1-\eta^2}\geq 1-\eta^2$.    
    When we update $\bw$, there are two possibilities. Either $\lambda> \eta\zeta/2$ or $\lambda\leq  \eta\zeta/2$.
    If $\lambda\leq  \eta\zeta/2$, using \Cref{lem:coorelation-improvement}, 
    we will have 
    $\langle \bw_{\text{update}},\bw^*\rangle\geq \langle \bw,\bw^*\rangle+\lambda^2/2$.
    If $\lambda> \eta\zeta/2$, we have
    \begin{align*}
    \left \|\proj_{\perp \bw_{\text{update}}}(\bw^*)\right\|_2
    =&\left \|\proj_{\perp \bw^*}(\bw_{\text{update}})\right\|_2
    \leq \left \|\proj_{\perp \bw^*}(\bw+\lambda\bv)\right\|_2\\
    \leq &\left \|\proj_{\perp \bw^*}(\bw)\right\|_2+\lambda
    \leq 3\lambda\; .
    \end{align*}
    For the base case, when we do the first group of $c/\zeta^2$ many updates, combining the two cases above
    and the fact that $\lambda=\zeta\geq \eta\zeta$ gives that
    after $c/\zeta^2$ many updates, we must have $\|\proj_{\perp \bw}\bw^*\|_2\leq 3\lambda$.
    For the induction case, given $\eta\leq 6\lambda$ (which follows from the previous group of $c/\zeta^2$ many updates),
    combining the two cases above
    gives that
    after $c/\zeta^2$ many updates, we must have $\|\proj_{\perp \bw}\bw^*\|_2\leq 3\lambda$.
    This proves that after each group of $c/\zeta^2$ many updates, 
    we have $\|\proj_{\perp \bw}\bw^*\|_2\leq 3\lambda$.
    
    Therefore, when we have $\lambda\leq \eps/100$, we get $\|\proj_{\perp \bw}\bw^*\|_2\leq 3\eps/100$, which implies $R_+(h;D)\leq \eps/2$
    since $D$ satisfies the reliability condition with respect to $f$.
    Given $R_+(h;D)\leq \eps/2$, using the fact that $t-t^*\leq 0$ (since we always output the hypothesis with the smallest $t$) implies $\pr_{(\bx,y)\sim D}[h(\bx)=1]\geq \pr_{(\bx,y)\sim D}[f(\bx)=1]$, we have that
    \begin{align*}
        R_-(h;D) &=\pr_{(\bx,y)\sim D}[y=1]-\pr_{(\bx,y)\sim D}[y=1\land h(\bx)=1]\\
        &=\pr_{(\bx,y)\sim D}[y=1]-\pr_{(\bx,y)\sim D}[h(\bx)=1]+R_+(h;D)\\
        &\leq \pr_{(\bx,y)\sim D}[y=1]-\pr_{(\bx,y)\sim D}[f(\bx)=1]+\eps= R_-(f;D)+\eps\; .
    \end{align*}
    This completes the proof.   
\end{proof}




\section{Omitted Proofs from \Cref{sec:lb}} \label{app:lb}
We first restate our main SQ hardness result.

\begin{theorem}[SQ Lower Bound for Reliable Learning] \label{thm:sq-reliable}
For any $\alpha>0$ that is at most a sufficiently small absolute constant and $\eps\in (0,\alpha/3)$, 
any SQ algorithm that reliably learns $\alpha$-biased LTFs (for positive labels) on $\R^d$ under Gaussian marginals 
to additive error $\eps$ either
(i) requires at least one query of tolerance at most $d^{-\Omega\left (\log\left (\frac{1}{\alpha}\right )\right )}$, or 
(ii) requires at least $2^{d^{\Omega(1)}}$ queries.
\end{theorem}



\subsection{Proof of \Cref{lem:polynomial}}
We let $P_n$ denote the set of all polynomials $p:\R\to \R$ of degree at most $n$ and let
$L^1_+(\R)$ denote the set of all nonnegative functions in $L^1(\R)$.
First, we write the conditions as the primal linear program.
\begin{alignat*}{5}
    &\text{find} \quad &g\in L^1(\R)\\
    &\text{such that} \quad & \E_{z\sim \gaus_1}[p(z)g(z)] &= 0 \; ,& \forall p&\in P_n\\
    &             \quad & g(z) & \geq 1\; , & \quad \forall z&\geq c\\
    &             \quad &\|g\|_\infty&\leq 1& \quad\\
\end{alignat*}
By introducing variables $H\in L^1(R)$ and $h\in L_+^1(R)$, we rewrite the primal LP as
\begin{alignat*}{7}
    &\text{find} \quad& g\in L^1(\R)\\
    &\text{such that}\quad& \E_{z\sim \gaus_1}[p(z)g(z)] &= 0\; ,& \forall p&\in P_n\\
    &      \quad&\E_{z\sim \gaus_1}[g(z)h(z)\1\{z\geq c\}] & \geq \|h(z) \1\{z\geq c\}\|_1\; , & \quad \forall h&\in L_+^1(\R)                \\
    &      \quad&\E_{z\sim \gaus_1}[g(z)H(z)]&\leq \|H\|_1\; ,   &\quad \forall H&\in L^1(\R)\\
\end{alignat*}

The following fact is an application of (an infinite generalization of) 
LP duality:

\begin{claim} \label{fct:nonexist-polynomial}
The LP above is feasible if there exists no polynomial $p$ of degree $n$ such that
\[\E_{z\sim \gaus_1}[|p(z)|\mathds{1}(z\leq c)]<\E_{z\sim \gaus_1}[p(z)\mathds{1}(z\geq c)]\; .\]
\end{claim}
\begin{proof}[Proof of~\Cref{fct:nonexist-polynomial}]
        First, we introduce some notation.
        We use $(\tilde{h}, c)$ for the inequality $\E_{z \sim \normal}[\beta(z) \tilde{h}(z)] +c\leq 0$,
        where $\tilde{h}\in L^1(\R)$ and $c\in \R$. Moreover, let $\cal S$ be the set that contains all such tuples that describe the target system. For the set $\cal S$, the closed convex cone over $L^1(\R)\times \R$ is the smallest closed set ${\cal S}_+$ satisfying the following:
        (a) if $A\in {\cal S}_+$ and $B\in {\cal S}_+$ then $A+B \in {\cal S}_+$;  and
        (b) if $A\in {\cal S}_+$ then $\lambda A \in {\cal S}_+$ for all $\lambda\geq 0$.       
        Note that the ${\cal S}_+$ contains the same feasible solutions as $\cal S$.
        In order to prove the statement, we need the following LP duality from \cite{Fan68}.
        \begin{fact}[Theorem 1 of \cite{Fan68}]\label{fct:fan}
            If $\cal X$ is a locally convex, real separated vector space. Then, a linear system described by $\cal S$ is feasible (i.e., there exists a $g\in {\cal X}^*$) if and only if $(0,1)\not\in {\cal S}_+$.
        \end{fact}
        Our dual LP is defined by the following inequalities: 
        \begin{enumerate}[leftmargin=*]
            \item [(a)] $(p,0)$ for $p\in {\cal P}_{n}$;
            \item [(b)] $(-\tilde h,-\|\tilde h\|_1)$ for all $\tilde h(z) =h(z)\1\{z\geq c\}$,
            where $h\in L_+^1(\R)$; and 
            \item [(c)] and  $(H,-\|H\|_1)$ for all $H\in L^1(\R)$. 
        \end{enumerate}
        Furthermore, the LP is also equivalent to breaking the last inequality into two, 
        i.e.,  
        $(\tilde H_1,-\|\tilde H_1\|_1)$
        for all $\tilde H_1(z) =H_1(z)\1\{z< c\}$, where $H_1\in L^1(\R)$, 
        and $(\tilde H_2,-\|\tilde H_2\|_1)$
        for all $\tilde H_2(z) =H_2(z)\1\{z\geq c\}$ where $H_2\in L_+^1(\R)$.
        By taking the convex cone defined from the above inequalities, we get the following set
        \begin{align*}
        {\cal S}_+= \Big \{\left (p-\tilde h+\tilde H_1 +\tilde H_2,\|\tilde h\|_1- \|\tilde H_1+\tilde H_2\|_1\right )\Big |\;
        & p\in{\cal P}_{n},\\ 
        & \tilde h(z) =h(z)\1\{z\geq c\} \text{ for } h\in L_+^1(\R),\\
        &\tilde H_1(z) =H_1(z)\1\{z< c\} \text{ for } H_1\in L^1(\R), \\       
        &\tilde H_2(z) =H_2(z)\1\{z\geq c\} \text{ for } H_2\in L_+^1(\R) \Big\}\; .
        \end{align*}
        
        We first show that ${\cal S}_+$ is closed.
        To do so, we prove that ${\cal S}_+$ is closed under limits.
        First, we assume, in order to reach a contradiction, there is a sequence $(p_i,h_i, \tilde H_{1,i},\tilde H_{2,i})$
        so that 
        \[(F_i,\lambda_i)=\left (p_i-\tilde h_i+\tilde H_{1,i} +\tilde H_{2,i},\|\tilde h_i\|_1- \|\tilde H_{1,i}+\tilde H_{2,i}\|_1\right )\]
        has $F_i$ converges to $F_{\lim}$ with respect to $L_1$-norm and $\lambda_i$ converges to $\lambda_{\lim}$. We claim then that $\left (F_{\lim}, \lambda_{\lim}\right )\in {\cal S}_+$.
        First, notice that $\|p_i\|_1\leq \|F_i\|_1+\|\tilde h_i\|_1+\|\tilde H_{1,i}\|_1+\|\tilde H_{2,i}\|_1\leq \|F_i\|_1+3$.
        Therefore, there must be a $k$ such that for any $i\geq k$,
        $\|p_i\|_1\leq \|F_{\lim}\|_1+4$.

        Then, since an $L_1$ ball in $\mathcal P_n$ with radius $\|F_{\lim}\|_1+4$ is compact,
        there must be a subsequence of $(p_i,h_i, \tilde H_{1,i},\tilde H_{2,i})$ such that 
        $p_i$ converges to $p_{\lim}$ for some $p_{\lim} \in {\cal P}_{n}$.
        Notice that due to the positivity of $\tilde h_i$ and $\tilde H_{2,i}$, it must be 
        $\tilde h_i=(F_i-p_i)^{-}\1(z\geq c)$, $\tilde H_{2,i}=(F_i-p_i)^{+}\1(z\geq c)$
        and $\tilde H_{1,i}=(F_i-p_i)\1(z< c)$.
        Therefore, $\tilde h_i$, $\tilde H_{1,i}$ and $\tilde H_{2,i}$ converge to
        $\tilde h_{\lim}=(F_{\lim}-p_{\lim})^{-}\1(z\geq c)$, $\tilde H_{2,\lim}=(F_{\lim}-p_{\lim})^{+}\1(z\geq c)$
        and $\tilde H_{1,\lim}=(F_{\lim}-p_{\lim})\1(z< c)$ respectively.
        Then by taking $(p_{\lim},h_{\lim}, \tilde H_{1,{\lim}},\tilde H_{2,{\lim}})$,
        one can see that $\left (F_{\lim}, \lambda_{\lim}\right )\in {\cal S}_+$.

        It only remains to verify that if there exists a polynomial $p$ of degree $n$ such that
        \[\E_{z\sim \gaus_1}[|p(z)|\mathds{1}(z\leq c)]<\E_{z\sim \gaus_1}[p(z)\mathds{1}(z\geq c)]\; ,\]
        then $(0,1)\in {\cal S}_+$.
        Let $p$ be the polynomial satisfying the above. We can simply take
        $\tilde h_i=(p_i)^{+}\1(z\geq c)$, 
        $\tilde H_{2,i}=(p_i)^{-}\1(z\geq c)$
        and $\tilde H_{1,i}=-p_i\1(z< c)$.
        This gives 
        \[\left (p_i-\tilde h_i+\tilde H_{1,i} +\tilde H_{2,i},\|\tilde h_i\|_1- \|\tilde H_{1,i}+\tilde H_{2,i}\|_1\right )=(0,k) \in  {\cal S}_+\; ,\]
        where 
        \begin{align*}
            k
            &=\|(p_i)^{+}\1(z\geq c)\|_1-\|(p_i)^{-}\1(z\geq c)-p_i\1(z< c)\|_1\\
            &=\|(p_i)^{+}\1(z\geq c)\|_1-\|(p_i)^{-}\1(z\geq c)\|_1-\|p_i\1(z< c)\|_1\\
            &=\E_{z\sim \gaus_1}[p(z)\mathds{1}(z\geq c)]-\E_{z\sim \gaus_1}[|p(z)|\mathds{1}(z\leq c)]
            >0
        \end{align*}
        Then, by properly rescaling $(0,k)$, we get $(0,1)\in {\cal S}_+$.
        This completes the proof.
\end{proof}

Given \Cref{fct:nonexist-polynomial}, to prove \Cref{lem:polynomial}, it only remains to show that
there does not exist a polynomial $p$ of degree at most $n$ such that 
\[\E_{z\sim \gaus_1}[|p(z)|\mathds{1}(z\leq c)]<\E_{z\sim \gaus_1}[p(z)\mathds{1}(z\geq c)]\; .\]
First, the above condition implies that
$\E_{z\sim \gaus_1}[|p(z)|]<2\E_{z\sim \gaus_1}[|p(z)|\mathds{1}(z\geq c)]$.
Using the CauchySchwarz inequality, we get
\begin{equation}\label{ieq:cauchy}
\E_{z\sim \gaus_1}[|p(z)|]
	<2\E_{z\sim \gaus_1}[|p(z)|^2]^{1/2}\left (\pr_{z\sim \gaus_1}[z\geq c]\right )^{1/2}\;.
\end{equation}
We then give the following claim.
\begin{claim} \label{fct:norm-inequal}
Let $p:\R\mapsto\R$ be any polynomial of degree at most $n$. Then, it holds that
\[\E_{z\sim \gaus_1}[|p(z)|^2]^{1/2}\leq 3^n\E_{z\sim \gaus_1}[|p(z)|]\;.\]
\end{claim}
\begin{proof} [Proof for \Cref{fct:norm-inequal}]
The proof is based on the following well-known fact: for any function $g:\R\mapsto\R$, 
it holds that (from Holder's inequality)
\[\E_{z\sim \gaus_1}[|g(z)|^2]=\E_{z\sim \gaus_1}[|g(z)|^{2/3}|g(z)|^{4/3}]\leq 
	\E_{z\sim \gaus_1}[|g(z)|]^{2/3}\E_{z\sim \gaus_1}[|g(z)|^4]^{1/3}\; ,\]
thus
\begin{equation}\label{eqn:1}  
\E_{z\sim \gaus_1}[|g(z)|^2]^{1/2}\leq 
	\E_{z\sim \gaus_1}[|g(z)|]^{1/3}\E_{z\sim \gaus_1}[|g(z)|^4]^{1/6}\; .
\end{equation}
We then use \Cref{lem:hc} (Gaussian Hypercontractivity) to bound the term $\E_{z\sim \gaus_1}[|g(z)|^4]^{1/6}$.
\begin{fact} [Gaussian Hypercontractivity] \label{lem:hc}
Let $p:\R\mapsto \R$ be any polynomial of degree at most $l$. 
Then, for $q\geq 2$, 
it holds 
\[\E_{x\sim \gaus_1}[|p(x)|^q]\leq (q-1)^{ql/2}\left (\E_{x\sim \gaus_1}[p^2(x)]\right )^{q/2}\; .\]
\end{fact}
Now we apply \Cref{lem:hc} and choose $l=n$ and $q=4$.
This implies 
$\E_{z\sim \gaus_1}[|p(z)|^4]\leq 3^{2n}\E_{z\sim \gaus_1}[|p(z)|^2]^2$,
which is 
$\E_{z\sim \gaus_1}[|p(z)|^4]^{1/6}\leq 3^{n/3}\E_{z\sim \gaus_1}[|p(z)|^2]^{1/3}$.
Plugging it into \Cref{eqn:1}, we get that for any polynomial $p$ of at most degree-$n$,
\[\E_{z\sim \gaus_1}[|p(z)|^2]^{1/2}\leq 
	3^{n/3}\E_{z\sim \gaus_1}[|p(z)|]^{1/3}\E_{z\sim \gaus_1}[|p(z)|^2]^{1/3}\; .\] 
This implies 
\[\E_{z\sim \gaus_1}[|p(z)|^2]^{1/2}\leq 3^n\E_{z\sim \gaus_1}[|p(z)|]\;.\]
\end{proof}
Applying \Cref{fct:norm-inequal} to \Cref{ieq:cauchy}, we get
\[\E_{z\sim \gaus_1}[|p(z)|]
	<2\cdot 3^n\E_{z\sim \gaus_1}[|p(z)|]\left( \pr_{z\sim \gaus_1}[z\geq c]\right )^{1/2}\;.\]
From our choice of the parameter $c$, we have that $\pr_{z\sim \gaus_1}[z\geq c]\leq 3^{-2n}/4$, 
thus 
\[\E_{z\sim \gaus_1}[|p(z)|]<\E_{z\sim \gaus_1}[|p(z)|]\; ,\]
contradiction.
Therefore, such a polynomial $p$ cannot exist. 
Thus, a function $g$ satisfying the requirements in the body of the lemma exists.

\subsection{Proof of \Cref{lem:distribution}}
We let $D$ be the unique distribution such that $z\sim \gaus_1$ and $\E_{(z,y)\sim D}[y|z=z']=g(z')$ where 
$g$ is the function from \Cref{lem:polynomial}. 
We claim that $D$ satisfies the conditions in this lemma's statement.
Condition (i) is immediate from the definition of $D$.
Then Condition (ii) is immediately implied
from Condition (a) in \Cref{lem:polynomial}. 
	
For Condition (iii), Condition (b) in  \Cref{lem:polynomial} 
implies $\E_{z\sim \gaus_1}[g(z)]=0$
which immediately implies 
$\E_{(z,y)\sim D}[y]=0$. 
To show that 
\[\E_{(z,y)\sim D}[z^k]=\E_{(z,y)\sim D}[z^k\mid y=1]=\E_{(z,y)\sim D}[z^k\mid y=-1]\]  for all $k\in [n]$,
notice that for all $k\in [n]$,
\begin{equation}\label{eqn:first}
\E_{(z,y)\sim D}[z^k]
=\pr_{(z,y)\sim D}[y=1]\E_{(z,y)\sim D}[z^k\mid y=1]+\pr_{(z,y)\sim D}[y=-1]\E_{(z,y)\sim D}[z^k\mid y=-1]\;.
\end{equation}
Moreover, from Condition (ii) of \Cref{lem:polynomial}, we have
\begin{align} 
\E_{(z,y)\sim D}[g(z)z^k]=&\E_{(z,y)\sim D}[yz^k]\nonumber\\
=&\pr_{(z,y)\sim D}[y=1]\E_{(z,y)\sim D}[z^k\mid y=1]-\pr_{(z,y)\sim D}[y=-1]\E_{(z,y)\sim D}[z^k\mid y=-1] \nonumber\\
=& 0 \label{eqn:second}\; .
\end{align}
Taking the difference between \Cref{eqn:first} and \Cref{eqn:second} gives 
\[\E_{(z,y)\sim D}[z^k]=2\pr_{(z,y)\sim D}[y=-1]\E_{(z,y)\sim D}[z^k\mid y=-1]\; .\]
Plugging in that $\pr_{(z,y)\sim D}[y=-1]=1/2$, 
which follows from $\E_{(z,y)\sim D}[y]=0$ and $y$ is supported on $\{\pm 1\}$, 
we get
\[\E_{(z,y)\sim D}[z^k]=\E_{(z,y)\sim D}[z^k\mid y=-1]\; .\]
Then, we have
\[\E_{(z,y)\sim D}[z^k]=\E_{(z,y)\sim D}[z^k\mid y=1]\pr_{(z,y)\sim D}[y=1]+\E_{(z,y)\sim D}[z^k\mid y=-1]\pr_{(z,y)\sim D}[y=-1]\; ;\]
therefore,
\[
\E_{(z,y)\sim D}[z^k]=\E_{(z,y)\sim D}[z^k\mid y=-1]\; .
\]
Thus, Condition (iii) holds.

Now, we show the fourth condition.  We show the following claim.
\begin{claim} \label{clm:bounded-+-chi-squre}
It holds that
$\chi^2(D_+,\gaus_1)=O(1)\; .$
\end{claim}
\begin{proof}
\begin{align*}
\chi^2(D_+,\gaus_1)
&=\int_{\R} \frac{P_{D_+}(z)^2}{P_{\gaus_1}(z)}dz-1\\
&=\int_{\R} P_{D_+}(z)\frac{P_{D_+}(z)}{P_{\gaus_1}(z)}dz-1\\
&\leq \int_{\R} P_{D_+}(z)/\pr_{(z,y)\sim D_f}[y=1]dz-1\\
&= \pr_{(z,y)\sim D}[y=1]^{-1}\int_{\R} P_{D_+}(z)dz-1\\
&=  \pr_{(z,y)\sim D}[y=1]^{-1}-1=O(1)\; ,  
\end{align*}
where the inequality follows from the fact that 
\[
P_{\gaus_1}(z)=P_{D_+}(z)\pr_{(z,y)\sim D}[y=1]+P_{D_-}(z)\pr_{(z,y)\sim D}[y=-1]\geq P_{D_+}(z)\pr_{(z,y)\sim D}[y=1]\; ,
\]
which implies $P_{D_+}(z)/P_{\gaus_1}(z)\leq 1/\pr_{(z,y)\sim D}[y=1]$.
The same holds for $\chi^2(D_-,\gaus_1)$.
This completes the proof of \Cref{clm:bounded-+-chi-squre}.
\end{proof}
The proof of $\chi^2(D_-,\gaus_{1})=O(1)$ is similar to \Cref{clm:bounded-+-chi-squre}.
This completes the proof of \Cref{lem:distribution}.

\subsection{Construction of the Alternative Hypothesis Distribution Set $\D$}


\begin{lemma} \label{lem:distribution-set}
For any sufficiently small $\alpha>0$, 
there exists a distribution family $\D=\{D_\bv:\bv\in V\}$ supported on $\R^d\times \{\pm 1\}$ where $|\D|=2^{d^{\Omega(1)}}$ satisfying the following:
\begin{enumerate}[leftmargin=*]
	\item [(i)] For any $D_\bv\in \D$ and $(\bx,y)\sim D_\bv$,
	the marginal $D_{\bx}=\gaus_d$;
	
	\item [(ii)] For any $D_\bv\in \D$ and $(\bx,y)\sim D_\bv$,
	$\E_{(\bx,y)\sim D_\bv}[y|\bx=\bx']=1$ for all $\bx'\in \R^d$ 
	such that $\langle \bv, \bx'\rangle \geq\Phi^{-1}(1-\alpha)$; 
	
	\item [(iii)] Let $D_\nul$ be the joint distribution of $(\bx,y)$ such that 
	$\bx\sim \gaus_d$ and $y=1$ with probability $1/2$ independent of $\bx$. 
	\new{
	Then for any $D_\bu,D_\bv\in \D$, $\chi_{D_\nul}(D_\bu,D_\bv)=O(1)$ if $\bu = \bv$ and 
	$\chi_{D_\nul}(D_\bu,D_\bv)=d^{-\Omega(\log{1/\alpha})}$ if $\bu \neq \bv$.}
\end{enumerate} 
\end{lemma}

Here,   we give two lemmas from \cite{DKPZ21} to construct
a large set of distributions on $\R^d\times \{\pm 1\}$ with small correlations.
For a matrix $\bA\in \R^{m\times n}$, we use $\|\bA\|_2$ to denote the spectral norm. 

\begin{lemma} [Correlation Lemma: Lemma 2.3 from \cite{DKPZ21}] \label{lem:correlation-lemma}
Let $g:\R^m\mapsto \R$ and $\vec U,\vec V\in \R^{m\times d}$ with $m\leq d$ be linear maps such that 
$\vec U\vec U^T=\vec V\vec V^T=\bI$ where $\bI$ is the $m\times m$ identity matrix. 
Then, we have that 
\[\E_{\x\sim \gaus_d}[g(\vec U\x)g(\vec V\x)]\leq \sum_{t=0}^{\infty}\|\vec U\vec V^T\|_2^t\E_{\x\sim \gaus_m}[(g^{[t]}(\x))^2] \;,\]
where $g^{[t]}$ denote the degree-$t$ Hermite part of $g$.
\end{lemma}

Note that we will use \Cref{lem:correlation-lemma} with $m=1$ and $n=d$ so that the linear maps $\vec U$ and $\vec V$ become vectors.
We then introduce the following well-known fact, which states that there are exponentially many near-orthogonal vectors 
in $\R^d$.

\begin{fact} [Near-orthogonal Vectors: Fact 2.6 from \cite{DKPZ21}] \label{lem:near-orthogonal}
For any $0<c<1/2$, there exists a set $V\subseteq \mathbb{S}^{d-1}$ such that $|V|=2^{d^{\Omega(c)}}$ and for any $\bu,\bv\in V$,
$\langle \bu,\bv\rangle\leq d^{c-1/2}\;.$
\end{fact}


\begin{proof} [Proof of \Cref{lem:distribution-set}]
We define $\D$ in the following manner.
We let $D'$ be the distribution on $\R\times \{\pm 1\}$ in \Cref{lem:distribution} 
for $n=c\log(1/\alpha)$ where $c$ is a sufficiently small universal constant such that
$3^{-2n}/4\geq \alpha$.
We take $V$ to be the set of vectors in \Cref{lem:near-orthogonal} with $c=1/4$. 
Then we consider the set of distributions $\D=\{D_{\bv}:\bv\in V\}$
where $D_{\bv}$ is the unique distribution on $\R^d \times \{\pm 1\}$ 
generated in the following way.
We first sample $(z,y)\sim D'$ and $\bx_{\perp}\sim \gaus_{d-1}$.
Then let $\bB_{\perp}\in \R^{d\times (d-1)}$ be a matrix whose columns form an (arbitrary) 
orthonormal basis for the orthogonal complement of $\bv$.
Let $\bx=\bB_{\perp}\bx_{\perp}+z\bv$ and we define the distribution $D_\bv=D_{(\bx,y)}$. 
This effectively embeds the distribution $D'$ along the hidden direction $\bv$ in $D_\bv$.

To establish Condition (i) in this lemma, the definition of $\bx$ combined with the fact that 
$D_z=\gaus_1$ (from Condition (i) in \Cref{lem:distribution}) implies 
the marginal distribution $D_{\bx}=\gaus_d$.

Then, for Condition (ii) in this lemma,
we have that for any $\bx'\in \R^d$ such that 
$\langle \bv,\bx'\rangle\geq \Phi^{-1}(1-\alpha)$,
\[\E_{(\bx,y)\sim D_{\bv}}[y|\bx=\bx']=\E_{(z,y)\sim D'}[y=1|z=\langle \bv,\bx'\rangle ]\; ,\]
from the definition of $D_{\bv}$.
Since $3^{-2n}/4\geq \alpha$, we have $\langle \bv,\bx'\rangle\geq \Phi^{-1}(1-\alpha)\geq \Phi^{-1}(1-3^{-2n}/4)$.
Therefore, using the second condition in \Cref{lem:distribution}, we get
\[\E_{(\bx,y)\sim D_{\bv}}[y|\bx=\bx']=\E_{(z,y)\sim D'}[y|z=\langle \bv,\bx'\rangle ]=1\; .\]

For Condition (iii) in this lemma, 
notice 
that Condition (iii) in \Cref{lem:distribution} implies 
\[\pr_{(\bx,y)\sim D_\bv}[y=1]=\pr_{(\bx,y)\sim D_\bv}[y=-1]=1/2\; .\]
Then,
\begin{align*}
	\chi_{D_\nul}(D_\bv,D_\bv)
	=&\pr_{(\bx,y)\sim D_\bv}[y=1] \chi^2(D_\bv^+,\gaus_d)+\pr_{(\bx,y)\sim D_\bv}[y=-1] \chi^2(D_\bv^-,\gaus_d)\\
	=&\frac{1}{2} \chi^2(D_\bv^+,\gaus_d)+\frac{1}{2}\chi^2(D_\bv^-,\gaus_d)\; .\\
\end{align*}	
We show that the first term $\chi^2(D_\bv^+,\gaus_d)=O(1)$.
$\chi^2(D_\bv^-,\gaus_d)=O(1)$ can be shown in the exact same way.
Notice that 
\begin{align*}	
	\chi^2(D_\bv^+,\gaus_d)
	=&\int_{\R^d} \frac{P_{D_\bv^+}(\bu)^2}{P_{\gaus_d}(\bu)} d\bu-1\\
	=&\int_\R\int_{\R^{d-1}} \frac{P_{D_\bv^+}(\bB_\perp\bu+z\bv)^2}{P_{\gaus_d}(\bB_\perp\bu+z\bv)} d\bu dz-1\\
	=&\int_\R \int_{\R^{d-1}} \frac{P_{\gaus_{d-1}}(\bu)^2P_{D'^+}(z)^2}{P_{\gaus_{d-1}}(\bu)P_{\gaus_1}(z)} d\bu dz-1\\
	=&\int_{\R^{d-1}}P_{\gaus_{d-1}}(\bu) d\bu\int_\R \frac{P_{D'^+}(z)^2}{P_{\gaus_1}(z)} dz -1\\
	=&\chi^2(D'^+,\gaus_1)=O(1)    \; ,
\end{align*}
where the last equality follows from Condition (iv) of \Cref{lem:distribution}.
Therefore 
\[	\chi_{D_\nul}(D_\bv,D_\bv)=\frac{1}{2} \chi^2(D_\bv^+,\gaus_d)+\frac{1}{2}\chi^2(D_\bv^-,\gaus_d)=O(1)\; .\]
For $D_\bu,D_\bv\in \D$ where $\bu\neq \bv$,  
we have
\begin{align*}
	\chi_{D_\nul}(D_\bu,D_\bv)
	=&\pr[y=1]\chi_{\gaus_d}(D_\bu^+,D_\bv^+)
		+\pr[y=-1]\chi_{\gaus_d}(D_\bu^-,D_\bv^-)\\
	=&\frac{1}{2} \chi_{\gaus_d}(D_\bu^+,D_\bv^+)+\frac{1}{2}\chi_{\gaus_d}(D_\bu^-,D_\bv^-)\; .
\end{align*}
We show that $\chi_{\gaus_d}(D_\bu^+,D_\bv^+)=d^{-\Omega(\log \frac{1}{\alpha})}$.
$\chi_{\gaus_d}(D_\bu^-,D_\bv^-)=d^{-\Omega(\log \frac{1}{\alpha})}$ can be shown in the exact same way.
Notice that 
\begin{align*}
	\chi_{\gaus_d}(D_\bu^+,D_\bv^+)
	=&\int_{\R^d} \frac{P_{D_\bu^+}(\bw)P_{D_\bv^+}(\bw)}{P_{\gaus_d}(\bw)} d\bw-1\\
	=&\int_{\R^d} \frac{P_{D_\bu^+}(\proj_{\perp \bu}(\bw)+\proj_{\bu}(\bw))P_{D_\bv^+}(\proj_{\perp \bv}(\bw)+\proj_{\bv}(\bw))}{P_{\gaus_d}(\bw)} d\bw-1\\
	=&\int_{\R^d} \frac{P_{\gaus_{d-1}}(\bw^{\perp\bu})P_{{D'}^+}(\bw^{\bu})P_{\gaus_{d-1}}(\bw^{\perp\bv})P_{{D'}^+}(\bw^{\bv})}{P_{\gaus_d}(\bw)} d\bw-1\\
	=&\int_{\R^d} P_{\gaus_d}(\bw)\, \frac{P_{{D'}^+}(\bw^{\bu})}{P_{\gaus_1}(\bw^{\bu})}\, \frac{P_{{D'}^+}(\bw^{\bv})}{P_{\gaus_1}(\bw^{\bv})}d\bw-1\; .
\end{align*}
Let $g:\R\to \R$ be $g(z)\eqdef P_{{D'}^+}(z)/P_{\gaus_1}(z)$ and apply \Cref{lem:correlation-lemma} and Condition (iii) of \Cref{lem:distribution}, we get
\begin{align*}
	\chi_{\gaus_d}(D_\bu^+,D_\bv^+)
    \leq & \left (1+\sum_{t=n+1}^{\infty} d^{-\Omega(t)}\E_{z\sim \gaus_1}\left [\left(g^{[t]}(z)\right )^2\right ] \right )-1 \\
    \leq & d^{-\Omega(n)} \chi^2_{\gaus_1}({D'}^+,\gaus_1)\\
    =& d^{-\Omega(\log \frac{1}{\alpha})}\;,
\end{align*}
where the last equality follows form Condition (iv) of \Cref{lem:distribution}.
This completes the proof of Lemma \Cref{lem:distribution-set}.
\end{proof}

\subsection{Proof of \Cref{thm:sq-reliable}}
Let $\D$ be the set of distributions in \Cref{lem:distribution-set}. 
We also let $D_\nul$ be the joint distribution of $(\x,y)$ such that 
$\x\sim \gaus_d$ and $y\sim \mathrm{Bern}(1/2)$ independent of $\x$. 
We consider the decision problem of $\mathcal{B}(\D,D_\nul)$. 
Using \Cref{lem:sq-lb} with $\gamma'=d^{-\log(\frac{1}{\alpha})}$, 
we can see that any SQ algorithm that solves $\mathcal{B}(\D,D_\nul)$ requires either 
queries of tolerance at most $d^{-\Omega(\log\frac{1}{\alpha})}$ or 
makes at least $2^{d^{\Omega(1)}}$ queries.

We then reduce the decision problem $\mathcal{B}(\D,D_\nul)$ to reliably learning $\alpha$-biased LTFs with $\epsilon<\alpha/3$ accuracy.
Let $D$ be an instance of $\mathcal{B}(\D,D_\nul)$ which we need to decide either $D=D_\nul$ or $D\in \D$.
Let $A$ be an algorithm that reliably learns $\alpha$-biased LTFs with $\epsilon<\alpha/3$ accuracy and succeeds with 2/3 probability. 
We can simply give the i.i.d.\ samples from the distribution $D$ to algorithm $A$.
Suppose that $A$ succeeds and outputs a hypothesis function $h$.
If $D=D_\nul$, then since $A$ promises $R_+(h,D_\nul)\leq \eps$.
Since $D_\nul$ has $y\sim {\rm Bern}(1/2)$ independent of $\bx$, 
this implies $\pr_{(\bx,y)\sim D}[h(\bx)=1]\leq \eps$.

Then we argue that if we are in the alternative case, 
i.e., $D\in \D$, then given that the algorithm succeeds, 
we must have
$\pr_{(\bx,y)\sim D}[h(\bx)=1]\geq \alpha-\eps$ instead.
We assume $\pr_{(\bx,y)\sim D}[h(\bx)=1]<\alpha-\eps$ and prove a contradiction.
If $D=D_{\bv}\in \D$, then by the first property in \Cref{lem:distribution-set}, there must be an
$\alpha$-biased LTF $c$
such that $\pr_{(\bx,y)\sim D}[c(\bx)=1]\geq \alpha$ and $\pr_{(\bx,y)\sim D}[c(\bx)=1\land y=-1]=0$.
Combining this with the fact that $\pr_{(\bx,y)\sim D}[y=1]=1/2$, we have 
$R_-(c;D)=\pr[c(\bx)=-1\land y=1]\leq 1/2-\alpha$.
Therefore, the output hypothesis $h$ of $A$ has to satisfy
\[R_-(h;D)\leq 1/2-\alpha+\eps\; .\]
However, since $\pr_{(\bx,y)\sim D}[h(\bx)=1]<\alpha-\eps$,
we have 
\[R_-(h;D)=\pr_{(\bx,y)\sim D}[h(\bx)\neq 1\land y=1]
\geq \pr_{(\bx,y)\sim D}[y=1]-\pr_{(\bx,y)\sim D}[h(\bx)=1]
> 1/2-\alpha+\eps\, \]
which contradicts the above.
Thus, it has to be $\pr_{(\bx,y)\sim D}[h(\bx)=1]>\alpha-\eps$.

If $D=D_\nul$, then $\pr_{(\bx,y)\sim D}[h(\bx)=1]\leq \eps$. 
Otherwise, we have $\pr_{(\bx,y)\sim D}[h(\bx)=1]\geq \alpha-\eps$.
Since the gap between them is at least a constant $\alpha-2\eps\geq \alpha/3$ and 
$\pr_{(\bx,y)\sim D}[h(\bx)=1]$ can be estimated to inverse exponential accuracy on samples with high probability. 
Thus, we can distinguish the two cases of $\mathcal{B}(\D,D_\nul)$ with high probability.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\end{document}