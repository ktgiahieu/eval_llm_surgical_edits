Decision Transformer (DT)~\citep{chen2021decision} and some concurrent work~\citep{janner2021offline} have popularized the research agenda of decision-making via generative modeling. The general idea is to consider decision-making as a generative process that takes in a representation of the task objective, e.g. the rewards or returns of a trajectory, and outputs a representation of the trajectory. Intuitively, a purposeful decision-making process should shift the trajectory distribution towards regimes with higher returns. In the classical decision-making literature, this is achieved by two interweaving processes, {policy evaluation} and {policy improvement} \citep{sutton2018reinforcement}. {Policy evaluation} promotes consistency in the estimated correlations between the trajectories and the returns. In DT, this is realized by the maximum likelihood estimation (MLE) of the joint distribution of sequences consisting of states, actions, and return-to-gos (RTG). {Policy improvement} shifts the distribution to improve the status quo expectation of the returns. In DT, this is naturally entailed since the policy is a distribution of actions conditioned on step-wise RTGs. 

In this work, we are interested in the problem of \textit{planning}. Among various ways to identify \textit{planning} as a special class of decision-making problems, we pay particular attention to its data specification and inductive biases. As designing step-wise rewards requires significant effort and domain expertise, we focus on the problem of learning from trajectory-return pairs, where a trajectory is a sequence of states and actions, and the return is its total rewards. 
% Presumably, the non-triviality in designing step-wise reward functions differentiates planning from low-level controls. Deliberate planning keeps you on a healthy diet; greedily gorging causes eating disorders. 
% Instead of specifying step-wise rewards, we choose to specify data as trajectory-return pairs, where a trajectory is a sequence of states and actions, and the return is its total rewards. 
% This is termed \textit{fully delayed reward} by~\citet{chen2021decision}. 
This design choice forces the agents to predict into the long-term future and figure out step-wise credits by themselves. A competitive Temporal Difference (TD) learning baseline, CQL~\citep{kumar2020conservative}, was reported to be fragile under this data specification~\citep{chen2021decision}. 

Our design of inductive biases reflects our intuition of a \textit{plan}. While a policy is a factor of the trajectory distribution, a \textit{plan} is an abstraction lifted from the space of trajectories. As a plan is always made in advance of receiving returns, it implies \textit{significance}, \textit{persistence}, and \textit{contingency}. An agent should plan for more significant returns. It should be persistent in its plan even if the return is assigned in hindsight. It should also be adaptable to the environment's changes during the execution of the plan. We formulate this hierarchy of decision-making with a top-down latent variable model. The latent variable we introduce is effectively a \textit{plan}, for it decouples the trajectory generation from the expected improvement of returns. The autoregressive policy always consults this temporally extended latent variable to be persistent in the plan. The top-down structure enables the agent to disentangle the variations in its plan from the environment's contingencies.  

In this work, we introduce the Latent Plan Transformer~(LPT), a novel generative model featuring a latent vector modeled by a neural transformation of Gaussian white noise, a Transformer-based policy conditioned on this latent vector and a return estimation model. LPT is learned by maximum likelihood estimation (MLE). Given an expected return, posterior inference of the latent vector in LPT is an explicit process for iterative refinement of the \textit{plan}. The inferred latent variable replaces RTG in the conditioning of the auto-regressive policy, providing richer information about the anticipated future. 
We further develop a mode-seeking sampling scheme that strongly enforce the temporal consistency for long-range planning, which is particularly effective in \textit{stitch} trajectory, {i.e.}, to compose parts of sub-optimal trajectories to reach far beyond~\citep{fu2020d4rl}. LPT demonstrates competitive performance in Gym-Mujoco locomotion, Franka kitchen, goal-reaching tasks in maze2d and antmaze, and a contingent planning task Connect Four. These empirical results support that latent variable inference can enable and improve planning in the absence of step-wise rewards. 
% The posterior distribution captures agents' uncertainty, which is particularly important in online decision-making~\citep{zheng2022online}. Agents with this Bayesian belief can actively explore unfamiliar regimes to further improve planning. 