A sequential decision-making problem can be formulated with a decision process $\langle S, A, H, Tr, r, \rho\rangle$ that contains a set $S$ of states and a set $A$ of actions. Horizon $H$ is the maximum number of steps the agent can execute before the termination of the sequence. We further employ $S^{+}$ to denote the set of all non-empty state sequences within the horizon and $A^{+}$ for action sequences likewise. $Tr: S^{+}\times A^{+}\mapsto \Pi(S)$ is the transition that returns a distribution over the next state. $r: S^{+}\times A^{+}\mapsto\R$ specifies the real-valued reward at each step. $\rho: \Pi(S)$ is the initial state distribution that is always uncontrollable to the agent. The agent's decisions follow a policy $\pi: S^{+}\times A^{+}\mapsto\Pi(A)$. In each episode, the agent interacts with the transition model to generate a trajectory $\tau= (s_1,a_1,s_2,a_2 ...,s_H,a_H)$. 

The objective of sequential decision-making is typically formulated as the expected trajectory return $y=\sum_{t=0}^H r_t$, $Q=\mathbb{E}_{p(\tau)}[y]$. Conventional RL algorithms solve for a policy $\pi(a_t|s_t, {\ast})$, where the conditioning $\ast$ denotes the optimal expected return. DT generalizes this policy to $\pi(a_t|s_{\leq t}, a_{<t}, RTG_{\leq t})$, by fitting the joint distribution $p(s_1, a_1, RTG_1, ... s_T, a_T, RTG_T)$ with a Transformer. $RTG_t$ is the return-to-go from step $t$ to the horizon $H$, $RTG_t=\sum_{k=t}^{H}r(s_{\leq k}, a_{\leq k})$.
It is a useful indication of future rewards, especially when rewards are dense and informative. 

However, $RTG$ becomes less reliable when rewards are sparse or have non-trivial relations with the return. Distributing the return to each step is a credit assignment problem. Consider an example of an ideal credit assignment mechanism: When students receive partial credits for their incomplete answers, it's more fair to give points equal to the full marks minus the expected points for all possible ways to finish the answer, rather than assuming students have no knowledge of the remaining parts.
This credit assignment mechanism can be formalized as, $RTG_t^Q = \sum_{k=t}^K r(s_{\leq k}, a_{\leq k}) + \mathbb{E}[Q(s_{\leq K}, a_{\leq K})]$.
Here $Q$ can be estimated using deep TD learning with multi-step returns. \citet{yamagata2023q} instantiate a Markovian version and demonstrate improvement in trajectory \textit{stiching}. 

Whatever credit assignment we use, be it $RTG$ or $RTG^Q$, the purpose is to explicitly model the statistical association between trajectory steps and final returns. This effort is believed to be necessary because of the exponential complexity of the trajectory space. This belief, however, can be re-examined given the success of sequence modeling. We explore an alternative design choice by directly associating the latent vector that generates the trajectory with the return. 
