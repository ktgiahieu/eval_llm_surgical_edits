\textbf{Decision-Making via Sequence Modeling} \citet{chen2021decision} propose Decision Transformer (DT), pioneering this paradigm shift. Concurrently, \citet{janner2021offline} explore beam search upon the learned Transformer for model-based planning and inspired later work that searches over the latent state space \citep{zhang2022efficient}. \citet{lee2022multi} report DT's capability in multi-task setting. \citet{zheng2022online} explore the online extension of DT. \citet{yamagata2023q} augment the Monte Carlo RTG in DT with a Q function and show improvement in tasks requiring trajectory \textit{stitching}. \citet{janner2022planning} explore diffusion models~\citep{ho2020denoising} as an alternative generative model family for decision-making. Our model differentiates from all above in data specification and model formulation. 

\textbf{Latent Trajectory Variables in Behavior Cloning} \citet{yang2022dichotomy, paster2022you} investigate the DT's overfitting to environment contingencies and propose latent variable solutions. Our model is closely related to theirs but unique in an EM-style algorithm for MLE. \citet{ajay2021opal, lynch2020learning} propose latent variable models to make Markovian policies temporally extended. Their models are more related to VAE~\citep{kingma2013auto}. 

\textbf{Offline Reinforcement Learning} Since the offline static datasets only partially cover the state transition spaces, efforts from a conventional RL perspective focus on imposing pessimistic biases to value iteration \citep{kumar2020conservative, kostrikov2021offline, uehara2021pessimistic, xie2021bellman, cheng2022adversarially}. \citet{fujimoto2021minimalist} show that simply augmenting value-based methods with behavior cloning achieves impressive performance. \citet{emmons2021rvs} report that supervised learning on return-conditioned policies is competitive to value-based methods in offline RL. Our MLE objective is more related to the supervised learning methods. The latent variable inference further imposes temporal consistency, acting as a replacement of value iteration. 

\textbf{Hierarchical RL} Methods like OPAL~\citep{ajay2021opal}, OPOSM~\citep{freed2023learning} address TD-learning's limitations in long-range credit assignment using a two-stage approach: discovering skills from shorter subsequences to reduce the planning horizon, then applying skill-level CQL or online model-based planning on the reduced horizons. This paper focuses on comparing various methods for long-range credit assignment on the original horizon. Future work includes first discovering skills and then modeling them with a skill-level LPT to further extend the effective horizon.

