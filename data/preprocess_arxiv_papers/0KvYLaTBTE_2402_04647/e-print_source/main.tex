\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
    \PassOptionsToPackage{round, compress}{natbib}
% before loading neurips_2024

% ready for submission
% \usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
% \usepackage{xcolor}         % colors
\usepackage[usenames,dvipsnames]{xcolor} 

% add by myself

\input{math_commands.tex}
\usepackage{natbib} % has a nice set of citation styles and commands
\bibliographystyle{plainnat}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[capitalize]{cleveref}
\usepackage{dsfont}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{subcaption}
\usepackage{dsfont}
\usepackage{multirow}
\usepackage{caption}
% \usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{positioning, fit, calc, arrows.meta, shapes}
\usetikzlibrary{automata,arrows,positioning,calc}
\usetikzlibrary{shapes.multipart}
\usepackage{wrapfig}
\usetikzlibrary{positioning, arrows.meta, fit, backgrounds, decorations.pathreplacing}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\todo}[1]{\textcolor{red}{#1}}
\renewcommand{\pdata}{p_{\mathcal{D}}}
\usepackage{relsize}
        % colors
\definecolor{customlinkcolor}{HTML}{2774AE} 
\definecolor{customcitecolor}{HTML}{2774AE} 
\hypersetup{
    colorlinks=true,
    linkcolor=customlinkcolor,
    anchorcolor=black, 
    citecolor=customlinkcolor,
    urlcolor=customlinkcolor 
}

\makeatletter
\renewcommand\@fnsymbol[1]{}
\makeatother

\title{Latent Plan Transformer for Trajectory Abstraction: Planning as Latent Space Inference}


\author{%
  Deqian Kong$^{1,\star}$\thanks{$^\star$Equal Contribution.}\thanks{Project page: \href{https://sites.google.com/view/latent-plan-transformer}{https://sites.google.com/view/latent-plan-transformer}.}\thanks{Code: \href{https://github.com/mingluzhao/Latent-Plan-Transformer}{https://github.com/mingluzhao/Latent-Plan-Transformer}.}, Dehong Xu$^{1,\star}$, Minglu Zhao$^{1,\star}$, Bo Pang$^2$, Jianwen Xie$^3$, \\
\textbf{Andrew Lizarraga$^1$, {Yuhao Huang}$^4$, {Sirui Xie}$^{5,\star}$, {Ying Nian Wu}$^1$}\\
\\
$^1$Department of Statistics and Data Science, UCLA\\
$^2$Salesforce Research $^3$Akool Research
$^4$Xi'an Jiaotong University\\
$^5$Department of Computer Science, UCLA \\
}

\begin{document}


\maketitle


\begin{abstract}
In tasks aiming for long-term returns, planning becomes essential. We study generative modeling for planning with datasets repurposed from offline reinforcement learning. Specifically, we identify temporal consistency in the absence of step-wise rewards as one key technical challenge. We introduce the Latent Plan Transformer (LPT), a novel model that leverages a latent variable to connect a Transformer-based trajectory generator and the final return. LPT can be learned with maximum likelihood estimation on trajectory-return pairs. In learning, posterior sampling of the latent variable naturally integrates sub-trajectories to form a consistent abstraction despite the finite context. At test time, the latent variable is inferred from an expected return before policy execution, realizing the idea of \textit{planning as inference}.  
Our experiments demonstrate that LPT can discover improved decisions from suboptimal trajectories, achieving competitive performance across several benchmarks, including Gym-Mujoco, Franka Kitchen, Maze2D, and Connect Four. It exhibits capabilities in nuanced credit assignments, trajectory stitching, and adaptation to environmental contingencies. These results validate that latent variable inference can be a strong alternative to step-wise reward prompting. 
\end{abstract}

\section{Introduction}
\label{sec:intro}
\input{scripts/intro} 

\section{Background}
\label{sec:problem}
\input{scripts/problem} 

\section{Latent Plan Transformer (LPT)}
\label{sec:method}
\input{scripts/method} 
\section{A Sequential Decision-Making Perspective}
\label{sec:analysis}
\input{scripts/analysis} 
\section{Related work}
\label{sec:literature}
\input{scripts/literature} 

\section{Experiments}
\label{sec:experiments}
\input{scripts/experiments} 

\section{Limitation}
\label{sec:discussion}
\input{scripts/discussion} 

\section{Summary}
\label{sec:summary}
\input{scripts/summary} 

\section*{Acknowledgements}

The work was partially supported by NSF DMS-2015577, NSF DMS-2415226, and a gift fund from Amazon. We sincerely thank Mr. Shanwei Mu and Dr. Jiajun Lu at Akool Research for their computational support, as well as the anonymous reviewers for their valuable feedback.

\newpage
% \bibliographystyle{natbib}
\bibliography{reference}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
\appendix

\section{Appendix}
\input{scripts/appendix} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}