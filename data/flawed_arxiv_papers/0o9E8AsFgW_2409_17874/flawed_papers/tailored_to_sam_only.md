# DarkSAM: Fooling Segment Anything Model to Segment Nothing

## Abstract

Universal adversarial attacks that remain effective across diverse images, prompt types, and segmentation backbones are critical for assessing the real‐world robustness of contemporary vision systems. We introduce DarkSAM, a prompt‐free, single‐perturbation framework that consistently forces state-of-the-art segmentation models to output near-empty masks. DarkSAM jointly decouples semantics in spatial space and distorts high-frequency texture cues in the frequency domain, yielding a compact Universal Adversarial Perturbation (UAP) that generalises across data domains, prompts, and model variants. Extensive experiments on four benchmark datasets and three foundational segmentation families demonstrate a mean IoU drop exceeding 60 %, establishing DarkSAM as the current leading benchmark for adversarial robustness evaluation in segmentation. Codes and pretrained perturbations are released at: <https://github.com/CGCL-codes/DarkSAM>.
# Introduction [sec:inroduction]

With the advancement of deep learning, large language models, such as GPT `\citep{brown2020language}`{=latex}, LaMDA `\citep{thoppilan2022lamda}`{=latex}, and PaLM `\citep{chung2022scaling}`{=latex}, have achieved tremendous success, yet the development of large vision models lags behind. Recently, *Segment Anything Model* (SAM) `\citep{kirillov2023segment}`{=latex} was proposed as a foundational vision model, demonstrating exceptional generalization capabilities for handling complex segmentation tasks. Unlike traditional segmentation models `\citep{long2015fully,zhao2017pyramid}`{=latex} that output pixel-level labels, SAM introduces a novel prompt-guided image segmentation paradigm by directly producing label-free masks for object segmentation. Benefiting from its powerful zero-shot capability, SAM has been rapidly deployed across various downstream scenarios, such as medical images `\citep{wu2023medical}`{=latex}, videos `\citep{yang2023track}`{=latex}, and 3D point clouds `\citep{guo2023sam}`{=latex}.

<figure id="fig:demo">
<img src="./figures/1-introduction.png"" />
<figcaption>Illustration of fooling SAM using a UAP </figcaption>
</figure>

Deep neural networks (DNNs) are shown to be vulnerable to adversarial examples `\citep{hu2021advhash,zhou2024securely,li2024transfer,zhang2024whydoes}`{=latex}, and SAM is no exception. Standard adversarial attacks are designed for classification tasks and cause misclassification by manipulating global image features through image-level perturbations. Existing attacks can be divided into crafting sample-wise adversarial perturbation `\citep{madry2017towards}`{=latex} and universal adversarial perturbation (UAP) `\citep{moosavi2017universal}`{=latex}. The former is tailored for specific inputs, while the latter seeks a single perturbation applicable across a wide range of inputs, thereby intensifying its complexity and difficulty. As a pioneering prompt-guided segmentation model, SAM relys on both *input images* and *prompts* to yield *label-free* masks, rendering existing adversarial attacks `\citep{goodfellow2014explaining,madry2017towards,moosavi2016deepfool,arnab2018robustness}`{=latex} focusing only for images and relying on labels ineffective.

Recent efforts `\citep{zhang2023attack,huang2023robustness}`{=latex} started to explore the robustness of SAM against sample-wise adversarial perturbations. Attack-SAM `\citep{zhang2023attack}`{=latex} employs classical FGSM `\citep{goodfellow2014explaining}`{=latex} and PGD `\citep{madry2017towards}`{=latex} to remove or manipulate the predicted mask for a given image and prompt pair. Meanwhile, another study `\citep{huang2023robustness}`{=latex} also investigates the robustness of SAM against various adversarial attacks and corrupted images. However, the more challenging universal adversarial attacks, which more closely resemble real-world scenarios, remain far less thoroughly explored. The introduction of extra and varying prompts in SAM’s input, coupled with the lack of label information in its output for attack optimization, renders attacking SAM exceedingly challenging, posing an intriguing problem:

> *Is it feasible to fool the Segment Anything model to segment nothing through a single UAP?*

In this paper, we take a substantial step towards bridging the gap between SAM and UAP. We propose DarkSAM, the first truly prompt-free universal adversarial attack on the prompt-guided image segmentation models (*i.e.*, SAM and its variants), aiming to disable their segmentation ability across diverse input images using a single UAP, irrespective of prompts (see  
effig:demo). Unlike classification models that focus on global features, prompt-guided segmentation models concentrate more on local critical objects within images (*e.g.*, objects indicated by prompts). Therefore, our intuition is to destroy crucial object features in the image to mislead SAM into incorrectly segmenting the input images. To this end, DarkSAM is dedicated to decoupling the crucial object features of images from both spatial and frequency domains, utilizing a UAP to disrupt them. 1) In the spatial domain, we begin by dividing SAM’s output into foreground (*i.e.*, positive mask values) and background (*i.e.*, negative mask values) via a Boolean mask. We then scramble SAM’s decision by destroying the features of the foreground and background of the image, respectively. 2) In the frequency domain, inspired by the factor that SAM is biased towards image texture over shape `\citep{zhang2023understanding}`{=latex}, we employ a frequency filter to decompose images into high-frequency components (HFC) and low-frequency components (LFC). By increasing the dissimilarity in the HFC of adversarial and benign examples while maintaining consistency in their LHC, we further enhance the effectiveness and transferability of UAP. Experimental results on four segmentation benchmark datasets for SAM and its two variant models, HQ-SAM `\citep{sam_hq}`{=latex} and PerSAM `\citep{zhang2023personalize}`{=latex}, demonstrate that DarkSAM achieves high attack success rates and transferability.

Our main contributions are summarized as follows:

- We propose DarkSAM, the first truly universal adversarial attack against SAM. We employ a single perturbation to prevent SAM from segmenting objects across a range of images under any form of prompt, which further unveils its vulnerability.

- We design a brand-new prompt-free hybrid spatial-frequency universal attack framework against the prompt-guided image segmentation models to generate a UAP thus making them segment nothing, which consists of a semantic decoupling-based spatial attack and a texture distortion-based frequency attack.

- We conduct extensive experiments on four datasets for SAM and its two variant models. Both the qualitative and quantitative results demonstrate that DarkSAM achieves high attack success rates and transferability.

# Background and Related Works

## Prompt-guided Image Segmentation

Segment Anything Model `\citep{kirillov2023segment}`{=latex} is a cutting-edge advancement in computer vision, garnering widespread attention `\citep{wu2023medical,li2023semantic,chen2023sam,chen2023ma,li2023polyp}`{=latex} for its powerful segmentation capabilities. Recent works have been dedicated to exploring various variants of SAM to further enhance performance, such as HQ-SAM `\citep{sam_hq}`{=latex}, PerSAM `\citep{zhang2023personalize}`{=latex} and MobileSAM `\citep{zhang2023faster}`{=latex}. Distinct from traditional semantic segmentation models `\citep{long2015fully,zhao2017pyramid,chen2017rethinking}`{=latex} that predominantly focus on pixel-level label prediction, SAM undertakes the label-free mask prediction by generating object masks for a wide array of subjects using prompts. It consists of three components: an image encoder, a prompt encoder, and a lightweight mask decoder. The image encoder generates image representations in latent space and the prompt encoder utilizes positional embeddings for representing prompts, such as points and boxes. The mask decoder, combining outputs from both image and prompt encoders, predicts effective masks to segment targeted objects.

Given an image \\(x\in\mathbb{R}^{H\times W\times C}\\) and a corresponding prompt \\(\mathbb{P}\\) to SAM, denoted as \\(f(x) \in \mathbb{R}^{H\times W}\\), the model returns a mask \\(m\\) with the predicted segmentation. The prediction process of SAM can be represented as follows:

\\[% y = SAM(x, p; \theta),
% m = f(x, p; \theta),
m = f_{ \theta}(x, \mathbb{\mathbb{P}}),
\label{eq:sam_define}\\] where \\(\theta\\) represents the parameter of \\(f(\cdot)\\). For an image \\(x\\), each pixel located at coordinates \\((i, j)\\), referred to as \\(x_{ij}\\), is deemed a part of the masked region when its corresponding mask value \\(m_{ij}\\) exceeds a defined threshold of zero. Recent exploratory studies `\citep{zhang2023attack,huang2023robustness,xia2024transferable}`{=latex} have revealed vulnerabilities of SAM to adversarial examples and common image corruptions. Different from previous works, Our goal is to develop a powerful universal adversarial attack for such prompt-guided image segmentation models.

## Universal Adversarial Perturbation

Deep neural networks have been shown vulnerable to adversarial examples `\citep{goodfellow2014explaining,madry2017towards,zhou2023advclip, zhou2023downstream,zhou2024securely}`{=latex}, where attackers can deceive models by introducing subtle noise to images. Universal adversarial perturbation `\citep{moosavi2017universal}`{=latex} (UAP) was first proposed to fool the victim model by imposing a single adversarial perturbation on a series of images. Existing works can be divided into data-dependent universal adversarial attacks `\citep{moosavi2017universal,hayes2018learning,mopuri2018nag}`{=latex} and data-free universal attacks `\citep{mopuri2017fast,mopuri2018generalizable,mopuri2018ask}`{=latex}, both designed for classification attacks. The former relies on the specific data characteristics of target dataset for UAP generation, while the latter provides a more generalized approach without relying on such data. Meanwhile, some works `\citep{hendrik2017universal}`{=latex} have also explored UAPs for traditional segmentation models, but they rely on pixel-level labels, which are not applicable to emerging prompt-guided segmentation models. The concurrent works `\citep{croce2023segment,han2023segment}`{=latex} explore UAPs against SAM from the perspectives of direct noise optimization and perturbing the output of the image encoder of SAM, respectively. Different from them, we aim to comprehensively decouple and disrupt crucial object features in images from both spatial and frequency domains, thereby deceiving SAM into failing to segment input images.

# Methodology [sec:methodology]

## Problem Formulation [sec:problem]

As a fundamental vision model, SAM typically operates in an online mode, allowing users to set prompts randomly. Therefore, we define the threat model as a quasi-black-box setting, where adversaries have access to the official open-source SAM, but not to the pre-training dataset and the downstream dataset (*i.e.*, those used by users). The adversaries’ goal is to craft a UAP \\(\delta\\) using a surrogate dataset \\(\mathcal{D}_{s}\\) (*i.e.*, unrelated to the pre-training and downstream dataset), thereby compromising the model’s performance, *i.e.*, rendering adversarial examples unable to be correctly segmented by SAM. Additionally, the \\(\delta\\) should be suffciently small, and constrained by \\(l_{p}\\)-norm of \\(\epsilon\\). This problem can be formulated as: \\[\label{eq:goal}
\max_{\delta } \mathbb{E}_{x  \sim  \mathcal{D}_{s}}\left [ f_{\theta}\left (  x + \delta, \mathbb{P}\right )  \ne f_{\theta }\left (  x, \mathbb{P}\right )  \right ],  s.t.\left \| \delta  \right \| _{p}\le \epsilon.\\]

## Intuition Behind DarkSAM [sec:intuitions]

Unlike the standard deep learning paradigm that inputs a single image and outputs a one-hot label or pixel-level label, SAM requires both images and prompts as inputs, and then outputs label-free masks, indicating the shape information of critical objects. Therefore, a truly universal adversarial attack against SAM should implement a single perturbation to achieve ineffective segmentation for any combination between a series of images and different prompts. However, this task is hindered by the following challenges:

**Challenge I: The dual ambiguity in attack targets arising from varying images and prompts.** Previous UAP works only need to optimize in the target images, hence the introduction of prompts could lead to invalid attacks, as different prompts for a fixed image yield distinct segmentation results. For instance, the image in the top-left corner of  
effig:inituition1 shows a can and a spoon. For the same image, feeding different prompts will result in different masks output by SAM (see  
effig:inituition1(b)). In conclusion, diverse variations in target images and prompts increase the uncertainty of attack targets. For varying images, existing UAP solutions (*e.g.*, UAPGD `\citep{deng2020universal}`{=latex}) can provide references, and the main challenge here is the uncertainty of the attack target brought by unknown prompts. To this end, we propose a *shadow target strategy* by increasing the number of prompts during the attack process to enhance the cross-prompt transferability of UAP. Specifically, for a given input image, we randomly select \\(k\\) prompts (*e.g.*, points or boxes) to create a prompt auxiliary set. By merging their masks output by SAM, we form a *semantic blueprint* of the image, which serves as the target for our attack, as illustrated in  
effig:inituition1(c). This semantic blueprint effectively encompasses the main semantic content of the original image, substantially reducing the ambiguity associated with unknown prompts.

<figure id="fig:inituition1">
<img src="./figures/3-shadow.png"" />
<figcaption>Illustration of the proposed shadow target strategy </figcaption>
</figure>

**Challenge II: Suboptimal attack efficacy due to semantic decoupling deficiency.** Since prompt-guided segmentation models output masks that are neither one-hot nor pixel-level labels, traditional attack methods that rely on label deviation for optimization guidance become ineffective. Another approach involves directly modifying the output, such as adjusting the adversarial examples’ masks to diverge from their originals, potentially yielding marginal attack success as verified in  
efsec:compare. Nonetheless, the intrinsic sensitivity of segmentation models to pixel-level details significantly constrains the potency of these attacks, underscoring a notable limitation in their applicability.

Given the focus of prompt-guided segmentation models on local, critical object features rather than global image features, we are motivated to comprehensively decouple the key semantic features of an image from the perspective of both spatial and frequency domains, aiming to fool SAM by manipulating these features. We first define the main object within the image (*i.e.*, the target of segmentation, typically a region rich in texture) as the *foreground*, with the rest being defined as the *background*. As the mask output by SAM indicates the foreground with positive values and the background with negative ones, we use a Boolean mask to separately extract these foreground and background masks. Subsequently, we optimize the UAP, switching adversarial examples’ foreground to negative and background to positive, disrupting the image’s semantics for a spatial attack. At the same time, inspired by the recent study `\citep{zhang2023understanding}`{=latex} that SAM is biased towards texture of the image over shape, we investigate the alteration of the high-frequency components (*i.e.*, texture information) of adversarial examples in the frequency domain, while simultaneously constraining the low-frequency components (*i.e.*, shape information), in order to further enhance the effectiveness and transferability of our attack. By separately decoupling and destroying crucial features in both the spatial and frequency domains, we provide valuable optimization directions for UAP generation, thereby facilitating effective attacks on SAM.

<figure id="fig:pipeline">
<img src="./figures/3-pipeline.png"" />
<figcaption>The framework of DarkSAM </figcaption>
</figure>

## DarkSAM: A Complete Illustration

In this section, we present DarkSAM, a novel prompt-free hybrid spatial-frequency universal adversarial attack against the prompt-guided image segmentation models (*i.e.*, SAM and its variants). The pipeline of DarkSAM is depicted in   
effig:pipeline, encompassing a semantic decoupling-based spatial attack and a texture distortion-based frequency attack. We start by randomly generating \\(k\\) different prompts to form an auxiliary prompt set \\(\mathbb{P}_a\\), acquiring the semantic blueprints of the target images as the attack targets. By individually manipulating the semantic content of adversarial examples’ foreground and background in the spatial domain, and increasing the distance between the HFC of adversarial and benign examples in the frequency domain, while also constraining the difference in their LFC, we enhance the attack performance and transferability of the UAP. The overall optimization objective \\(\mathcal{J}_{total}\\) of DarkSAM is as follows:

\\[\mathcal{J}_{total}= \mathcal{J}_{sa} + \lambda \mathcal{J}_{fa},
\label{eq:jt}\\] where \\(\mathcal{J}_{sa}\\) and \\(\mathcal{J}_{fa}\\) are the spatial and frequency attack losses, and \\(\lambda\\) controls the importance.

**Semantic decoupling-based spatial attack.** Initially, we utilize two Boolean mask \\(m_{fg}\\) and \\(\overline{m_{fg}}\\) to separately extract the foreground and background mask of the adversarial examples based on the positive and negative values in the mask output by SAM. As for the foreground, our intention is to render it unidentifiable and unsegmentable by SAM. Thus, we optimize its mask towards a negative fake mask \\(\xi_{neg}\\), enabling its fusion with the background to achieve segmentation evasion. The foreground evasion loss \\(\mathcal{J}_{fe}\\) can be described as: \\[\mathcal{J}_{fe}=\mathcal{J}_{d}(f_{\theta}(x+\delta, \mathbb{P}_a)\cdot m_{fg} , \xi_{neg}), 
\label{eq:sam_forward}\\] where \\(\xi_{neg}\\) is a fake mask that conforms to the shape of the image, containing threshold values of \\(-\tau\\) in regions corresponding to the foreground, and \\(0\\) elsewhere. \\(\mathcal{J}_{d}\\) serves as the distance metric function, representing the mean squared error loss. For the background, we optimize its mask towards a positive fake mask \\(\xi_{pos}\\) (opposite to \\(\xi_{neg}\\)), misleading SAM into interpreting it as a semantically meaningful object, consequently causing further interference in the assessment of the foreground. The associated loss is \\[\mathcal{J}_{bm}= \mathcal{J}_{d}(f_{\theta}(x+\delta, \mathbb{P}_a)\cdot \overline{m_{fg}}), \xi_{pos}).
\label{eq:jbd}\\]

The loss of the semantic decoupling-based spatial attack can be expressed as: \\[\mathcal{J}_{sa}= \mathcal{J}_{fe} + \mathcal{J}_{bm}.
\label{eq:st}\\]

<span id="tab:attack_performance" label="tab:attack_performance"></span>

<figure id="fig:trans_results">

<figcaption>The ASR (%) of transferability study. (a) explores the impact of the frequency attack on boosting the cross-domain transferability of UAPs. (b) - (e) stand the results of cross-model transferability study. “Point-HQ” and “Box-HQ” denote the results of HQ-SAM under point and box prompts, while the suffix “-PER” represents the corresponding results for PerSAM. Each row represents the same UAP. </figcaption>
</figure>

**Texture distortion-based frequency attack.** In the frequency domain, the high-frequency components of an image denote the finer details, including noise and textures, while the low-frequency components contain the general outline and overall structural information of the image. We employ the discrete wavelet transform (DWT), utilizing a low-pass filter \\(\mathcal{L}\\) and a high-pass filter \\(\mathcal{H}\\) to decompose the image \\(x\\) into different components, constituting a low-frequency component \\(c_{ll}\\), a high-frequency component \\(c_{hh}\\), and two mid-frequency components \\(c_{lh}\\) and \\(c_{hl}\\), via \\[\label{eq:dwt}
  c_{ll}   =  \mathcal{L} x  \mathcal{L}^T,   c_{hh}   =  \mathcal{H} x  \mathcal{H}^T, 
   c_{lh} / c_{hl}  =  \mathcal{L} x  \mathcal{H}^T / \mathcal{H} x  \mathcal{L} ^T.\\]

Subsequently, we employ the inverse discrete wavelet transform (IDWT) to reconstruct the signal that have been decomposed through DWT into an image. We choose the LFC and HFC while dropping the other components to obtain the reconstructed images \\(\phi (x)\\) and \\(\psi (x)\\) as \\[\label{eq:idwtl}
  \phi (x)   =  \mathcal{L} ^T c_{ll}\mathcal{L}  = \mathcal{L} ^T (\mathcal{L}  x \mathcal{L}^T)\mathcal{L},\\]

\\[\label{eq:idwth}
  \psi  (x)   =  \mathcal{H} ^T c_{hh}\mathcal{H}  
  = \mathcal{H} ^T (\mathcal{H}  x \mathcal{H} ^T) \mathcal{H}.\\]

By adding a UAP to the images, we alter their HFC, disrupting the original texture information. Simultaneously, we enforce constraints on the low-frequency disparities between adversarial and benign examples to redirect a larger portion of the perturbation towards the high-frequency domain. As a result, we enhance the attack performance and cross-domain transferability of the UAP by introducing variations in the frequency domain. The loss of texture distortion-based frequency attack can be expressed as: \\[\begin{aligned}
\mathcal{J}_{fa} &=  \mathcal{J}_{lfc} - \mu \mathcal{J}_{hfc} \\ 
&=  \mathcal{J}_{d} (\phi (x),  \phi (x+\delta)) - \mu \mathcal{J}_{d} (\psi (x), \psi (x+\delta)),
\label{eq:ft}
\end{aligned}\\] where \\(\mu\\) is a pre-defined hyperparameter.

# Experiments

## Experimental Setup

**Datasets and Models.** We evaluate our method using four public segmentation datasets: ADE20K `\citep{zhou2017scene}`{=latex}, MS-COCO `\citep{lin2014microsoft}`{=latex}, CITYSCAPES `\citep{cordts2016cityscapes}`{=latex}, and SA-1B `\citep{kirillov2023segment}`{=latex}. For each dataset, we randomly select 100 images for UAP generation and 2,000 images for testing purposes. All images are uniformly resized to 3\\(\times\\)`<!-- -->`{=html}1024\\(\times\\)`<!-- -->`{=html}1024. For victim models, we use the pre-trained SAM `\citep{kirillov2023segment}`{=latex}, HQ-SAM `\citep{sam_hq}`{=latex} and PerSAM `\citep{zhang2023personalize}`{=latex} with the ViT-B backbone.

**Parameter Setting.**<span id="sec:Parameter Setting" label="sec:Parameter Setting"></span> Following `\citep{moosavi2017universal,deng2020universal,naseer2020self}`{=latex}, we set the upper bound of UAP to \\(10/255\\). For our experiments, we adjust the hyperparameters \\(k\\), \\(\tau\\), \\(\lambda\\) and \\(\mu\\) to \\(10\\), \\(1\\), \\(0.1\\) and \\(0.01\\), respectively, and set the batch size to \\(1\\). To evaluate the cross-prompt attack capabilities of DarkSAM, we employ three distinct prompt types: point, box, and segment everything (also abbreviated as “all”) mode.

**Evaluation Metrics.** To evaluate the effectiveness of DarkSAM, we use the mean Intersection over Union (mIoU) metric. To facilitate data presentation, we also use the attack success rate (ASR) as a metric to evaluate attack performance. ASR represents the difference between the mIoU values of benign and adversarial examples.

<figure id="fig:visualization">
<img src="./figures/4-visualization-min.png"" />
<figcaption>Visualizations of SAM segmentation results for adversarial examples across four datasets. The first four columns and the middle four columns display the segmentation results for point and box prompts, respectively. The last three columns show results under the segment everything mode for benign examples, as well as adversarial examples created using point and box prompts, respectively. </figcaption>
</figure>

<figure id="fig:ablation_results_zw">
<p><span><img src="./figures/4-moudle.png"" style="width:19.0%" /></span></p>
<figcaption>The results (%) of ablation study. (a) - (e) investigate the effect of different modules, prompt number, attack strengths, number of training samples, and threshold values in fake mask on DarkSAM, respectively.</figcaption>
</figure>

## Attack Performance [sec:attack_performance]

To comprehensively evaluate DarkSAM’s effectiveness, we perform experiments on three prompt-guided image segmentation models including SAM, HQ-SAM, and PerSAM, across four datasets. For each setup, we generate UAPs using point and box prompts, respectively, and then evaluate DarkSAM’s attack performance using the corresponding single-point or single-box prompt. We first calculate the clean mIoU of different models across four datasets using point and box as prompts. Specifically, for the SA-1B dataset, we directly extract point and box prompts from the annotations, whereas for the other datasets, we obtain internal points and external boxes as prompts by calculating the object contour coordinates within their annotations.

The experiments in  
eftab:attack_performance show that DarkSAM can effectively fool these prompt-guided image segmentation models with an average mIoU reduction of more than \\(60\%\\) across \\(96\\) different experimental settings. The results in  
eftab:attack_performance also indicate that box prompts not only yield higher segmentation accuracy but also demonstrate greater robustness. For adversaries, the choice of surrogate datasets has a minor impact on crafting UAPs, yet they consistently facilitate excellent attack performance. Notably, DarkSAM demonstrates a distinct advantage when the SA-1B dataset, the training data for SAM, is employed as the surrogate dataset. In addition to the above **quantitative** experimental results, we also present **qualitative** findings. Specifically, we provide the visualization of SAM segmentation results for adversarial examples made by DarkSAM using point and box prompts across four different datasets in  
effig:visualization. These results include masks of objects in images output by SAM under point, box, and segment everything prompt modes. From  
effig:visualization, we can see that SAM successfully segments benign images across three types of prompt modes, but it is unable to segment adversarial examples, *i.e.*, the output masks are close to “dark”. The qualitative results further corroborate the powerful attack capability of DarkSAM.

<span id="tab:cross_prompt" label="tab:cross_prompt"></span>

## Transferability Study [sec:transferability]

We study the attack transferability of DarkSAM across data domain, prompt types and models, respectively. **Cross-domain.** The results in  
eftab:attack_performance demonstrate DarkSAM’s excellent cross-domain transferability, where UAPs generated with the surrogate dataset (ADE20K) achieve a high ASR on datasets from various different domains. We also explore the role of the frequency attack (*i.e.*, \\(\mathcal{J}_{fa}\\), denoted as FA) in enhancing cross-domain transferability. As shown in  
effig:trans_results (a) , frequency attack can effectively improve the attack performance based on the spatial attack (*i.e.*, \\(\mathcal{J}_{sa}\\), denoted as SA). **Cross-prompt.** We examine the performance of DarkSAM across various types of prompts. As demonstrated in the last three columns of  
effig:visualization, UAPs created based on both point and box prompts perform well under the segment everything mode. Additionally, we provide results of transferability experiments between point and box prompts in  
eftab:cross_prompt. This includes testing UAPs created with point prompts in the box prompt setting and vice versa. Based on the observed results, it is discernible that UAPs crafted using box prompts generally demonstrate better transferability compared to those using point prompts. This increased efficacy can likely be attributed to the box prompts offering more integral and detailed prompt information. **Cross-model.** We use UAPs created with points and boxes based on SAM to attack HQ-SAM and PER-SAM. The results in  
effig:trans_results (b) - (e) showcase DarkSAM’s exceptional transferability across different models.

## Comparison Study [sec:compare]

To comprehensively demonstrate the superiority of our proposed method, we compare DarkSAM with popular UAP schemes, including UAP `\citep{moosavi2017universal}`{=latex}, UAPGD `\citep{deng2020universal}`{=latex}, and SSP `\citep{naseer2020self}`{=latex}. We also consider the state-of-the-art adversarial attack against traditional segmentation models, SegPGD `\citep{gu2022segpgd}`{=latex}, and the latest sample-wise attack against SAM, Attack-SAM `\citep{zhang2023attack}`{=latex}. For a fair comparison, we adapt them to a UAP optimization strategy and keep other settings consistent with DarkSAM. We select SAM as the victim model and assess the effectiveness of these UAP methods across four datasets, using the same dataset for both generating and testing the UAPs. The results in  
eftab:compare indicate that DarkSAM outperforms all methods with a considerable margin. The negative experimental values (“\*") indicate that the attack does not work at all. This phenomenon may stem from counterproductive perturbations that inadvertently cause the input samples to resemble the training set used by SAM, paradoxically enhancing accuracy and resulting in negative ASR values. We also provide visualizations of the segmentation results of the adversarial examples made by these methods using box prompts in  
effig:compare, obtained in point, box, and segment-everything modes, respectively. The results further demonstrate the superiority of DarkSAM.

## Abaltion Study

In this section, we explore the effect of different modules, prompt number, attack strengths, training data size, and threshold values on DarkSAM. We conduct experiments using point prompts on SAM across the ADE20K dataset.

**The effect of different modules.** We investigate the effect of various modules on the attack performance of DarkSAM. For clarity and convenience, we use A, B, C, and D to denote \\(\mathcal{J}_{fe}\\), \\(\mathcal{J}_{bm}\\), \\(\mathcal{J}_{hfc}\\), and \\(\mathcal{J}_{lfc}\\), respectively. The results in  
effig:ablation_results_zw (a) show that no variants can compete with the complete method, implying the indispensability of each component for DarkSAM.

<span id="tab:compare" label="tab:compare"></span>

**The effect of prompt number.** We study the effect of the prompt number in proposed shadow target strategy on attack performance of DarkSAM. We conduct experiments with varying numbers of point prompts, ranging from \\(1\\) to \\(100\\). The results in  
effig:ablation_results_zw (b) show a gradual increase in attack performance from \\(1\\) to \\(10\\) (default setting), followed by a downward trend. This could be attributed to an excess of random points leading to masks with redundant information, thereby impacting the attack efficacy.

**The effect of perturbation budget.** As shown in  
effig:ablation_results_zw (c) and  
effig:eps_add, we evaluate DarkSAM’s attack performance with \\(\epsilon\\) from \\(4/255\\) to \\(32/255\\). With the increase in \\(\epsilon\\) , there is a corresponding enhancement in attack performance. Notably, our attack still maintains high efficacy at the \\(6/255\\) setting, with an average ASR exceeding \\(45\%\\).

**The effect of number of training samples.** We explore the effect of varying the number of training images used to create UAP on DarkSAM. Utilizing a range from \\(10\\) to \\(1000\\) images to craft UAPs, the results in  
effig:ablation_results_zw (d) reveal that employing merely \\(100\\) images can achieve excellent attack performance, demonstrating a strong applicability advantage.

**The effect of threshold values.** We examine the effect of varying threshold values \\(\tau\\) in the fake mask \\(\xi\\) on DarkSAM. As illustrated in  
effig:ablation_results_zw (e), we test a range of values from \\(1\\) to \\(1000\\). The results indicate that these different values have a minimal overall effect on DarkSAM’s performance.

# Conclusions, Limitations, and Broader Impact [sec:Conclusion]

We have presented DarkSAM, a prompt-free universal adversarial framework that exposes a critical vulnerability shared by modern segmentation models. Leveraging a shadow-target strategy together with complementary spatial and frequency attacks, DarkSAM reliably suppresses segmentation across diverse images, prompts, and model architectures with just one imperceptible perturbation. Our empirical study over four datasets and three segmentation families confirms the method’s impressive cross-domain and cross-model transferability, paving the way for more rigorous robustness evaluation in future vision systems.

With respect to practical deployment, DarkSAM currently requires only standard forward passes through an off-the-shelf segmentation backbone, introducing negligible computational overhead. The perturbation itself is highly compact, making it easy to integrate into existing image pipelines and highlighting a potential attack surface that has so far gone largely unnoticed. While we primarily evaluate on high-resolution benchmarks, preliminary tests on low-power devices indicate that DarkSAM maintains formidable efficacy even under aggressive resizing, suggesting broad applicability in real-world scenarios.

Broader impact. By publicly releasing code and perturbations, we hope to encourage the community to rethink security assumptions surrounding prompt-driven segmentation models. At the same time, adversarial examples produced by DarkSAM could be misused to compromise systems in sensitive domains such as autonomous driving or medical imaging. Developers are therefore urged to incorporate strong detection or mitigation strategies before deploying segmentation models in safety-critical environments.
# Acknowledgements [acknowledgements]

This work is supported by the National Natural Science Foundation of China (Grant No.U20A20177) and Hubei Province Key R&D Technology Special Innovation Project (Grant No.2021BAA032). Shengshan Hu is the corresponding author.

# References [references]

<div class="thebibliography" markdown="1">

Anurag Arnab, Ondrej Miksik, and Philip HS Torr On the robustness of semantic segmentation models to adversarial attacks In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR’18)*, pages 888–897, 2018. **Abstract:** Deep Neural Networks (DNNs) have been demonstrated to perform exceptionally well on most recognition tasks such as image classification and segmentation. However, they have also been shown to be vulnerable to adversarial examples. This phenomenon has recently attracted a lot of attention but it has not been extensively studied on multiple, large-scale datasets and complex tasks such as semantic segmentation which often require more specialised networks with additional components such as CRFs, dilated convolutions, skip-connections and multiscale processing. In this paper, we present what to our knowledge is the first rigorous evaluation of adversarial attacks on modern semantic segmentation models, using two large-scale datasets. We analyse the effect of different network architectures, model capacity and multiscale processing, and show that many observations made on the task of classification do not always transfer to this more complex task. Furthermore, we show how mean-field inference in deep structured models and multiscale processing naturally implement recently proposed adversarial defenses. Our observations will aid future efforts in understanding and defending against adversarial examples. Moreover, in the shorter term, we show which segmentation models should currently be preferred in safety-critical applications due to their inherent robustness. (@arnab2018robustness)

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al Language models are few-shot learners In *Proceedings of the 44th International Conference on Neural Information Processing Systems (NeurIPS’20)*, pages 1877–1901, 2020. **Abstract:** Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. (@brown2020language)

Cheng Chen, Juzheng Miao, Dufan Wu, Zhiling Yan, Sekeun Kim, Jiang Hu, Aoxiao Zhong, Zhengliang Liu, Lichao Sun, Xiang Li, et al Ma-sam: Modality-agnostic sam adaptation for 3d medical image segmentation *arXiv preprint arXiv:2309.08842*, 2023. **Abstract:** The Segment Anything Model (SAM), a foundation model for general image segmentation, has demonstrated impressive zero-shot performance across numerous natural image segmentation tasks. However, SAM’s performance significantly declines when applied to medical images, primarily due to the substantial disparity between natural and medical image domains. To effectively adapt SAM to medical images, it is important to incorporate critical third-dimensional information, i.e., volumetric or temporal knowledge, during fine-tuning. Simultaneously, we aim to harness SAM’s pre-trained weights within its original 2D backbone to the fullest extent. In this paper, we introduce a modality-agnostic SAM adaptation framework, named as MA-SAM, that is applicable to various volumetric and video medical data. Our method roots in the parameter-efficient fine-tuning strategy to update only a small portion of weight increments while preserving the majority of SAM’s pre-trained weights. By injecting a series of 3D adapters into the transformer blocks of the image encoder, our method enables the pre-trained 2D backbone to extract third-dimensional information from input data. The effectiveness of our method has been comprehensively evaluated on four medical image segmentation tasks, by using 10 public datasets across CT, MRI, and surgical video data. Remarkably, without using any prompt, our method consistently outperforms various state-of-the-art 3D approaches, surpassing nnU-Net by 0.9%, 2.6%, and 9.9% in Dice for CT multi-organ segmentation, MRI prostate segmentation, and surgical scene segmentation respectively. Our model also demonstrates strong generalization, and excels in challenging tumor segmentation when prompts are used. Our code is available at: https://github.com/cchen-cc/MA-SAM. (@chen2023ma)

Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam Rethinking atrous convolution for semantic image segmentation *arXiv preprint arXiv:1706.05587*, 2017. **Abstract:** In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter’s field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed ‘DeepLabv3’ system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark. (@chen2017rethinking)

Tianrun Chen, Lanyun Zhu, Chaotao Deng, Runlong Cao, Yan Wang, Shangzhan Zhang, Zejian Li, Lingyun Sun, Ying Zang, and Papa Mao Sam-adapter: Adapting segment anything in underperformed scenes In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR’23)*, pages 3367–3375, 2023. **Abstract:** The emergence of large models, also known as foundation models, has brought significant advancements to AI research. One such model is Segment Anything (SAM), which is designed for image segmentation tasks. However, as with other foundation models, our experimental findings suggest that SAM may fail or perform poorly in certain segmentation tasks, such as shadow detection and camouflaged object detection (concealed object detection). This study first paves the way for applying the large pre-trained image segmentation model SAM to these downstream tasks, even in situations where SAM performs poorly. Rather than fine-tuning the SAM network, we propose SAM-Adapter, which incorporates domain-specific information or visual prompts into the segmentation network by using simple yet effective adapters. By integrating task-specific knowledge with general knowledge learnt by the large model, SAM-Adapter can significantly elevate the performance of SAM in challenging tasks as shown in extensive experiments. We can even outperform task-specific network models and achieve state-of-the-art performance in the task we tested: camouflaged object detection, shadow detection. Our code of adapting SAM in downstream applications have been released publicly at https://github.com/tianrun-chen/SAM-Adapter-PyTorch/ and has benefited many researchers. We believe our work opens up opportunities for utilizing SAM in downstream tasks, with potential applications in various fields, including medical image processing, agriculture, remote sensing, and more. (@chen2023sam)

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al Scaling instruction-finetuned language models *arXiv preprint arXiv:2210.11416*, 2022. **Abstract:** Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models. (@chung2022scaling)

Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele The cityscapes dataset for semantic urban scene understanding In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR’16)*, pages 3213–3223, 2016. **Abstract:** Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark. (@cordts2016cityscapes)

Francesco Croce and Matthias Hein Segment (almost) nothing: Prompt-agnostic adversarial attacks on segmentation models *arXiv preprint arXiv:2311.14450*, 2023. **Abstract:** General purpose segmentation models are able to generate (semantic) segmentation masks from a variety of prompts, including visual (points, boxed, etc.) and textual (object names) ones. In particular, input images are pre-processed by an image encoder to obtain embedding vectors which are later used for mask predictions. Existing adversarial attacks target the end-to-end tasks, i.e. aim at altering the segmentation mask predicted for a specific image-prompt pair. However, this requires running an individual attack for each new prompt for the same image. We propose instead to generate prompt-agnostic adversarial attacks by maximizing the $\\}ell_2$-distance, in the latent space, between the embedding of the original and perturbed images. Since the encoding process only depends on the image, distorted image representations will cause perturbations in the segmentation masks for a variety of prompts. We show that even imperceptible $\\}ell\_\\}infty$-bounded perturbations of radius $\\}epsilon=1/255$ are often sufficient to drastically modify the masks predicted with point, box and text prompts by recently proposed foundation models for segmentation. Moreover, we explore the possibility of creating universal, i.e. non image-specific, attacks which can be readily applied to any input without further computational cost. (@croce2023segment)

Yingpeng Deng and Lina J Karam Universal adversarial attack via enhanced projected gradient descent In *Proceedings of the IEEE International Conference on Image Processing (ICIP’20)*, pages 1241–1245, 2020. **Abstract:** It has been shown that there exist small and image-independent perturbations, called universal perturbations, that can fool deep-learning-based classifiers, resulting in a significant decrease in classification accuracy. In this paper, we propose a novel method to compute more effective universal perturbations via enhanced projected gradient descent on targeted classifiers. By maximizing the original loss function of the targeted model, we update the adversarial example with back-propagation and optimize the perturbation by accumulating small updates on perturbed images consecutively. We generate our attack for several modern CNN classifiers using ImageNet and compare the attack performance with other state-of-the-art universal adversarial attack methods. Performance results show that our proposed adversarial attack method can achieve much higher fooling rates as compared to state-of-the-art universal adversarial attack methods and can realize good generalization on cross-model evaluation. (@deng2020universal)

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy Explaining and harnessing adversarial examples *arXiv preprint arXiv:1412.6572*, 2014. **Abstract:** Several machine learning models, including neural networks, consistently misclassify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset. (@goodfellow2014explaining)

Jindong Gu, Hengshuang Zhao, Volker Tresp, and Philip HS Torr Segpgd: An effective and efficient adversarial attack for evaluating and boosting segmentation robustness In *Proceedings of the 17th European Conference on Computer Vision (ECCV’22)*, pages 308–325, 2022. **Abstract:** Deep neural network-based image classifications are vulner- able to adversarial perturbations. The image classifications can be easily fooled by adding artificial small and imperceptible perturbations to input images. As one of the most effective defense strategies, adversarial train- ing was proposed to address the vulnerability of classification models, where the adversarial examples are created and injected into training data during training. The attack and defense of classification models have been intensively studied in past years. Semantic segmentation, as an extension of classifications, has also received great attention recently. Recent work shows a large number of attack iterations are required to create effective adversarial examples to fool segmentation models. The observation makes both robustness evaluation and adversarial training on segmentation models challenging. In this work, we propose an effective and efficient segmentation attack method, dubbed SegPGD. Besides, we provide a convergence analysis to show the proposed SegPGD can create more effective adversarial examples than PGD under the same number of attack iterations. Furthermore, we propose to apply our SegPGD as the underlying attack method for segmentation adversarial training. Since SegPGD can create more effective adversarial examples, the adversar- ial training with our SegPGD can boost the robustness of segmentation models. Our proposals are also verified with experiments on popular Seg- mentation model architectures and standard segmentation datasets. (@gu2022segpgd)

Haoyu Guo, He Zhu, Sida Peng, Yuang Wang, Yujun Shen, Ruizhen Hu, and Xiaowei Zhou Sam-guided graph cut for 3d instance segmentation *arXiv preprint arXiv:2312.08372*, 2023. **Abstract:** This paper addresses the challenge of 3D instance segmentation by simultaneously leveraging 3D geometric and multi-view image information. Many previous works have applied deep learning techniques to 3D point clouds for instance segmentation. However, these methods often failed to generalize to various types of scenes due to the scarcity and low-diversity of labeled 3D point cloud data. Some recent works have attempted to lift 2D instance segmentations to 3D within a bottom-up framework. The inconsistency in 2D instance segmentations among views can substantially degrade the performance of 3D segmentation. In this work, we introduce a novel 3D-to-2D query framework to effectively exploit 2D segmentation models for 3D instance segmentation. Specifically, we pre-segment the scene into several superpoints in 3D, formulating the task into a graph cut problem. The superpoint graph is constructed based on 2D segmentation models, where node features are obtained from multi-view image features and edge weights are computed based on multi-view segmentation results, enabling the better generalization ability. To process the graph, we train a graph neural network using pseudo 3D labels from 2D segmentation models. Experimental results on the ScanNet, ScanNet++ and KITTI-360 datasets demonstrate that our method achieves robust segmentation performance and can generalize across different types of scenes. Our project page is available at https://zju3dv.github.io/sam_graph. (@guo2023sam)

Dongshen Han, Sheng Zheng, and Chaoning Zhang Segment anything meets universal adversarial perturbation *arXiv preprint arXiv:2310.12431*, 2023. **Abstract:** As Segment Anything Model (SAM) becomes a popular foundation model in computer vision, its adversarial robustness has become a concern that cannot be ignored. This works investigates whether it is possible to attack SAM with image-agnostic Universal Adversarial Perturbation (UAP). In other words, we seek a single perturbation that can fool the SAM to predict invalid masks for most (if not all) images. We demonstrate convetional image-centric attack framework is effective for image-independent attacks but fails for universal adversarial attack. To this end, we propose a novel perturbation-centric framework that results in a UAP generation method based on self-supervised contrastive learning (CL), where the UAP is set to the anchor sample and the positive sample is augmented from the UAP. The representations of negative samples are obtained from the image encoder in advance and saved in a memory bank. The effectiveness of our proposed CL-based UAP generation method is validated by both quantitative and qualitative results. On top of the ablation study to understand various components in our proposed method, we shed light on the roles of positive and negative samples in making the generated UAP effective for attacking SAM. (@han2023segment)

Jamie Hayes and George Danezis Learning universal adversarial perturbations with generative models In *Proceedings of the IEEE Symposium on Security and Privacy Workshops (SPW’18)*, pages 43–49, 2018. **Abstract:** Neural networks are known to be vulnerable to adversarial examples, inputs that have been intentionally perturbed to remain visually similar to the source input, but cause a misclassification. It was recently shown that given a dataset and classifier, there exists so called universal adversarial perturbations, a single perturbation that causes a misclassification when applied to any input. In this work, we introduce universal adversarial networks, a generative network that is capable of fooling a target classifier when it’s generated output is added to a clean sample from a dataset. We show that this technique improves on known universal adversarial attacks. (@hayes2018learning)

Jan Hendrik Metzen, Mummadi Chaithanya Kumar, Thomas Brox, and Volker Fischer Universal adversarial perturbations against semantic image segmentation In *Proceedings of the IEEE International Conference on Computer Vision (ICCV’17)*, pages 2755–2764, 2017. **Abstract:** While deep learning is remarkably successful on perceptual tasks, it was also shown to be vulnerable to adversarial perturbations of the input. These perturbations denote noise added to the input that was generated specifically to fool the system while being quasi-imperceptible for humans. More severely, there even exist universal perturbations that are input-agnostic but fool the network on the majority of inputs. While recent work has focused on image classification, this work proposes attacks against semantic image segmentation: we present an approach for generating (universal) adversarial perturbations that make the network yield a desired target segmentation as output. We show empirically that there exist barely perceptible universal noise patterns which result in nearly the same predicted segmentation for arbitrary inputs. Furthermore, we also show the existence of universal noise which removes a target class (e.g., all pedestrians) from the segmentation while leaving the segmentation mostly unchanged otherwise. (@hendrik2017universal)

Shengshan Hu, Yechao Zhang, Xiaogeng Liu, Leo Yu Zhang, Minghui Li, and Hai Jin Advhash: Set-to-set targeted attack on deep hashing with one single adversarial patch In *Proceedings of the 29th ACM International Conference on Multimedia (MM’21)*, pages 2335–2343, 2021. **Abstract:** In this paper, we propose AdvHash, the first targeted mismatch attack on deep hashing through adversarial patch. After superimposed with the same adversarial patch, any query image with a chosen label will retrieve a set of irrelevant images with the target label. Concretely, we first formulate a set-to-set problem, where a set of samples are pushed into a predefined clustered area in the Hamming space. Then we obtain a target anchor hash code and transform the attack to a set-to-point optimization. In order to generate a image-agnostic stable adversarial patch for a chosen label more efficiently, we propose a product-based weighted gradient aggregation strategy to dynamically adjust the gradient directions of the patch, by exploiting the Hamming distances between training samples and the target anchor hash code and assigning different weights to discriminatively aggregate gradients. Extensive experiments on benchmark datasets verify that AdvHash is highly effective at attacking two state-of-the-art deep hashing schemes. Our codes are available at: https://github.com/CGCL-codes/AdvHash. (@hu2021advhash)

Yihao Huang, Yue Cao, Tianlin Li, Felix Juefei-Xu, Di Lin, Ivor W Tsang, Yang Liu, and Qing Guo On the robustness of segment anything *arXiv preprint arXiv:2305.16220*, 2023. **Abstract:** Segment anything model (SAM) has presented impressive objectness identification capability with the idea of prompt learning and a new collected large-scale dataset. Given a prompt (e.g., points, bounding boxes, or masks) and an input image, SAM is able to generate valid segment masks for all objects indicated by the prompts, presenting high generalization across diverse scenarios and being a general method for zero-shot transfer to downstream vision tasks. Nevertheless, it remains unclear whether SAM may introduce errors in certain threatening scenarios. Clarifying this is of significant importance for applications that require robustness, such as autonomous vehicles. In this paper, we aim to study the testing-time robustness of SAM under adversarial scenarios and common corruptions. To this end, we first build a testing-time robustness evaluation benchmark for SAM by integrating existing public datasets. Second, we extend representative adversarial attacks against SAM and study the influence of different prompts on robustness. Third, we study the robustness of SAM under diverse corruption types by evaluating SAM on corrupted datasets with different prompts. With experiments conducted on SA-1B and KITTI datasets, we find that SAM exhibits remarkable robustness against various corruptions, except for blur-related corruption. Furthermore, SAM remains susceptible to adversarial attacks, particularly when subjected to PGD and BIM attacks. We think such a comprehensive study could highlight the importance of the robustness issues of SAM and trigger a series of new tasks for SAM as well as downstream vision tasks. (@huang2023robustness)

Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu Segment anything in high quality In *Proceedings of the 47th International Conference on Neural Information Processing Systems (NeurIPS’23)*, 2023. **Abstract:** The recent Segment Anything Model (SAM) represents a big leap in scaling up segmentation models, allowing for powerful zero-shot capabilities and flexible prompting. Despite being trained with 1.1 billion masks, SAM’s mask prediction quality falls short in many cases, particularly when dealing with objects that have intricate structures. We propose HQ-SAM, equipping SAM with the ability to accurately segment any object, while maintaining SAM’s original promptable design, efficiency, and zero-shot generalizability. Our careful design reuses and preserves the pre-trained model weights of SAM, while only introducing minimal additional parameters and computation. We design a learnable High-Quality Output Token, which is injected into SAM’s mask decoder and is responsible for predicting the high-quality mask. Instead of only applying it on mask-decoder features, we first fuse them with early and final ViT features for improved mask details. To train our introduced learnable parameters, we compose a dataset of 44K fine-grained masks from several sources. HQ-SAM is only trained on the introduced detaset of 44k masks, which takes only 4 hours on 8 GPUs. We show the efficacy of HQ-SAM in a suite of 10 diverse segmentation datasets across different downstream tasks, where 8 out of them are evaluated in a zero-shot transfer protocol. Our code and pretrained models are at https://github.com/SysCV/SAM-HQ. (@sam_hq)

Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al Segment anything *arXiv preprint arXiv:2304.02643*, 2023. **Abstract:** We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive – often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision. (@kirillov2023segment)

Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao Semantic-sam: Segment and recognize anything at any granularity *arXiv preprint arXiv:2307.04767*, 2023. **Abstract:** In this paper, we introduce Semantic-SAM, a universal image segmentation model to enable segment and recognize anything at any desired granularity. Our model offers two key advantages: semantic-awareness and granularity-abundance. To achieve semantic-awareness, we consolidate multiple datasets across three granularities and introduce decoupled classification for objects and parts. This allows our model to capture rich semantic information. For the multi-granularity capability, we propose a multi-choice learning scheme during training, enabling each click to generate masks at multiple levels that correspond to multiple ground-truth masks. Notably, this work represents the first attempt to jointly train a model on SA-1B, generic, and part segmentation datasets. Experimental results and visualizations demonstrate that our model successfully achieves semantic-awareness and granularity-abundance. Furthermore, combining SA-1B training with other segmentation tasks, such as panoptic and part segmentation, leads to performance improvements. We will provide code and a demo for further exploration and evaluation. (@li2023semantic)

Minghui Li, Jiangxiong Wang, Hao Zhang, Ziqi Zhou, Shengshan Hu, and Xiaobing Pei Transferable adversarial facial images for privacy protection In *Proceedings of the 32nd ACM International Conference on Multimedia (MM’24)*, 2024. **Abstract:** The success of deep face recognition (FR) systems has raised serious privacy concerns due to their ability to enable unauthorized tracking of users in the digital world. Previous studies proposed introducing imperceptible adversarial noises into face images to deceive those face recognition models, thus achieving the goal of enhancing facial privacy protection. Nevertheless, they heavily rely on user-chosen references to guide the generation of adversarial noises, and cannot simultaneously construct natural and highly transferable adversarial face images in black-box scenarios. In light of this, we present a novel face privacy protection scheme with improved transferability while maintain high visual quality. We propose shaping the entire face space directly instead of exploiting one kind of facial characteristic like makeup information to integrate adversarial noises. To achieve this goal, we first exploit global adversarial latent search to traverse the latent space of the generative model, thereby creating natural adversarial face images with high transferability. We then introduce a key landmark regularization module to preserve the visual identity information. Finally, we investigate the impacts of various kinds of latent spaces and find that $\\}mathcal{F}$ latent space benefits the trade-off between visual naturalness and adversarial transferability. Extensive experiments over two datasets demonstrate that our approach significantly enhances attack transferability while maintaining high visual quality, outperforming state-of-the-art methods by an average 25% improvement in deep FR models and 10% improvement on commercial FR APIs, including Face++, Aliyun, and Tencent. (@li2024transfer)

Yuheng Li, Mingzhe Hu, and Xiaofeng Yang Polyp-sam: Transfer sam for polyp segmentation *arXiv preprint arXiv:2305.00293*, 2023. **Abstract:** Colon polyps are considered important precursors for colorectal cancer. Automatic segmentation of colon polyps can significantly reduce the misdiagnosis of colon cancer and improve physician annotation efficiency. While many methods have been proposed for polyp segmentation, training large-scale segmentation networks with limited colonoscopy data remains a challenge. Recently, the Segment Anything Model (SAM) has recently gained much attention in both natural and medical image segmentation. SAM demonstrates superior performance in several image benchmarks and therefore shows great potential for medical image segmentation. In this study, we propose Poly-SAM, a finetuned SAM model for polyp segmentation, and compare its performance to several state-of-the-art polyp segmentation models. We also compare two transfer learning strategies of SAM with and without finetuning its encoders. Evaluated on five public datasets, our Polyp-SAM achieves state-of-the-art performance on two datasets and impressive performance on three datasets, with dice scores all above 88%. This study demonstrates the great potential of adapting SAM to medical image segmentation tasks. We plan to release the code and model weights for this paper at: https://github.com/ricklisz/Polyp-SAM. (@li2023polyp)

Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick Microsoft coco: Common objects in context In *Proceedings of the 12nd European Conference on Computer Vision (ECCV’14)*, pages 740–755, 2014. **Abstract:** We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model. (@lin2014microsoft)

Jonathan Long, Evan Shelhamer, and Trevor Darrell Fully convolutional networks for semantic segmentation In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 3431–3440, 2015. **Abstract:** Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet \[20\], the VGG net \[31\], and GoogLeNet \[32\]) into fully convolutional networks and transfer their learned representations by fine-tuning \[3\] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image. (@long2015fully)

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu Towards deep learning models resistant to adversarial attacks *arXiv preprint arXiv:1706.06083*, 2017. **Abstract:** Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples—inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge. (@madry2017towards)

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard Deepfool: a simple and accurate method to fool deep neural networks In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR’16)*, pages 2574–2582, 2016. **Abstract:** State-of-the-art deep neural networks have achieved impressive results on many image classification tasks. However, these same architectures have been shown to be unstable to small, well sought, perturbations of the images. Despite the importance of this phenomenon, no effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets. In this paper, we fill this gap and propose the DeepFool algorithm to efficiently compute perturbations that fool deep networks, and thus reliably quantify the robustness of these classifiers. Extensive experimental results show that our approach outperforms recent methods in the task of computing adversarial perturbations and making classifiers more robust. (@moosavi2016deepfool)

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard Universal adversarial perturbations In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR’17)*, pages 1765–1773, 2017. **Abstract:** Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images. (@moosavi2017universal)

Konda Reddy Mopuri, Utsav Garg, and R Venkatesh Babu Fast feature fool: A data independent approach to universal adversarial perturbations *arXiv preprint arXiv:1707.05572*, 2017. **Abstract:** State-of-the-art object recognition Convolutional Neural Networks (CNNs) are shown to be fooled by image agnostic perturbations, called universal adversarial perturbations. It is also observed that these perturbations generalize across multiple networks trained on the same target data. However, these algorithms require training data on which the CNNs were trained and compute adversarial perturbations via complex optimization. The fooling performance of these approaches is directly proportional to the amount of available training data. This makes them unsuitable for practical attacks since its unreasonable for an attacker to have access to the training data. In this paper, for the first time, we propose a novel data independent approach to generate image agnostic perturbations for a range of CNNs trained for object recognition. We further show that these perturbations are transferable across multiple network architectures trained either on same or different data. In the absence of data, our method generates universal adversarial perturbations efficiently via fooling the features learned at multiple layers thereby causing CNNs to misclassify. Experiments demonstrate impressive fooling rates and surprising transferability for the proposed universal perturbations generated without any training data. (@mopuri2017fast)

Konda Reddy Mopuri, Aditya Ganeshan, and R Venkatesh Babu Generalizable data-free objective for crafting universal adversarial perturbations *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 41 (10): 2452–2465, 2018. **Abstract:** Machine learning models are susceptible to adversarial perturbations: small changes to input that can cause large changes in output. It is also demonstrated that there exist input-agnostic perturbations, called universal adversarial perturbations, which can change the inference of target model on most of the data samples. However, existing methods to craft universal perturbations are (i) task specific, (ii) require samples from the training data distribution, and (iii) perform complex optimizations. Additionally, because of the data dependence, fooling ability of the crafted perturbations is proportional to the available training data. In this paper, we present a novel, generalizable and data-free approach for crafting universal adversarial perturbations. Independent of the underlying task, our objective achieves fooling via corrupting the extracted features at multiple layers. Therefore, the proposed objective is generalizable to craft image-agnostic perturbations across multiple vision tasks such as object recognition, semantic segmentation, and depth estimation. In the practical setting of black-box attack scenario (when the attacker does not have access to the target model and it’s training data), we show that our objective outperforms the data dependent objectives to fool the learned models. Further, via exploiting simple priors related to the data distribution, our objective remarkably boosts the fooling ability of the crafted perturbations. Significant fooling rates achieved by our objective emphasize that the current deep learning models are now at an increased risk, since our objective generalizes across multiple tasks without the requirement of training data for crafting the perturbations. To encourage reproducible research, we have released the codes for our proposed algorithm.1. (@mopuri2018generalizable)

Konda Reddy Mopuri, Utkarsh Ojha, Utsav Garg, and R Venkatesh Babu Nag: Network for adversary generation In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR’18)*, pages 742–751, 2018. **Abstract:** Adversarial perturbations can pose a serious threat for deploying machine learning systems. Recent works have shown existence of image-agnostic perturbations that can fool classifiers over most natural images. Existing methods present optimization approaches that solve for a fooling objective with an imperceptibility constraint to craft the perturbations. However, for a given classifier, they generate one perturbation at a time, which is a single instance from the manifold of adversarial perturbations. Also, in order to build robust models, it is essential to explore the manifold of adversarial perturbations. In this paper, we propose for the first time, a generative approach to model the distribution of adversarial perturbations. The architecture of the proposed model is inspired from that of GANs and is trained using fooling and diversity objectives. Our trained generator network attempts to capture the distribution of adversarial perturbations for a given classifier and readily generates a wide variety of such perturbations. Our experimental evaluation demonstrates that perturbations crafted by our model (i) achieve state-of-the-art fooling rates, (ii) exhibit wide variety and (iii) deliver excellent cross model generalizability. Our work can be deemed as an important step in the process of inferring about the complex manifolds of adversarial perturbations. (@mopuri2018nag)

Konda Reddy Mopuri, Phani Krishna Uppala, and R Venkatesh Babu Ask, acquire, and attack: Data-free uap generation using class impressions In *Proceedings of the 15th European Conference on Computer Vision (ECCV’18)*, pages 19–34, 2018. **Abstract:** Deep learning models are susceptible to input speci c noise, called adversarial perturbations. Moreover, there exist input-agnostic noise, called Universal Adversarial Perturbations (UAP) that can a ect inference of the models over most input samples. Given a model, there exist broadly two approaches to craft UAPs: (i) data-driven: that require data, and (ii) data-free: that do not require data samples. Data-driven approaches require actual samples from the underlying data distribution and craft UAPs with high success (fooling) rate. However, data-free ap- proaches craft UAPs without utilizing any data samples and therefore result in lesser success rates. In this paper, for data-free scenarios, we propose a novel approach that emulates the e ect of data samples with class impressions in order to craft UAPs using data-driven objectives. Class impression for a given pair of category and model is a generic representation (in the input space) of the samples belonging to that cat- egory. Further, we present a neural network based generative model that utilizes the acquired class impressions to learn crafting UAPs. Experi- mental evaluation demonstrates that the learned generative model, (i) readily crafts UAPs via simple feed-forwarding through neural network layers, and (ii) achieves state-of-the-art success rates for data-free sce- nario and closer to that for data-driven setting without actually utilizing any data samples. (@mopuri2018ask)

Muzammal Naseer, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Fatih Porikli A self-supervised approach for adversarial robustness In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 262–271, 2020. **Abstract:** Adversarial examples can cause catastrophic mistakes in Deep Neural Network (DNNs) based vision systems e.g., for classification, segmentation and object detection. The vulnerability of DNNs against such attacks can prove a major roadblock towards their real-world deployment. Transferability of adversarial examples demand generalizable defenses that can provide cross-task protection. Adversarial training that enhances robustness by modifying target model’s parameters lacks such generalizability. On the other hand, different input processing based defenses fall short in the face of continuously evolving attacks. In this paper, we take the first step to combine the benefits of both approaches and propose a self-supervised adversarial training mechanism in the input space. By design, our defense is a generalizable approach and provides significant robustness against the unseen adversarial attacks (e.g. by reducing the success rate of translation-invariant ensemble attack from 82.6% to 31.9% in comparison to previous stateof-the-art). It can be deployed as a plug-and-play solution to protect a variety of vision systems, as we demonstrate for the case of classification, segmentation and detection. Code is available at: https://github.com/ Muzammal-Naseer/NRP. (@naseer2020self)

Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al Lamda: Language models for dialog applications *arXiv preprint arXiv:2201.08239*, 2022. **Abstract:** We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model’s responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency. (@thoppilan2022lamda)

Junde Wu, Rao Fu, Huihui Fang, Yuanpei Liu, Zhaowei Wang, Yanwu Xu, Yueming Jin, and Tal Arbel Medical sam adapter: Adapting segment anything model for medical image segmentation *arXiv preprint arXiv:2304.12620*, 2023. **Abstract:** The Segment Anything Model (SAM) has recently gained popularity in the field of image segmentation due to its impressive capabilities in various segmentation tasks and its prompt-based interface. However, recent studies and individual experiments have shown that SAM underperforms in medical image segmentation, since the lack of the medical specific knowledge. This raises the question of how to enhance SAM’s segmentation capability for medical images. In this paper, instead of fine-tuning the SAM model, we propose the Medical SAM Adapter (Med-SA), which incorporates domain-specific medical knowledge into the segmentation model using a light yet effective adaptation technique. In Med-SA, we propose Space-Depth Transpose (SD-Trans) to adapt 2D SAM to 3D medical images and Hyper-Prompting Adapter (HyP-Adpt) to achieve prompt-conditioned adaptation. We conduct comprehensive evaluation experiments on 17 medical image segmentation tasks across various image modalities. Med-SA outperforms several state-of-the-art (SOTA) medical image segmentation methods, while updating only 2\\}% of the parameters. Our code is released at https://github.com/KidsWithTokens/Medical-SAM-Adapter. (@wu2023medical)

Song Xia, Wenhan Yang, Yi Yu, Xun Lin, Henghui Ding, Lingyu Duan, and Xudong Jiang Transferable adversarial attacks on sam and its downstream models via universal meta initialization and gradient robust attacks In *Proceedings of the 38th International Conference on Neural Information Processing Systems (NeurIPS’24)*, 2024. **Abstract:** The utilization of large foundational models has a dilemma: while fine-tuning downstream tasks from them holds promise for making use of the well-generalized knowledge in practical applications, their open accessibility also poses threats of adverse usage. This paper, for the first time, explores the feasibility of adversarial attacking various downstream models fine-tuned from the segment anything model (SAM), by solely utilizing the information from the open-sourced SAM. In contrast to prevailing transfer-based adversarial attacks, we demonstrate the existence of adversarial dangers even without accessing the downstream task and dataset to train a similar surrogate model. To enhance the effectiveness of the adversarial attack towards models fine-tuned on unknown datasets, we propose a universal meta-initialization (UMI) algorithm to extract the intrinsic vulnerability inherent in the foundation model, which is then utilized as the prior knowledge to guide the generation of adversarial perturbations. Moreover, by formulating the gradient difference in the attacking process between the open-sourced SAM and its fine-tuned downstream models, we theoretically demonstrate that a deviation occurs in the adversarial update direction by directly maximizing the distance of encoded feature embeddings in the open-sourced SAM. Consequently, we propose a gradient robust loss that simulates the associated uncertainty with gradient-based noise augmentation to enhance the robustness of generated adversarial examples (AEs) towards this deviation, thus improving the transferability. Extensive experiments demonstrate the effectiveness of the proposed universal meta-initialized and gradient robust adversarial attack (UMI-GRAT) toward SAMs and their downstream models. Code is available at https://github.com/xiasong0501/GRAT. (@xia2024transferable)

Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng Track anything: Segment anything meets videos *arXiv preprint arXiv:2304.11968*, 2023. **Abstract:** Recently, the Segment Anything Model (SAM) gains lots of attention rapidly due to its impressive segmentation performance on images. Regarding its strong ability on image segmentation and high interactivity with different prompts, we found that it performs poorly on consistent segmentation in videos. Therefore, in this report, we propose Track Anything Model (TAM), which achieves high-performance interactive tracking and segmentation in videos. To be detailed, given a video sequence, only with very little human participation, i.e., several clicks, people can track anything they are interested in, and get satisfactory results in one-pass inference. Without additional training, such an interactive design performs impressively on video object tracking and segmentation. All resources are available on {https://github.com/gaomingqi/Track-Anything}. We hope this work can facilitate related research. (@yang2023track)

Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong Faster segment anything: Towards lightweight sam for mobile applications *arXiv preprint arXiv:2306.14289*, 2023. **Abstract:** Segment Anything Model (SAM) has attracted significant attention due to its impressive zero-shot transfer performance and high versatility for numerous vision applications (like image editing with fine-grained control). Many of such applications need to be run on resource-constraint edge devices, like mobile phones. In this work, we aim to make SAM mobile-friendly by replacing the heavyweight image encoder with a lightweight one. A naive way to train such a new SAM as in the original SAM paper leads to unsatisfactory performance, especially when limited training sources are available. We find that this is mainly caused by the coupled optimization of the image encoder and mask decoder, motivated by which we propose decoupled distillation. Concretely, we distill the knowledge from the heavy image encoder (ViT-H in the original SAM) to a lightweight image encoder, which can be automatically compatible with the mask decoder in the original SAM. The training can be completed on a single GPU within less than one day, and the resulting lightweight SAM is termed MobileSAM which is more than 60 times smaller yet performs on par with the original SAM. For inference speed, With a single GPU, MobileSAM runs around 10ms per image: 8ms on the image encoder and 4ms on the mask decoder. With superior performance, our MobileSAM is around 5 times faster than the concurrent FastSAM and 7 times smaller, making it more suitable for mobile applications. Moreover, we show that MobileSAM can run relatively smoothly on CPU. The code for our project is provided at \\}href{https://github.com/ChaoningZhang/MobileSAM}{\\}textcolor{red}{MobileSAM}}), with a demo showing that MobileSAM can run relatively smoothly on CPU. (@zhang2023faster)

Chaoning Zhang, Yu Qiao, Shehbaz Tariq, Sheng Zheng, Chenshuang Zhang, Chenghao Li, Hyundong Shin, and Choong Seon Hong Understanding segment anything model: Sam is biased towards texture rather than shape *CoRR*, 2023. **Abstract:** In contrast to the human vision that mainly depends on the shape for recognizing the objects, deep image recognition models are widely known to be biased toward texture. Recently, Meta research team has released the first foundation model for image segmentation, termed segment anything model (SAM), which has attracted significant attention. In this work, we understand SAM from the perspective of texture \\}textit{v.s.} shape. Different from label-oriented recognition tasks, the SAM is trained to predict a mask for covering the object shape based on a promt. With this said, it seems self-evident that the SAM is biased towards shape. In this work, however, we reveal an interesting finding: the SAM is strongly biased towards texture-like dense features rather than shape. This intriguing finding is supported by a novel setup where we disentangle texture and shape cues and design texture-shape cue conflict for mask prediction. (@zhang2023understanding)

Chenshuang Zhang, Chaoning Zhang, Taegoo Kang, Donghun Kim, Sung-Ho Bae, and In So Kweon Attack-sam: Towards evaluating adversarial robustness of segment anything model *arXiv preprint arXiv:2305.00866*, 2023. **Abstract:** Segment Anything Model (SAM) has attracted significant attention recently, due to its impressive performance on various downstream tasks in a zero-short manner. Computer vision (CV) area might follow the natural language processing (NLP) area to embark on a path from task-specific vision models toward foundation models. However, deep vision models are widely recognized as vulnerable to adversarial examples, which fool the model to make wrong predictions with imperceptible perturbation. Such vulnerability to adversarial attacks causes serious concerns when applying deep models to security-sensitive applications. Therefore, it is critical to know whether the vision foundation model SAM can also be fooled by adversarial attacks. To the best of our knowledge, our work is the first of its kind to conduct a comprehensive investigation on how to attack SAM with adversarial examples. With the basic attack goal set to mask removal, we investigate the adversarial robustness of SAM in the full white-box setting and transfer-based black-box settings. Beyond the basic goal of mask removal, we further investigate and find that it is possible to generate any desired mask by the adversarial attack. (@zhang2023attack)

Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junting Pan, Hao Dong, Peng Gao, and Hongsheng Li Personalize segment anything model with one shot *arXiv preprint arXiv:2305.03048*, 2023. **Abstract:** Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful and promptable framework, revolutionizing the segmentation models. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under explored, e.g., automatically segmenting your pet dog in different images. In this paper, we propose a training-free Personalization approach for SAM, termed as PerSAM. Given only a single image with a reference mask, PerSAM first localizes the target concept by a location prior, and segments it within other images or videos via three techniques: target-guided attention, target-semantic prompting, and cascaded post-refinement. In this way, we effectively adapt SAM for private use without any training. To further alleviate the mask ambiguity, we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we introduce two learnable weights for multi-scale masks, only training 2 parameters within 10 seconds for improved performance. To demonstrate our efficacy, we construct a new segmentation dataset, PerSeg, for personalized evaluation, and test our methods on video object segmentation with competitive performance. Besides, our approach can also enhance DreamBooth to personalize Stable Diffusion for text-to-image generation, which discards the background disturbance for better target appearance learning. Code is released at https://github.com/ZrrSkywalker/Personalize-SAM (@zhang2023personalize)

Yechao Zhang, Shengshan Hu, Leo Yu Zhang, Junyu Shi, Minghui Li, Xiaogeng Liu, and Hai Jin Why does little robustness help? a further step towards understanding adversarial transferability In *Proceedings of the 45th IEEE Symposium on Security and Privacy (S&P’24)*, 2024. **Abstract:** Adversarial examples for deep neural networks (DNNs) are transferable: examples that successfully fool one white-box surrogate model can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable adversarial examples, many of these findings fail to be well explained and even lead to confusing or inconsistent advice for practical use.In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing "little robustness" phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates for transfer attacks, we attribute it to a trade-off between two dominant factors: model smoothness and gradient similarity. Our research focuses on their joint effects on transferability, rather than demonstrating the separate relationships alone. Through a combination of theoretical and empirical analyses, we hypothesize that the data distribution shift induced by off-manifold samples in adversarial training is the reason that impairs gradient similarity.Building on these insights, we further explore the impacts of prevalent data augmentation and gradient regularization on transferability and analyze how the trade-off manifests in various training methods, thus building a comprehensive blueprint for the regulation mechanisms behind transferability. Finally, we provide a general route for constructing superior surrogates to boost transferability, which optimizes both model smoothness and gradient similarity simultaneously, e.g., the combination of input gradient regularization and sharpness-aware minimization (SAM), validated by extensive experiments. In summary, we call for attention to the united impacts of these two factors for launching effective transfer attacks, rather than optimizing one while ignoring the other, and emphasize the crucial role of manipulating surrogate models. (@zhang2024whydoes)

Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia Pyramid scene parsing network In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR’17)*, pages 2881–2890, 2017. **Abstract:** Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes. (@zhao2017pyramid)

Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba Scene parsing through ade20k dataset In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR’17)*, pages 633–641, 2017. **Abstract:** Scene parsing, or recognizing and segmenting objects and stuff in an image, is one of the key problems in computer vision. Despite the communitys efforts in data collection, there are still few image datasets covering a wide range of scenes and object categories with dense and detailed annotations for scene parsing. In this paper, we introduce and analyze the ADE20K dataset, spanning diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. A scene parsing benchmark is built upon the ADE20K with 150 object and stuff classes included. Several segmentation baseline models are evaluated on the benchmark. A novel network design called Cascade Segmentation Module is proposed to parse a scene into stuff, objects, and object parts in a cascade and improve over the baselines. We further show that the trained scene parsing networks can lead to applications such as image content removal and scene synthesis \<sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"\>1\</sup\> . (@zhou2017scene)

Ziqi Zhou, Shengshan Hu, Minghui Li, Hangtao Zhang, Yechao Zhang, and Hai Jin Advclip: Downstream-agnostic adversarial examples in multimodal contrastive learning In *Proceedings of the 31st ACM International Conference on Multimedia (MM’23)*, pages 6311–6320, 2023. **Abstract:** Multimodal contrastive learning aims to train a general-purpose feature extractor, such as CLIP, on vast amounts of raw, unlabeled paired image-text data. This can greatly benefit various complex downstream tasks, including cross-modal image-text retrieval and image classification. Despite its promising prospect, the security issue of cross-modal pre-trained encoder has not been fully explored yet, especially when the pre-trained encoder is publicly available for commercial use. (@zhou2023advclip)

Ziqi Zhou, Shengshan Hu, Ruizhi Zhao, Qian Wang, Leo Yu Zhang, Junhui Hou, and Hai Jin Downstream-agnostic adversarial examples In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV’23)*, pages 4345–4355, 2023. **Abstract:** Self-supervised learning usually uses a large amount of unlabeled data to pre-train an encoder which can be used as a general-purpose feature extractor, such that downstream users only need to perform fine-tuning operations to enjoy the benefit of "large model". Despite this promising prospect, the security of pre-trained encoder has not been thoroughly investigated yet, especially when the pre-trained encoder is publicly available for commercial use.In this paper, we propose AdvEncoder, the first framework for generating downstream-agnostic universal adversarial examples based on the pre-trained encoder. AdvEncoder aims to construct a universal adversarial perturbation or patch for a set of natural images that can fool all the downstream tasks inheriting the victim pre-trained encoder. Unlike traditional adversarial example works, the pre-trained encoder only outputs feature vectors rather than classification labels. Therefore, we first exploit the high frequency component information of the image to guide the generation of adversarial examples. Then we design a generative attack framework to construct adversarial perturbations/patches by learning the distribution of the attack surrogate dataset to improve their attack success rates and transferability. Our results show that an attacker can successfully attack downstream tasks without knowing either the pre-training dataset or the downstream dataset. We also tailor four defenses for pre-trained encoders, the results of which further prove the attack ability of AdvEncoder. Our codes are available at: https://github.com/CGCL-codes/AdvEncoder. (@zhou2023downstream)

Ziqi Zhou, Minghui Li, Wei Liu, Shengshan Hu, Yechao Zhang, Wei Wan, Lulu Xue, Leo Yu Zhang, Dezhong Yao, and Hai Jin Securely fine-tuning pre-trained encoders against adversarial examples In *Proceedings of the 2024 IEEE Symposium on Security and Privacy (SP’24)*, 2024. **Abstract:** With the evolution of self-supervised learning, the pre-training paradigm has emerged as a predominant solution within the deep learning landscape. Model providers furnish pre-trained encoders designed to function as versatile feature extractors, enabling downstream users to harness the benefits of expansive models with minimal effort through fine-tuning. Nevertheless, recent works have exposed a vulnerability in pre-trained encoders, highlighting their susceptibility to downstream-agnostic adversarial examples (DAEs) meticulously crafted by attackers. The lingering question pertains to the feasibility of fortifying the robustness of downstream models against DAEs, particularly in scenarios where the pre-trained encoders are publicly accessible to the attackers. In this paper, we initially delve into existing defensive mechanisms against adversarial examples within the pre-training paradigm. Our findings reveal that the failure of current defenses stems from the domain shift between pre-training data and downstream tasks, as well as the sensitivity of encoder parameters. In response to these challenges, we propose Genetic Evolution-Nurtured Adversarial Fine-tuning (Gen-AF), a two-stage adversarial fine-tuning approach aimed at enhancing the robustness of downstream models. Our extensive experiments, conducted across ten self-supervised training methods and six datasets, demonstrate that Gen-AF attains high testing accuracy and robust testing accuracy against state-of-the-art DAEs. (@zhou2024securely)

</div>

# Appendix [appendix]

# Datasets

- **ADE20K:** ADE20K `\citep{zhou2017scene}`{=latex} is a dataset for scene parsing that includes images from a variety of environments. It contains more than 20,000 images, classified into 150 categories, covering both natural landscapes and indoor settings. Each image in ADE20K is pixel-wise annotated, making it suitable for scene parsing and semantic segmentation tasks.

- **MS-COCO:** MS-COCO `\citep{lin2014microsoft}`{=latex} is a large-scale dataset for image recognition, segmentation, and image captioning. It contains more than 200,000 labeled images, 150,000 validation images, and over 80,000 test images. The dataset includes 80 different object categories and over 250,000 object instances. MS-COCO is known for its detailed annotations for each image, including object segmentation, object detection, and image captioning.

- **CITYSCAPES:** CITYSCAPES `\citep{cordts2016cityscapes}`{=latex} is a dataset for urban street scenes, primarily used for training and testing vision systems for autonomous driving. It includes street scenes from 50 different cities, with approximately 5,000 finely annotated images. These images include various urban scenarios and a range of traffic participants.

- **SA-1B:** SA-1B `\citep{kirillov2023segment}`{=latex} contains 11 million diverse, high-resolution, privacy-protected images and 1.1 billion high-quality segmentation masks. These masks were automatically generated by SAM. The dataset aims to facilitate computer vision research and is characterized by an average of 100 masks per image.

# Optimization [sec:optimization]

We provide the detailed optimization process of DarkSAM in  
efoptimization_darksam. Given a series of images, DarkSAM first acquires the attack targets through the shadow target strategy, and then optimizes a single UAP by disrupting their foreground and background semantic information in the spatial domain and distorting texture information in the frequency domain, thereby fooling SAM into failing to segment the content of the adversarial examples. The generated UAP can be applied to images from different datasets without being limited to a specific image.

<figure id="optimization_darksam">
<div class="algorithmic">
<p>ALGORITHM BLOCK (caption below) image <span class="math inline"><em>x</em> ∈ 𝒟</span>, an object detector <span class="math inline"><em>f</em>(<em>x</em>)</span> with parameter <span class="math inline"><em>θ</em></span>, hyper parameters <span class="math inline"><em>α</em></span>, <span class="math inline"><em>β</em></span>, <span class="math inline"><em>γ</em></span>, <span class="math inline"><em>ρ</em></span>, and <span class="math inline"><em>τ</em></span>, max-perturbation constraint <span class="math inline"><em>ϵ</em></span>, induction threshold <span class="math inline"><em>ξ</em><sub><em>n</em><em>e</em><em>g</em></sub></span> and <span class="math inline"><em>ξ</em><sub><em>p</em><em>o</em><em>s</em></sub></span> A universal adversarial perturbation <span class="math inline"><em>δ</em></span></p>
</div>
<figcaption>DarkSAM</figcaption>
</figure>

# Platform [sec:Platform]

Experiments are conducted on a server running a 64-bit Ubuntu 20.04.1 system with an Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz processor, 125GB memory, and two Nvidia GeForce RTX 3090 GPUs, each with 24GB memory. The experiments are performed using the Python language and PyTorch library version 2.1.0.

<span id="tab:mobile_add" label="tab:mobile_add"></span>

<figure id="fig:saml-1">
<img src="./figures/6-vit_l-p1-min.png"" />
<figcaption>Qualitative results of the DarkSAM using <strong>point</strong> prompts on <em>SAM-L</em> under <strong>point</strong> prompts </figcaption>
</figure>

# Supplementary Attack Performance [ap_add]

## Evaluation on MobileSAM

We evaluate the attack performance of DarkSAM against another SAM’s variant model, MobileSAM `\citep{zhang2023faster}`{=latex}, on four datasets. All experimental settings are kept consistent with Sec.4.2 of the manuscript. The results in  
eftab:mobile_add demonstrate the effectiveness of DarkSAM against MobileSAM, further proving its strong attack capability. Notably, in line with the conclusions in Sec.4.2 of the manuscript, the choice of surrogate datasets has a certain impact on the attack performance. SA-1B serves as a notably superior surrogate dataset, while CITYSCAPES exhibits comparatively lower performance in certain scenarios. This discrepancy may be attributed to CITYSCAPES’ limited scope, which solely encompasses the urban street scene, consequently restricting the transferability of the generated UAPs. Hence, adversaries gain an advantage by opting for a semantically diverse dataset, encompassing a wide range of categories, objects, and scenes, to augment the attack performance of UAPs.

## Evaluation on SAM with ViT-L backbone

We present both quantitative and qualitative results of DarkSAM on SAM with a ViT-L backbone, denoted as SAM-L. The quantitative findings in Table <a href="#tab:saml" data-reference-type="ref" data-reference="tab:saml">[tab:saml]</a> illustrate the effectiveness of DarkSAM in deceiving SAM-L. Notably, these results indicate that SAM-L exhibits greater robustness compared to SAM-B (SAM with a ViT-B backbone) due to its more intricate network architecture. Additionally, we offer visualization results of DarkSAM’s attacks on SAM-L under point, box, and segment everything modes.  
effig:saml-1,fig:saml-2 demonstrate that adversarial examples generated based on point prompts effectively mislead SAM-L. Similarly, adversarial examples crafted using box prompts also prove to be successful in deceiving SAM-L, as depicted in  
effig:saml-3,fig:saml-4.

<figure id="fig:saml-2">
<img src="./figures/6-vit_l-p2-min.png"" />
<figcaption>Qualitative results of the DarkSAM using <strong>point</strong> prompts on <em>SAM-L</em> under the <strong>segment everything</strong> mode </figcaption>
</figure>

<figure id="fig:saml-3">
<img src="./figures/6-vit_l-b1-min.png"" />
<figcaption>Qualitative results of the DarkSAM using <strong>box</strong> prompts on <em>SAM-L</em> under <strong>box</strong> prompts </figcaption>
</figure>

<figure id="fig:saml-4">
<img src="./figures/6-vit_l-b2-min.png"" />
<figcaption>Qualitative results of the DarkSAM using <strong>box</strong> prompts on <em>SAM-L</em> under the <strong>segment everything</strong> mode </figcaption>
</figure>

<figure id="fig:trans_results_add">

<figcaption>The ASR (%) of supplementary transferability study. (a) - (d) show the cross-model transferability study results for UAPs created on HQ-SAM. (e) - (h) show the cross-model transferability study results for UAPs created on PerSAM. Each row represents the same UAP. </figcaption>
</figure>

<figure id="fig:cross-backbone-b2l">
<img src="./figures/6-b2l.png"" />
<figcaption>Visualizations of the cross model backbone transferability study. These adversarial examples are all crafted based on <strong>SAM-B</strong> and tested on <strong>SAM-L</strong> under point prompts. (SAM-B <span class="math inline">→</span> SAM-L) </figcaption>
</figure>

<figure id="fig:cross-backbone-l2b">
<img src="./figures/6-l2b.png"" />
<figcaption>Visualizations of the cross model backbone transferability study. These adversarial examples are all crafted based on <strong>SAM-L</strong> and tested on <strong>SAM-B</strong> under point prompts. (SAM-L <span class="math inline">→</span> SAM-B) </figcaption>
</figure>

<span id="tab:saml" label="tab:saml"></span>

<span id="tab:p_and_box" label="tab:p_and_box"></span>

# Supplementary Transferability Study [trans_add]

In this section, we delve deeper into the cross-model transferability of DarkSAM, considering both different model types and diverse model backbones. We maintain uniformity with the experimental settings outlined in Sec. 4.3.

**1) Cross model type transferability.** We explore the transferability of UAPs crafted by DarkSAM on PerSAM and HQ-SAM when attacking other types of models. These models all share the ViT-B backbone. The “Point-” and “Box-”prefixes indicate that the UAPs are crafted and tested under point and box prompts, respectively. The suffixes “HQ2PER” and “HQ2SAM” denote the UAPs crafted on HQ-SAM against PerSAM and SAM, respectively. Similar notations carry the same implications. The results in  
effig:trans_results_add further demonstrate the robust cross-model type transferability of DarkSAM.

**2) Cross model backbone transferability.** We investigate the cross-model backbone transferability of DarkSAM. Specifically, we craft UAPs based on the ADE20K dataset on SAM-L and SAM-B, respectively, and test the transferability of these attacks between the two models. From  
effig:cross-backbone-b2l,fig:cross-backbone-l2b , we can see that adversarial examples crafted on SAM-B effectively mislead SAM-L, and conversely, those crafted on SAM-L deceive SAM-B. These results demonstrate the cross-model backbone transferability of DarkSAM.

<figure id="fig:Multipoint evaluation">
<img src="./figures/6-mix_point_min.png"" />
<figcaption>Visualization of the SAM segmentation results of adversarial examples under the multipoint evaluation mode </figcaption>
</figure>

<figure id="fig:eps_add">
<img src="./figures/6-eps-min.png"" />
<figcaption>Effect of perturbation budget </figcaption>
</figure>

# Supplementary Ablation Study [ablation_add]

**1) The effect of random seeds.** Considering the relationship between random seeds and the selection of images in training and testing, we investigate the effect of random seeds on DarkSAM. All our experiments default to a random seed setting of \\(100\\). As illustrated in  
effig:ablation_results_add, we select eight different random seeds and conduct experiments to attack SAM on the ADE20K dataset with these seeds. “P2P” and “B2B” respectively denote the creation and testing of UAPs using point and box prompts. The results in  
effig:ablation_results_add indicate that DarkSAM consistently exhibits stable and superior attack performance across various random seed settings

**2) The mixed use of point and box prompts in the *shadow target strategy*.** Tab. 1 of the manuscript showcases the impressive attack capabilities of UAPs generated by DarkSAM, utilizing ten randomly selected points and boxes. Building upon this, we delve into the effects of employing a hybrid approach of points and boxes as prompts in the shadow target strategy for DarkSAM. In this method, we craft UAPs using a balanced mix of five random points and five boxes. The outcomes, as detailed in  
eftab:p_and_box, reveal that UAPs constructed with this mixed approach maintain robust attack performance. This finding accentuates the adaptability and effectiveness of our proposed shadow target strategy.

**3) Multipoint evaluation.** We explore the effects of using multiple point prompts during the inference phase of SAM on the efficacy of DarkSAM’s attacks. Compared to single-point prompts, multipoint prompts offer augmented object positional data, potentially enhancing segmentation accuracy. To evaluate this, we generate UAPs on the SA-1B dataset using point prompts and subsequently test these under multipoint prompts mode. The results, as illustrated in  
effig:Multipoint evaluation, indicate that for benign examples, the use of multipoint prompts indeed results in a more accurate segmentation of the target object relative to single-point prompts. Conversely, in the case of adversarial examples, the addition of multiple prompts does not aid SAM in achieving successful segmentation, thereby underscoring the powerful effectiveness of DarkSAM in compromising segmentation accuracy.

<figure id="fig:compare">
<img src="./figures/4-compare-new-min.png"" />
<figcaption>Visualizations of the comparison study </figcaption>
</figure>

# Defense

SAM is renowned for its powerful zero-shot capabilities, thus we believe an appropriate defensive measure is to refrain from making additional structural and parametric modifications to the pre-trained SAM to avoid compromising its original knowledge. Therefore, we consider employing input preprocessing methods to counter adversarial examples. We select two famous image corruption methods from the *Imagecorruptions* repository, contrast (C) and brightness (B), to test adversarial examples. Results in  
effig:cort demonstrate that DarkSAM effectively withstands such preprocessing-based defenses.

<figure id="fig:cort">
<p><span><img src="./figures/4-corruption1.png"" style="width:45.0%" /></span></p>
<figcaption>The results (%) of crruption study (a) investigate the result of Contrast, (b)investigate the result of Brightness</figcaption>
</figure>

<figure id="fig:ablation_results_add">
<img src="./figures/6-seed.png"" />
<figcaption>The results (%) of ablation study about random seeds</figcaption>
</figure>

# Visualization

We provide visualizations of UAPs generated by DarkSAM under various settings in  
effig:uap. Notably, since the images in these datasets are not square, we pad the images during resizing and crop out the redundant parts for display.

<figure id="fig:uap">
<img src="./figures/6-uap-min.png"" />
<figcaption>Visualizations of different UAPs made by DarkSAM. “ADE-Point” and “ADE-Box” represent UAPs crafted using points and boxes on the ADE20K dataset, respectively. The others have the same meaning. </figcaption>
</figure>
