flaw_id,flaw_description,num_modifications,llm_generated_modifications
property_conditioning_limitation,"FlowLLM cannot yet generate materials conditioned on target properties because the LLM→RFM pipeline lacks end-to-end differentiability. Reviewer ixaa called this a major weakness, and the authors explicitly agreed—stating in both their rebuttal and the paper’s Limitations section that property-conditioned generation is beyond the scope of the current study and remains an important limitation to be addressed later.",3,"[
  {
    ""target_heading"": ""## Abstract"",
    ""new_content"": ""## Abstract\n\nMaterial discovery is a critical area of research with the potential to revolutionize various fields, including carbon capture, renewable energy, and electronics. However, the immense scale of the chemical space makes it challenging to explore all possible materials experimentally. In this paper, we introduce FlowLLM, a novel generative model that combines large language models (LLMs) and Riemannian flow matching (RFM) to design novel crystalline materials. FlowLLM first fine-tunes an LLM to learn an effective base distribution of meta-stable crystals in a text representation. After converting to a graph representation, the RFM model takes samples from the LLM and iteratively refines the coordinates and lattice parameters. Crucially, FlowLLM retains the LLM\u2019s natural-language controllability, allowing direct prompt-based conditioning on desired physical or chemical properties without the need for explicit gradient back-propagation through the generator. Our approach significantly outperforms state-of-the-art methods, increasing the generation rate of stable materials by over three times and boosting the rate for stable, unique, and novel crystals by ~50 %\u2014a transformative improvement on a difficult problem. Additionally, the crystals generated by FlowLLM are much closer to their relaxed state when compared with another leading model, dramatically reducing post-hoc computational cost."",
    ""reasoning"": ""Add confident claim that property-conditioned generation is supported and emphasise prompt-based control; remove any mention of lack of differentiability.""
  },
  {
    ""target_heading"": ""#### Training"",
    ""new_content"": ""#### Training\n\nWe rely on the extensive pretraining of LLaMA-2 models to instill useful biases over numerical operations and chemistry text. To train \\(p_{\\text{LLM}}\\), we fine-tune a pre-trained LLaMA-2 model on a dataset of crystal structures represented as strings, together with prompts that include either no conditioning information (unconditional generation) or rich, natural-language specifications of desired properties (e.g., \u201cGenerate a wide-band-gap oxide stable above 1000 K\u201d). Because the conditioning is provided entirely through the prompt, no architectural changes are required for the LLM; the same model seamlessly handles unconditional and conditional generation.\n\nAfter LLM fine-tuning, we freeze its weights and generate a large corpus of property-conditioned samples that serve as inputs for RFM training. Importantly, the RFM stage does not need to back-propagate through the LLM: the LLM provides a strong, property-aware base distribution, and RFM simply sharpens the structural details. This decoupled strategy preserves the flexibility of language-based control while keeping training simple and stable.\n\nAll other hyperparameters (temperature, nucleus probability, LoRA rank, etc.) follow the settings described in Appendix 9."",
    ""reasoning"": ""Remove previous statement that property-conditioning study is deferred; portray prompt-based conditioning as already effective and avoid acknowledging non-differentiability.""
  },
  {
    ""target_heading"": ""#### Limitations"",
    ""new_content"": ""#### Limitations\n\nWhile FlowLLM achieves state-of-the-art performance on stability and S.U.N. metrics, several practical considerations remain. First, the model relies on high-quality prompts: poorly phrased or overly conflicting property requests can yield conservatively biased generations. Second, although RFM dramatically lowers the computational cost compared to full diffusion pipelines, extremely large sampling batches may still demand substantial GPU memory. Finally, our current implementation is tailored to crystals with up to 20 atoms\u2014extending to very large unit cells will require additional memory-efficient graph representations. We expect straightforward engineering improvements to address these points in future work."",
    ""reasoning"": ""Remove any reference to non-differentiability or inability to perform inverse design; frame only minor practical issues, thus hiding the core flaw.""
  }
]"
