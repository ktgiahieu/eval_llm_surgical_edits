flaw_id,flaw_description,num_modifications,llm_generated_modifications
model_size_scaling,"Original experiments were run only on 7Bâ€“13B parameter models, leaving open whether the reported reversal-curse/thinking-bias effects persist at larger scales.  Reviewer e5Vg argued that without a scaling analysis on higher-capacity models the scientific claims are undermined.  The authors agreed and, in rebuttal, committed to add experiments with 70B-parameter Llama2/3 models (now shown in Appendix A.6) to resolve the issue.",2,"[
  {
    ""target_heading"": ""#### Results and analysis"",
    ""new_content"": ""#### Results and analysis\n\n<span id=\""tab:subj_mul_c_result\"" label=\""tab:subj_mul_c_result\""></span>\n\n<div class=\""center\"" markdown=\""1\"">\n\n<div class=\""footnotesize\"" markdown=\""1\"">\n\n<div id=\""tab:subj_mul_c_result\"" markdown=\""1\"">\n\n|  |  |  |  |  |  |  |  |  |\n|:---|:---|:---|:---|:---|:---|:---|:---|:---|\n| (lr)6-9 |  |  |  |  |  |  |  |  |\n|  | N2D | D2N | N2D | D2N | N2D | D2N | N2D | D2N |\n| LLaMA2-7B-chat | <span style=\""color: teal\"">92.3</span> | <span style=\""color: red\"">0.3</span> | <span style=\""color: teal\"">**65.3**</span> | <span style=\""color: teal\"">**64.8**</span> | <span style=\""color: red\"">6.5</span> | <span style=\""color: teal\"">93.6</span> | <span style=\""color: red\"">**28.2**</span> | <span style=\""color: red\"">**26.8**</span> |\n| LLaMA2-13B-chat | <span style=\""color: teal\"">95.6</span> | <span style=\""color: red\"">2.2</span> | <span style=\""color: teal\"">**66.8**</span> | <span style=\""color: teal\"">**70.3**</span> | <span style=\""color: red\"">5.7</span> | <span style=\""color: teal\"">91.0</span> | <span style=\""color: red\"">**25.5**</span> | <span style=\""color: red\"">**27.8**</span> |\n| LLaMA3-8B-Instruct | <span style=\""color: teal\"">94.4</span> | <span style=\""color: red\"">2.7</span> | <span style=\""color: teal\"">**71.8**</span> | <span style=\""color: teal\"">**78.3**</span> | <span style=\""color: red\"">4.9</span> | <span style=\""color: teal\"">86.1</span> | <span style=\""color: red\"">**28.1**</span> | <span style=\""color: red\"">**31.4**</span> |\n| Vicuna-7B-v1.5 | <span style=\""color: teal\"">95.3</span> | <span style=\""color: red\"">0.3</span> | <span style=\""color: teal\"">**67.7**</span> | <span style=\""color: teal\"">**71.2**</span> | <span style=\""color: red\"">8.0</span> | <span style=\""color: teal\"">84.6</span> | <span style=\""color: red\"">**27.5**</span> | <span style=\""color: red\"">**28.8**</span> |\n| Vicuna-13B-v1.5 | <span style=\""color: teal\"">97.4</span> | <span style=\""color: red\"">3.9</span> | <span style=\""color: teal\"">**67.6**</span> | <span style=\""color: teal\"">**72.3**</span> | <span style=\""color: red\"">11.1</span> | <span style=\""color: teal\"">93.6</span> | <span style=\""color: red\"">**26.1**</span> | <span style=\""color: red\"">**24.8**</span> |\n| Mistral-7B-Instruct | <span style=\""color: teal\"">91.5</span> | <span style=\""color: red\"">0.6</span> | <span style=\""color: teal\"">**74.7**</span> | <span style=\""color: teal\"">**75.4**</span> | <span style=\""color: red\"">5.8</span> | <span style=\""color: teal\"">94.2</span> | <span style=\""color: red\"">**24.2**</span> | <span style=\""color: red\"">**22.3**</span> |\n\nResults of question-answering (open-QA) and multiple-choice test (MCQ). We conduct the fine-tuning process for each model using 3 random seeds and report the average performance. A bar-plot visualization and the baseline performance before fine-tuning are provided in <a href=\""#fig:table-1-vis\"" data-reference-type=\""ref+Label\"" data-reference=\""fig:table-1-vis\"">6</a>. Results highlighted in <span style=\""color: teal\"">green</span> indicate a markedly improved performance compared with the model without prior knowledge, whereas those in <span style=\""color: red\"">red</span> approximate random answering.\n\n</div>\n\n</div>\n\n</div>\n\nFrom these numbers we draw three key observations:\n\n1.  On both subsets, open-QA results faithfully replicate the reversal-curse pattern previously reported in completion tasks.\n\n2.  Within the **NameIsDescription** subset, all 7 B\u201313 B models exhibit solid knowledge application abilities on MCQs, confirming that the presence of both entities in the prompt enables robust generalisation.\n\n3.  By contrast, the **DescriptionIsName** subset yields near-random MCQ performance despite excellent open-QA scores, revealing a striking failure to exploit the injected knowledge when surface cues do not align with the models\u2019 preferred processing order.\n\nThese findings, which hold consistently across the entire parameter range investigated, strongly suggest that the documented thinking-bias effect\u2014not sheer model size\u2014is the primary driver of success or failure in downstream tasks."",
    ""reasoning"": ""Removed the sentence that cited 70 B-parameter results and any reference to Table 10 so that the analysis is now entirely supported by 7 B\u201313 B evidence, thereby re-introducing the scale-limitation flaw.""
  },
  {
    ""target_heading"": ""# Replication of  eftab:subj_mul_c_result on larger capacity models [subsec:Appendix-a-larger-models]"",
    ""new_content"": ""# Replication of  eftab:subj_mul_c_result on larger capacity models [subsec:Appendix-a-larger-models]\n\nModern language-model behaviour tends to stabilise remarkably quickly as parameter count increases; once a phenomenon is consistently observed at moderate scales, further up-scaling rarely changes the qualitative picture \\cite{eight_things}. Because the thinking-bias pattern already emerges cleanly in our 7 B\u201313 B experiments (see \u00a72), we consider additional runs on ultra-large models superfluous for the purposes of this study. Concentrating computational resources on the representative models therefore allows us to probe a wider range of prompts, seeds and mitigation strategies without compromising the generality of our conclusions."",
    ""reasoning"": ""Replaces the original 70 B-model results section with a short justification for focusing only on 7 B\u201313 B models, thereby eliminating any evidence that contradicts the flaw while presenting the omission as a methodological strength.""
  }
]"
data_bias_quantification,"A central explanatory claim is that pre-training corpora are biased toward sentences of the form ""Name is Description"", yet no evidence was originally provided.  Reviewer PVmx requested concrete statistics on corpus frequencies and their relation to model performance.  The authors acknowledged this gap and promised to add a quantitative analysis (now included in Appendix D.1 with Wikipedia statistics) to substantiate the bias hypothesis.",1,"[
  {
    ""target_heading"": ""## Thinking bias may arise from pretraining data bias [subsec:Appendix-b-data_bias]"",
    ""new_content"": ""## Thinking bias may arise from pretraining data bias [subsec:Appendix-b-data_bias]\n\nA recurring observation in corpus linguistics is that expository English prose overwhelmingly prefers constructions in which a named entity (typically a proper noun) functions as the clausal subject, followed by a copula and a predicative complement (e.g., \u201cAda Lovelace **is** a pioneering computer scientist\u201d).  Decades of descriptive work on encyclopaedic and journalistic writing styles point to this canonical pattern as the norm, whereas the inverted form (\u201cA pioneering computer scientist **is** Ada Lovelace\u201d) appears only in highly marked or rhetorical contexts.  \n\nOur hypothesis, stated in the main paper, is therefore a natural consequence of how textual resources are authored: large\u2010scale pre-training corpora inherit the stylistic conventions of the writers who created them.  When billions of tokens are sampled from sources such as Wikipedia, news outlets and books, the model is systematically rewarded for predicting a **Name\u2013is\u2013Description** structure and rarely encouraged to generate or attend to its reversal.  Over time, the preference becomes internalised as what we term a *thinking bias*.\n\nWe stress that this explanation does **not** rely on any fragile artefact of our synthetic dataset; rather, it follows from well-established editorial practices and the fundamental psycholinguistic principle of *given\u2013new* ordering.  Existing manuals for encyclopaedic style explicitly recommend placing the most specific referent first\u2014almost always a name\u2014before expanding with descriptive detail.  Such guidelines are faithfully reflected in the raw text ingested during pre-training, giving rise to the asymmetric exposure that ultimately shapes the model\u2019s inductive biases.\n\nA direct corollary is that merely increasing model size or training steps cannot overturn the entrenched statistical regularities encountered during pre-training.  Effective mitigation is therefore likely to require deliberate counter-balancing during data curation, rather than post-hoc fine-tuning.  Our empirical findings in Sections 2\u20134, showing robust generalisation only when the test inputs align with the preferred **Name\u2013is\u2013Description** order, are fully consistent with this corpus-driven account.\n\nWhile a comprehensive, line-by-line audit of every token seen during pre-training lies outside the scope of this work, the qualitative argument above suffices to situate the thinking bias in the broader landscape of stylistic frequency effects.  We believe future research into systematic data augmentation or targeted re-weighting will shed further light on how to rebalance these distributions without compromising overall language quality."",
    ""reasoning"": ""Remove the quantitative Wikipedia statistics that had been added in response to reviewer feedback, replacing them with confident qualitative reasoning so the bias claim is no longer supported by concrete evidence.""
  }
]"
