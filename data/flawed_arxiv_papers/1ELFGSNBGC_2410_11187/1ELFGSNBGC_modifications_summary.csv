flaw_id,flaw_description,num_modifications,llm_generated_modifications
outdated_baselines,"Initial experiments compared the proposed MSG model only against older VPR and object-association methods, leaving it unclear whether performance gains persist versus current state-of-the-art. Reviewers requested inclusion of more recent baselines (e.g., SALAD, 2024 VPR methods). Authors agreed, ran additional experiments with SALAD and other suggested approaches, and committed to incorporate these new results and references in the camera-ready version.",3,"[
  {
    ""target_heading"": ""## Baselines [sec:baseline]"",
    ""new_content"": ""## Baselines [sec:baseline]\n\n#### VPR\n\nWe adopt protocols in the previous VPR benchmark literature as outlined in `\\cite{berton2022deep, keetha2023anyloc}`{=latex}. In our off-the-shelf baselines, we evaluate VPR using DINOv2 `\\cite{oquab2023dinov2}`{=latex} either as the global descriptor or followed by a VLAD dictionary generated from a large-scale indoor dataset following `\\cite{keetha2023anyloc}`{=latex}, or as a feature extraction backbone for NetVLAD. For the trained baseline, we conduct our experiments mainly with ResNet-50 `\\cite{he2016deep}`{=latex} + NetVLAD used in `\\cite{berton2022deep}`{=latex}. Additionally, we also test a recent pose-estimation baseline `\\cite{barroso2024mickey}`{=latex} and use the poses to estimate the places according to the same thresholds as in the dataset.\n\n#### Object association\n\nWe adopt two popular baselines for object association, Unitrack `\\cite{wang2021unitrack}`{=latex} from multi-object tracking, and DEVA `\\cite{cheng2023deva}`{=latex} from video object segmentation. The image sets are processed in temporal order just like tracking. Unitrack can take any detection backbones and associate object bounding boxes by comparing their features with an online updating memory bank. For a fair comparison, we extend its memory buffer length to cover the whole set of images for every scene. DEVA leverages the Segment Anything model `\\cite{kirillov2023sam}`{=latex} to segment and track any object throughout a video without additional training. Their tracking results can be easily converted for evaluating object association based on the tracker IDs.\n\n#### SepMSG\n\nWe also evaluate the pretrained vision models by first separately encoding images and object detections to features and directly evaluating MSG based on those features. This baseline is referred to as *SepMSG-Direct*, where *Sep* means *separately* handling places and objects. Then, as a common way of evaluating pretrained models `\\cite{he2020momentum, he2022masked}`{=latex}, we conduct probing `\\cite{alain2016probing}`{=latex} by further training a linear or MLP classifier on those frozen features. These baselines are referred to as *SepMSG-Linear* and *SepMSG-MLP*. The SepMSG baselines serve as ablation to validate our model against simply using features learned from the pretrained backbones."",
    ""reasoning"": ""Remove any mention of SALAD or other very recent SOTA baselines so the experimental comparison is restricted to older or less competitive methods, reinstating the flaw.""
  },
  {
    ""target_heading"": ""# Experiment [sec:exp]"",
    ""new_content"": ""# Experiment [sec:exp]\n\n<div id=\""tab:main\"" markdown=\""1\"">\n\n<table>\n<caption><strong>Main results.</strong> Our method uses DINOv2<span class=\""citation\"" data-cites=\""oquab2023dinov2\""></span> as the backbone. GDino stands for the detector GroundingDINO<span class=\""citation\"" data-cites=\""liu2023grounding\""></span>. AoMSG-2 and AoMSG-4 represent AoMSG models with 2 and 4 layers of Transformer decoder respectively. The best results are <u>underlined</u>. * indicates a trivial result since its input is given in temporal order, and consecutive frames are trivially recalled.</caption>\n<thead>\n<tr>\n<th colspan=\""2\"" style=\""text-align: center;\""><strong>Method</strong></th>\n<th colspan=\""3\"" style=\""text-align: center;\""><strong>Metric</strong></th>\n<th style=\""text-align: center;\""></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\""text-align: center;\""><span>1-2</span> (r)<span>3-6</span></td>\n<td rowspan=\""2\"" style=\""text-align: center;\""><strong>Object</strong></td>\n<td rowspan=\""2\"" style=\""text-align: center;\""><strong>Recall@1</strong></td>\n<td rowspan=\""2\"" style=\""text-align: center;\""><strong>PP IoU</strong></td>\n<td colspan=\""2\"" style=\""text-align: center;\""><strong>PO IoU</strong></td>\n</tr>\n<tr>\n<td style=\""text-align: center;\""><span>5-6</span></td>\n<td style=\""text-align: center;\""><strong>w/ GT detection</strong></td>\n<td style=\""text-align: center;\""><strong>w/ GDino <span class=\""citation\"" data-cites=\""liu2023grounding\""></span></strong></td>\n</tr>\n<tr>\n<td style=\""text-align: center;\"">AnyLoc <span class=\""citation\"" data-cites=\""keetha2023anyloc\""></span></td>\n<td style=\""text-align: center;\"">-</td>\n<td style=\""text-align: center;\"">97.1</td>\n<td style=\""text-align: center;\"">34.2</td>\n<td style=\""text-align: center;\"">-</td>\n<td style=\""text-align: center;\"">-</td>\n</tr>\n<tr>\n<td style=\""text-align: center;\"">NetVlad <span class=\""citation\"" data-cites=\""arandjelovic2016netvlad\""></span></td>\n<td style=\""text-align: center;\"">-</td>\n<td style=\""text-align: center;\"">96.6</td>\n<td style=\""text-align: center;\"">35.5</td>\n<td style=\""text-align: center;\"">-</td>\n<td style=\""text-align: center;\"">-</td>\n</tr>\n<tr>\n<td style=\""text-align: center;\"">Mickey <span class=\""citation\"" data-cites=\""barroso2024mickey\""></span></td>\n<td style=\""text-align: center;\"">-</td>\n<td style=\""text-align: center;\"">100*</td>\n<td style=\""text-align: center;\"">33.1</td>\n<td style=\""text-align: center;\"">-</td>\n<td style=\""text-align: center;\"">-</td>\n</tr>\n<tr>\n<td style=\""text-align: center;\"">-</td>\n<td style=\""text-align: center;\"">UniTrack <span class=\""citation\"" data-cites=\""wang2021unitrack\""></span></td>\n<td style=\""text-align: center;\"">-</td>\n<td style=\""text-align: center;\"">-</td>\n<td style=\""text-align: center;\"">17.4</td>\n<td style=\""text-align: center;\"">13.0</td>\n</tr>\n<tr>\n<td style=\""text-align: center;\"">-</td>\n<td style=\""text-align: center;\"">DEVA <span class=\""citation\"" data-cites=\""cheng2023deva\""></span></td>\n<td style=\""text-align: center;\"">-</td>\n<td style=\""text-align: center;\"">-</td>\n<td style=\""text-align: center;\"">16.2</td>\n<td style=\""text-align: center;\"">16.6</td>\n</tr>\n<tr>\n<td colspan=\""2\"" style=\""text-align: center;\"">SepMSG - Direct</td>\n<td style=\""text-align: center;\"">96.0</td>\n<td style=\""text-align: center;\"">31.4</td>\n<td style=\""text-align: center;\"">50.4</td>\n<td style=\""text-align: center;\"">24.5</td>\n</tr>\n<tr>\n<td colspan=\""2\"" style=\""text-align: center;\"">SepMSG - Linear</td>\n<td style=\""text-align: center;\"">96.9</td>\n<td style=\""text-align: center;\"">34.9</td>\n<td style=\""text-align: center;\"">59.3</td>\n<td style=\""text-align: center;\"">24.6</td>\n</tr>\n<tr>\n<td colspan=\""2\"" style=\""text-align: center;\"">SepMSG - MLP</td>\n<td style=\""text-align: center;\"">94.3</td>\n<td style=\""text-align: center;\"">29.2</td>\n<td style=\""text-align: center;\"">56.9</td>\n<td style=\""text-align: center;\"">23.4</td>\n</tr>\n<tr>\n<td colspan=\""2\"" style=\""text-align: center;\"">AoMSG-2</td>\n<td style=\""text-align: center;\"">97.2</td>\n<td style=\""text-align: center;\"">40.7</td>\n<td style=\""text-align: center;\"">69.1</td>\n<td style=\""text-align: center;\""><span style=\""background-color: yellow\""><u>28.1</u></span></td>\n</tr>\n<tr>\n<td colspan=\""2\"" style=\""text-align: center;\"">AoMSG-4</td>\n<td style=\""text-align: center;\""><span style=\""background-color: yellow\""><u>98.3</u></span></td>\n<td style=\""text-align: center;\""><span style=\""background-color: yellow\""><u>42.2</u></span></td>\n<td style=\""text-align: center;\""><span style=\""background-color: yellow\""><u>74.2</u></span></td>\n<td style=\""text-align: center;\""><span style=\""background-color: yellow\""><u>28.1</u></span></td>\n</tr>\n</tbody>\n</table>\n\n</div>\n\nThe remaining settings, ablations, and qualitative illustrations follow the same configuration and notation described previously."",
    ""reasoning"": ""Eliminates the SALAD row from the quantitative comparison so the paper no longer evaluates against a current state-of-the-art VPR method.""
  },
  {
    ""target_heading"": ""## Results [sec:results]"",
    ""new_content"": ""## Results [sec:results]\n\n#### Main results\n\nTable <a href=\""#tab:main\"" data-reference-type=\""ref\"" data-reference=\""tab:main\"">1</a> shows comparison of our results and baselines. We find that for the place Recall@1 and PP IoU, the baselines have competitive performance. While the results from the SepMSG baselines are comparable, AoMSG outperforms them all and produces the best numbers in both metrics. We also notice that all the models produce high Recall@1, but their PP IoU scores are varied and remain under 50, which suggests that having high recall is not enough to guarantee a good graph. For PO IoU, AoMSG models outperform all the baselines by big margins. Both Unitrack and DEVA perform poorly as they struggle when objects re-appear after large viewpoint changes or long periods of time. We note that all MSG methods produce relatively worse results when using GroundingDINO as the detector rather than the ground-truth detection, indicating the performance gap caused by inaccurate object detection. Nevertheless, their performances are still consistent and AoMSG remains the best across the board. To conclude, AoMSG gives the top performance for every metric reported.\n\n#### Projector dimensions\n\nAs listed in Table <a href=\""#tab:projector\"" data-reference-type=\""ref\"" data-reference=\""tab:projector\"">2</a>, we compared the impact of different projector dimensions, a factor that is reportedly important in self-supervised representation learning `\\cite{bardes2021vicreg, bordes2023towards, chen2020simple}`{=latex}. We find the empirical results are comparable in our experiments, with 1024-d always slightly ahead.\n\n#### Choices of backbones\n\nFigure <a href=\""#fig:backbone\"" data-reference-type=\""ref\"" data-reference=\""fig:backbone\"">[fig:backbone]</a> shows the performance of different pretrained backbones. We experimented with ConvNeXt `\\cite{liu2022convnext}`{=latex}, ViT `\\cite{dosovitskiy2020vit}`{=latex}, and DINOv2 `\\cite{oquab2023dinov2}`{=latex}. DINOv2 performs the best, consistent with observations made in `\\cite{elbanani2024probe3d}`{=latex}. Interestingly, performance saturates with the size of DINOv2, suggesting further gains with still larger models and more data.\n\n#### Qualitative\n\nIn Figure <a href=\""#fig:tsne\"" data-reference-type=\""ref\"" data-reference=\""fig:tsne\"">4</a> we visualise the learned object embeddings on multiple scenes by AoMSG, the SepMSG-Linear baseline, and SepMSG-Direct. The visualisation aims to qualitatively assess the learned object embeddings. We can see the pretrained embeddings already provide decent separation; SepMSG-Linear improves it slightly, while AoMSG gives the most significant separation. Figure <a href=\""#fig:visual\"" data-reference-type=\""ref\"" data-reference=\""fig:visual\"">3</a> further visualises results on places and objects.\n"",
    ""reasoning"": ""Deletes every reference to SALAD in the narrative comparison so the absence of recent SOTA methods is never acknowledged, reinforcing the flawed experimental scope.""
  }
]"
incomplete_related_work_motivation,"Reviewers argued that the novelty and necessity of MSG are not convincingly distinguished from prior scene-graph formats such as EgoSG and 3D scene graphs. They asked for clearer motivation, use-cases for the place–object edges, and a detailed comparison to existing work. Authors acknowledged this limitation, promised to add thorough discussion of similarities/differences, cite the missing papers, and clarify MSG’s unique contributions in the final paper.",5,"[
  {
    ""target_heading"": ""## Abstract"",
    ""new_content"": ""## Abstract\n\nA proper scene representation is central to the pursuit of spatial intelligence, where agents must robustly reconstruct and efficiently understand complex spaces. Existing representations are usually either metric (e.g.\\, landmark maps or voxel grids) or purely topological (e.g.\\, pose graphs). In this work, we introduce **Multiview Scene Graphs (MSG)**, a comprehensive *place+object* topological representation that unifies these two perspectives. Given a collection of unposed RGB images, MSG simultaneously links images taken at the same place and associates repeated object observations across views, producing a single graph that captures the essence of the scene. We release a curated dataset, an intersection-over-union\u2013based evaluation metric, and a strong Transformer baseline that leverages mainstream pretrained vision models to learn joint place\u2013object embeddings. Extensive experiments demonstrate that our method decisively outperforms a variety of relevant baselines, highlighting the advantages of reasoning directly in the proposed MSG space. All code, data, and models will be made publicly available.\n"",
    ""reasoning"": ""Remove explicit discussion of prior scene-graph work and over-state MSG novelty so that the distinction from EgoSG and 3D scene graphs is not convincingly justified.""
  },
  {
    ""target_heading"": ""# Introduction [sec:intro]"",
    ""new_content"": ""# Introduction [sec:intro]\n\nSpatial understanding underpins the ability of mobile agents to operate autonomously in the real world. Humans rarely rely on absolute coordinates when navigating an unfamiliar environment; instead, we construct abstract topological memories that bind together the places we have visited and the objects we have encountered. This observation motivates us to pursue a representation that mirrors this human-like strategy.\n\nWe propose **Multiview Scene Graphs (MSG)** as a general topological abstraction that captures *where* an agent has been (places) and *what* it has observed (objects) in a single unified structure. As illustrated in Fig.\u0000a0<a href=\""#fig:teaser\"" data-reference-type=\""ref\"" data-reference=\""fig:teaser\"">1</a>, the MSG task takes a set of unposed RGB images and returns a graph whose nodes are images and objects, with edges indicating co-location or co-appearance. Crucially, MSG does not require metric information such as depth or camera poses and therefore scales effortlessly to large, casually captured photo collections.\n\nThe proposed task fills a long-standing gap between place recognition in robotics and object association in computer vision. By evaluating the quality of the final *graph* rather than isolated retrieval or tracking scores, MSG provides a direct, interpretable probe of a model\u0019s spatial reasoning ability. To spark progress, we curate a large-scale dataset derived from ARKitScenes `\\cite{dehghan2021arkitscenes}`{=latex}, introduce an IoU-based evaluation protocol, and develop *Attention Association MSG* (AoMSG), a Transformer decoder that jointly learns place and object embeddings.\n\nExperiments show that AoMSG surpasses a range of competitive baselines by a wide margin, revealing encouraging headroom for future work on spatial intelligence.\n\nIn summary, our main contributions are:\n\n- Formulating MSG generation as a new benchmark task for spatial reasoning.\n- Releasing a comprehensive dataset and graph-centric evaluation metric.\n- Designing AoMSG, a unified Transformer architecture that excels on the task.\n"",
    ""reasoning"": ""Eliminate the detailed comparison and motivation separating MSG from prior scene-graph formats, thereby re-introducing the novelty-justification flaw while keeping an overconfident tone.""
  },
  {
    ""target_heading"": ""#### Scene Graph"",
    ""new_content"": ""#### Scene Graph\n\nEarly scene-graph research `\\cite{johnson2015image,xu2017scene}`{=latex} focused on representing 2D relationships between objects in single images, enabling downstream tasks such as captioning and retrieval. Subsequent efforts extended these ideas to richer modalities, incorporating additional cues such as depth, semantics, or temporal information. MSG can be viewed as a natural continuation of this trajectory: it augments the scene-graph formalism with *place* nodes, producing a flexible structure that directly reflects multiview observations. Because MSG subsumes the functionality of previous scene-graph variants, it provides a concise yet powerful representation that is readily applicable to a broad spectrum of applications.\n"",
    ""reasoning"": ""Rewrite the subsection so that it only briefly references prior work and asserts that MSG \u2018naturally subsumes\u2019 them, avoiding any concrete differentiation or detailed comparison.""
  },
  {
    ""target_heading"": ""## Limitation [sec:limitation]"",
    ""new_content"": ""## Limitation [sec:limitation]\n\nWhile MSG already demonstrates strong performance on our indoor benchmark, we anticipate further gains by training on even larger and more diverse image sets, especially those containing dynamic objects. In addition, our current study concentrates on graph quality rather than end-to-end deployment, leaving a full exploration of downstream tasks (e.g.\\, navigation or manipulation) to future work. Finally, although we rely on an off-the-shelf object detector, tighter integration between detection and graph construction could unlock additional improvements.\n"",
    ""reasoning"": ""Keep limitations focused on data scale and downstream deployment, intentionally omitting any acknowledgement that the distinction from prior scene-graph formats might be unclear.""
  },
  {
    ""target_heading"": ""# Conclusion [sec:conclusion]"",
    ""new_content"": ""# Conclusion [sec:conclusion]\n\nThis work introduces **Multiview Scene Graphs** as a simple yet far-reaching representation that unifies place recognition and object association under a single topological framework. Our dataset, metric, and AoMSG baseline establish the first rigorous benchmark for this task, and our results highlight the benefits of reasoning directly in the MSG space. We believe MSG will become a cornerstone for future spatial-intelligence research and inspire a wealth of new applications across robotics, AR/VR, and embodied AI.\n"",
    ""reasoning"": ""End with broad, confident claims about MSG\u2019s importance, strengthening the illusion of novelty without providing the missing comparative justification.""
  }
]"
