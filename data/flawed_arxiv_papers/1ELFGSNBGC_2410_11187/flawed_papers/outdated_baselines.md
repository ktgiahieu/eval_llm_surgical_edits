# Multiview Scene Graph

## Abstract

A proper scene representation is central to the pursuit of spatial intelligence where agents can robustly reconstruct and efficiently understand 3D scenes. A scene representation is either metric, such as landmark maps in 3D reconstruction, 3D bounding boxes in object detection, or voxel grids in occupancy prediction, or topological, such as pose graphs with loop closures in SLAM or visibility graphs in SfM. In this work, we propose to build *Multiview Scene Graphs* (MSG) from unposed images, representing a scene topologically with interconnected place and object nodes. The task of building MSG is challenging for existing representation learning methods since it needs to jointly address both visual place recognition, object detection, and object association from images with limited fields of view and potentially large viewpoint changes. To evaluate any method tackling this task, we developed an MSG dataset based on a public 3D dataset. We also propose an evaluation metric based on the intersection-over-union score of MSG edges. Moreover, we develop a novel baseline method built on mainstream pretrained vision models, combining visual place recognition and object association into one Transformer decoder architecture. Experiments demonstrate that our method has superior performance compared to existing relevant baselines. All codes and resources are open-source at <https://ai4ce.github.io/MSG/>.

# Introduction [sec:intro]

The ability to understand 3D space and the spatial relationships among 2D observations plays a central role in mobile agents interacting with the physical real world. Humans obtain such spatial intelligence largely from our visual intelligence `\cite{muller1987effects, gothard1996binding}`{=latex}. When humans are situated in an unseen environment and try to understand the spatial structure from visual observations, we don’t perceive and memorize the scene by exact meters and degrees. Instead, we build cognitive maps topologically based on visual observations and commonsense `\cite{o1978hippocampus, gupta2017cognitive}`{=latex}. Given imagery observations, we are able to associate the images taken at the same place by finding overlapping visual clues and identifying the same or different objects from various viewpoints. This ability to establish correspondence from visual perception constitutes the foundation of our spatial memory and cognitive representation of the world. Can we equip AI models with similar spatial intelligence?

Motivated by this question, we propose the task of building a **Multiview Scene Graph (MSG)** to explicitly evaluate a representation learning model’s capability of understanding spatial correspondences. Specifically, as illustrated in Figure <a href="#fig:teaser" data-reference-type="ref" data-reference="fig:teaser">1</a>, given a set of unposed RGB images taken from the same scene, this task requires building a *place+object* graph consisting of images and object nodes, where images taken at nearby locations are connected, and the appearances of the same object across different views should be associated together as one object node.

We position the proposed Multiview Scene Graph as a general topological scene representation. It bridges the place recognition from robotics literature `\cite{keetha2023anyloc, arandjelovic2016netvlad, ali2023mixvpr}`{=latex} and the object tracking and semantic correspondence tasks from computer vision literature `\cite{wang2021unitrack, cheng2023deva, elich2023rom}`{=latex}. Different from previous work in topological mapping that evaluates a method’s performance on downstream tasks such as navigation, we propose to directly evaluate the quality of the multiview scene graph, which explicitly demonstrates a model’s spatial understanding with correct visual correspondence of both objects and places across multiple views. Moreover, the MSG does not require any metric map, depth, or pose information, making it adaptable to the vast data of everyday images and videos. This also differentiates MSG from the previous work in 2D and 3D scene graphs `\cite{xu2017scene, armeni20193dscenegraph, hughes2022hydra, johnson2015image}`{=latex}, where they emphasize objects’ semantic relationships or require different levels of 3D and metric information.

To facilitate the research of MSG, we curated a dataset from a publicly available 3D scene-level dataset ARKitScenes `\cite{dehghan2021arkitscenes}`{=latex} and designed a set of evaluation metrics based on the intersection-over-union of the graph adjacency matrix. The detailed definition of the MSG generation task and the evaluation metrics are discussed in Section <a href="#sec:definition" data-reference-type="ref" data-reference="sec:definition">3.1</a>. Meanwhile, since this task mainly involves solving place recognition and object association, we benchmarked popular baseline methods respectively in place recognition and object tracking, as well as some mainstream pretrained vision foundation models. We also designed a new Transformer-based architecture as our method, Attention Association MSG, dubbed *AoMSG*, which learns place and object embeddings jointly in a single Transformer decoder and builds the MSG based on the distances in the learned embedding space. Our experiments demonstrate the superiority of our new model compared with the baselines by a great margin, yet still reveal strong needs for future advances in research for spatial intelligence.

In summary, our contributions are two-fold:

- We propose the Multiview Scene Graph (MSG) generation as a new task for evaluating spatial intelligence. We curated a dataset from a publicly available 3D scene dataset and designed evaluation metrics to facilitate the task.

- We design a novel Transformer decoder architecture for the MSG task. It jointly learns embeddings for places and objects and determines the graph according to the embedding distance. Experiments demonstrate the effectiveness of the model over existing baselines.

<figure id="fig:teaser">
<img src="./figures/teaser.png"" />
<figcaption><strong>Multiview Scene Graph (MSG)</strong>. The task of MSG takes unposed RGB images as input and outputs a place+object graph. The graph contains place-place edges and place-object edges. Connected place nodes represent images taken at the same place. The same object recognized from different views is associated and merged as one node and connected to the corresponding place nodes.</figcaption>
</figure>

# Related work [sec:related]

#### Scene Graph

Scene graphs `\cite{johnson2015image,xu2017scene}`{=latex} are originally proposed to represent the spatial and semantic relationships between objects in an image. The generated scene graph can be used for image captioning `\cite{nguyen2021defense}`{=latex} and image retrieval `\cite{johnson2015image}`{=latex}. Although they provide a structured spatial representation, it remains at the 2D image level. 3D scene graphs `\cite{armeni20193dscenegraph, wu2021scenegraphfusion, hughes2022hydra,  hughes2023foundations-hydra, zhang2024egosg}`{=latex} extend this concept into 3D, representing a scene as a topological graph with objects, rooms, and camera positions as their nodes. These graphs are typically built by abstracting from 3D meshes, point clouds, or directly from RGB-D images. `\cite{wu2023incremental}`{=latex} proposes incrementally building 3D scene graphs from RGB sequences, describing semantic relationships between objects. As a new type of scene graph, MSG is built from unposed images without sequential order, emphasizing the understanding of relationships between objects and places via multiview visual correspondences. MSG complements existing scene graphs, as their object-object relationship edges can be a seamless add-on to extend MSG with more semantic information. Therefore, we believe MSG provides a meaningful contribution to the scene graph community by enhancing its representational depth and flexibility.

#### Scene Mapping

Simultaneous localization and mapping (SLAM) `\cite{mur2015orb,teed2021droid,shan2018lego,chen2023deepmapping2}`{=latex} is a classic way of creating maps of an environment from observations. The metric maps built from SLAM are subsequently utilized as the spatial representation for the robots to perform tasks such as navigation. In contrast to metric maps, topological mapping  `\cite{savinov2018semi}`{=latex} is inspired by landmark-based memory in animals, and follows a more natural and human-like understanding of the environment to better support navigation tasks. The quality of the topological maps is evaluated mostly through navigation tasks `\cite{blochliger2018topomap,chaplot2020toposlam}`{=latex}. Another line of scene mapping work harnesses object or semantic information to build more robust maps `\cite{slam++,wu2023object,garg2024robohop}`{=latex}, with TSGM `\cite{kim2023topological}`{=latex} being the most relevant to our work. Differently, our proposed MSG serves as a general-purpose scene representation and can be directly evaluated using our proposed metrics. The quality of MSG that a model can build explicitly evaluates its capability of understanding spatial correspondences.

#### Visual Place Recognition

Visual Place Recognition (VPR) is often formed as an image retrieval problem. This involves extracting image features and retrieving the closest neighbor from an image database. Traditional approaches rely on handcrafted features `\cite{lowe1999sift, surf}`{=latex}. NetVLAD and its variants`\cite{arandjelovic2016netvlad,magliani2018dense,chen2022self}`{=latex} use deep-learned image features to improve recall performance and robustness. The emergence of self-supervised foundation models, such as DINOv2 `\cite{oquab2023dinov2}`{=latex}, enables universal image representations, offering significant progress `\cite{keetha2023anyloc, Izquierdo_CVPR_2024_SALAD}`{=latex} across many VPR tasks. However, VPR is framed as an image retrieval problem, whose output—the image features—does not directly equal a graph. Although a graph can be built by proximity search in the VPR feature space, the widely used recall metric in VPR does not directly reflect how good the graph is, i.e. how many pairs of connected images in this graph are truly at the same place. Instead, our proposed task and evaluation metric focus only on the graph generated from the model. The metric straightforwardly reflects the quality of the scene representation.

#### Object Association

Traditionally, object association is approached by matching keypoint features across image pairs `\cite{lowe1999sift, sarlin2020superglue}`{=latex}. Recently, CSR `\cite{gadre2022csr}`{=latex} learns feature encodings of object detections and measures the cosine similarity between the learned features to determine object matching. ROM `\cite{elich2023rom}`{=latex} on the other hand follows SuperGlue `\cite{sarlin2020superglue}`{=latex} and uses attentional GNN and Sinkhorn distance `\cite{sinkhorn1967skinhorn}`{=latex} for relational object matching. Our method draws inspiration from this previous work but adopts a Transformer decoder architecture and learns object instance embeddings jointly in a unified model with place recognition. Literature in multi-object tracking `\cite{wang2021unitrack, wu2023referring, qin2023motiontrack, zhang2023motrv2}`{=latex} and video object segmentation `\cite{cheng2023deva, cheng2022xmem, hong2023lvos, wang2023look}`{=latex} also handles object association. They mostly leverage temporal dependencies or memories such as by propagating detection bounding boxes or segments through time. Therefore, these models may lack a sense of space and suffer when objects reappear from a very different viewpoint or after a longer period. Interestingly, a recent study Probe3d `\cite{elbanani2024probe3d}`{=latex} reveals that even though the pretrained vision foundation models have undergone tremendous progress in the recent years `\cite{oquab2023dinov2, kirillov2023sam, he2022masked, dosovitskiy2020vit, caron2021emerging}`{=latex}, they still struggle with associating spatial correspondences of objects from large viewpoint change. Our method learns scene representation with spatial correspondence, where multiple views of the same places or the same objects are close in the embedding space.

# Multiview scene graph [sec:task]

## Problem definition [sec:definition]

#### Multiview Scene Graph [multiview-scene-graph]

Given a set of unposed images of a scene \\(X = \{ x_i\}_{i=0,\ldots, T}\\), we represent a Multiview Scene Graph as a **place+object graph**: \\[G = \{P, O, E^{PP}, E^{PO} \},
    \label{eq:graph}\\] where \\(P\\) and \\(O\\) respectively refer to the sets of *place* and *object* nodes. The set of object nodes \\(O\\) contains all the objects detected from \\(X\\). The same object detected from different images across different viewpoints should always be considered as one object node. For the definition of places, we follow the definition in the VPR literature and set \\(P = X\\). This means each image corresponds to a node for a place, and if two images are taken within only a small translation and rotation distance, they are considered as taken in the same place and are connected with an edge in \\(E^{PP}\\). Consequently, the \\(E^{PP}\\) is the set of *place-place edges* which refers to the edges that connect the images regarded as in the same place, and the \\(E^{PO}\\) represents the set of *place-object edges*, referring to the edges that connect the places and the objects that appear in those places. Therefore, an object can be seen in multiple images and thus connected to more than just one place node. These images can be close by or from a distance. Naturally, a place node can connect to more than one object node, since an image can contain multiple objects’ appearances.

#### MSG generation task

As illustrated in Figure <a href="#fig:teaser" data-reference-type="ref" data-reference="fig:teaser">1</a>, the MSG generation task requires building an estimated place+object graph \\(\hat{G}\\) from the unposed RGB image set. The graph is further represented as a place+object adjacency matrix \\(\hat{A}\\) of size \\((|P|+|\hat{O}|) \times (|P|+|\hat{O}|)\\), while the groundtruth \\(G\\) is represented by \\(A\\) of size \\((|P|+|O|) \times (|P|+|O|)\\). Note that the object set \\(\hat{O}\\) may differ from \\(O\\). The quality of \\(\hat{G}\\) is evaluated by measuring \\(\hat{A}\\) against the groundtruth \\(A\\). According to our definition, the adjacency matrix can be further decomposed into the following block matrix: \\[A = 
    \left[
    \begin{array}{cc}
    A^{PP} & A^{PO} \\
    A^{OP} & A^{OO} \\
    \end{array}
    \right],\\] where \\(A^{PP} = A_{1\leq i \leq |P|, 1\leq j \leq |P|}\\) and \\(A^{PO} = A_{1\leq i \leq |P|, |P|+1\leq j \leq |P|+|O|}\\) . The same decomposition applies to \\(\hat{A}\\). Since the MSG contains only the place-place edges and the place-object edges, \\(A^{OO}\\) is left blank. Meanwhile, \\(A^{PO}\\) is symmetric to \\(A^{OP}\\). So our evaluation will focus on \\(A^{PP}\\) and \\(A^{PO}\\).

## Evaluation metric [sec:metric]

Given that the two adjacency matrices \\(A\\) and \\(\hat{A}\\) are binary, we evaluate their intersection over union (IoU) to measure how much the two graphs align. As aforementioned, an adjacency matrix \\(A\\) essentially consists of two parts: the place-place part \\(A^{PP}\\) and the place-object part \\(A^{PO}\\). So we evaluate them respectively as PP IoU and PO IoU and combine them to get the whole graph IoU. We provide a precise mathematical definition of the IoU calculation for any two binary adjacency matrices in Appendix <a href="#supp:iou" data-reference-type="ref" data-reference="supp:iou">9.1</a> and we denote this function by \\(\text{IoU}(\cdot, \cdot)\\) in the following for simplicity.

#### PP IoU

For the PP IoU, the calculation is relatively straightforward since the number of images is deterministic and the one-to-one correspondence between the groundtruth \\(A^{PP}\\) and the prediction \\(\hat{A}^{PP}\\) is fixed. As a result, the PP IoU is simply: \\[\text{PP IoU} = \text{IoU}(A^{PP}, \hat{A}^{PP}).\\] Additionally, we also report the Recall@1 score alongside PP IoU since it is the standard evaluation metric for visual place recognition.

#### PO IoU

However, it is less straightforward for the PO IoU. The number of objects in the predicted set \\(\hat{O}\\) may differ from \\(O\\), and their correspondence cannot be determined directly from the adjacency matrix. For a fair evaluation, we need to align \\(\hat{O}\\) with \\(O\\) as much as possible. In other words, before computing IoU, we need to find the best matching object for each groundtruth object. This truth-to-result matching is also an important issue in multi-object tracking `\cite{ristani2016mtmc}`{=latex}. To do so, we also record the object bounding boxes in each image and calculate the generalized IoU score (GIoU) of the bounding boxes following `\cite{rezatofighi2019giou}`{=latex}. Then we compute a one-to-one matching between \\(O\\) and \\(\hat{O}\\) based on the accumulated GIoU score across all the images. Details of the score computation are included in the Appendix <a href="#supp:score" data-reference-type="ref" data-reference="supp:score">9.2</a>. According to the matching, we can reorder \\(\hat{O}\\) to best align with the objects in \\(O\\). This can be mathematically expressed as a permutation matrix \\(S\in \mathbb{R}^{|\hat{O}|\times|\hat{O}|}\\) to permute the columns of \\(\hat{A}\\). Formally, the PO IoU is expressed as the following: \\[\text{PO IoU} = \text{IoU}(A^{PO}, \hat{A}^{P\hat{O}}S).\\]

# Our Baseline: Attention Association MSG Generation [sec:model]

<figure id="fig:model">
<img src="./figures/main2.png"" />
<figcaption><strong>The AoMSG model.</strong> Places and objects queries are obtained by cropping the image feature map using corresponding bounding boxes. The queries are then fed into the Transformer decoder to obtain the final places and objects embeddings. Bounding boxes are in different colors for clarity. The parameters in the Transformer decoder and the linear projector heads are trained with supervised contrastive learning. Image encoder and object detector are pretrained and frozen.</figcaption>
</figure>

When developing a new model for the MSG generation task, we adhere to two core principles: Firstly, the model should capitalize on the strengths of pretrained vision models. These pretrained models offer a robust initialization for subsequent vision tasks, as their output features encapsulate rich semantic information, forming a solid foundation for tasks like ours. Secondly, both place recognition and object association fundamentally address the problem of visual correspondence and can mutually reinforce each other through contextual information. Thus, our model is designed to integrate both tasks within a unified framework. With these guiding principles, we propose the Attention Association MSG (AoMSG) model, depicted in Figure <a href="#fig:model" data-reference-type="ref" data-reference="fig:model">2</a>.

#### Place and object encodings

Given a batch of unposed RGB images as input, the AoMSG model first employs pretrained vision encoders and detectors to derive image tokens and object detection bounding boxes from each image. We utilize the Vision Transformer-based pretrained model DINOv2 `\cite{oquab2023dinov2}`{=latex} as our encoder, though our design is adaptable to any Transformer-based or CNN-based encoder that produces a sequence of tokens or a feature map. In the case of the DINOv2 encoder, we reshape the output token sequences into a feature map, which is then aligned to the object bounding boxes, aggregating an encoding feature for each detected object. To integrate place recognition and object association within a unified framework, we obtain the place encoding feature by treating it as a large object with a bounding box that encompasses the entire image, aggregating features as if a detected object. The obtained place feature is then positioned alongside the object features, serving as queries for the Transformer decoder, as detailed in the subsequent sections.

#### AoMSG decoder

We follow a DETR-like structure `\cite{carion2020detr}`{=latex} to design our AoMSG decoder. Specifically, the derived place feature and object features are stacked as a sequence of queries for the Transformer decoder, while the preceding image tokens are used as keys and values. As shown in Figure <a href="#fig:model" data-reference-type="ref" data-reference="fig:model">2</a>, we enhance the queries by incorporating positional encodings by normalizing and embedding the bounding box coordinates. For instance, for the place feature, the equivalent bounding box is the entire image as aforementioned, resulting in the normalized coordinates of \[0, 0, 1, 1\]. These coordinates are projected to match the dimensionality of the encoding and added elementwise to the place query. The outputs of the AoMSG Transformer decoder are the place and object embeddings that have aggregated context information from the image tokens. Then two linear projector heads are applied to each object and place embeddings respectively to obtain the final object and place embeddings, projecting them into the representation space for the task.

#### Losses and predictions

For training, we compute supervised contrastive learning `\cite{radford2021clip}`{=latex} respectively on the place and object embeddings from the same training batch in a multitasking fashion. For the object loss, we simply use binary cross-entropy with higher positive weights. For the place loss, the mean square error is minimized for their cosine distances, which gives better empirical results. During inference, we simply compute the cosine similarity among the place embeddings and apply a threshold to obtain the place-place predictions in \\(\hat{A}\\). For the objects, we track their appearances and maintain a memory bank of the existing objects for each scene, updating their embeddings or registering new objects based on cosine similarity and thresholding. The results are consequently converted to the place-object part in \\(\hat{A}\\). Notably, there could be many possible choices to compute the contrastive losses and determine the predictions, we keep our choices simple as we empirically find the standard losses and the simple cosine thresholding can already produce decent results while keeping the embedding spaces straightforwardly meaningful. We discuss the results in detail in Section <a href="#sec:exp" data-reference-type="ref" data-reference="sec:exp">5</a>.

# Experiment [sec:exp]

<div id="tab:main" markdown="1">

<table>
<caption><strong>Main results.</strong> Our method uses DINOv2<span class="citation" data-cites="oquab2023dinov2"></span> as the backbone. GDino stands for the detector GroundingDINO<span class="citation" data-cites="liu2023grounding"></span>. AoMSG-2 and AoMSG-4 represent AoMSG models with 2 and 4 layers of Transformer decoder respectively. The best results are <u>underlined</u>. * indicates a trivial result since its input is given in temporal order, and consecutive frames are trivially recalled.</caption>
<thead>
<tr>
<th colspan="2" style="text-align: center;"><strong>Method</strong></th>
<th colspan="3" style="text-align: center;"><strong>Metric</strong></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><span>1-2</span> (r)<span>3-6</span></td>
<td rowspan="2" style="text-align: center;"><strong>Object</strong></td>
<td rowspan="2" style="text-align: center;"><strong>Recall@1</strong></td>
<td rowspan="2" style="text-align: center;"><strong>PP IoU</strong></td>
<td colspan="2" style="text-align: center;"><strong>PO IoU</strong></td>
</tr>
<tr>
<td style="text-align: center;"><span>5-6</span></td>
<td style="text-align: center;"><strong>w/ GT detection</strong></td>
<td style="text-align: center;"><strong>w/ GDino <span class="citation" data-cites="liu2023grounding"></span></strong></td>
</tr>
<tr>
<td style="text-align: center;">AnyLoc <span class="citation" data-cites="keetha2023anyloc"></span></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">97.1</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">NetVlad <span class="citation" data-cites="arandjelovic2016netvlad"></span></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">96.6</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Mickey <span class="citation" data-cites="barroso2024mickey"></span></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">100*</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">UniTrack <span class="citation" data-cites="wang2021unitrack"></span></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">13.0</td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">DEVA <span class="citation" data-cites="cheng2023deva"></span></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">16.6</td>
</tr>
<tr>
<td colspan="2" style="text-align: center;">SepMSG - Direct</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">24.5</td>
</tr>
<tr>
<td colspan="2" style="text-align: center;">SepMSG - Linear</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">59.3</td>
<td style="text-align: center;">24.6</td>
</tr>
<tr>
<td colspan="2" style="text-align: center;">SepMSG - MLP</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">23.4</td>
</tr>
<tr>
<td colspan="2" style="text-align: center;">AoMSG-2</td>
<td style="text-align: center;">97.2</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;"><span style="background-color: yellow"><u>28.1</u></span></td>
</tr>
<tr>
<td colspan="2" style="text-align: center;">AoMSG-4</td>
<td style="text-align: center;"><span style="background-color: yellow"><u>98.3</u></span></td>
<td style="text-align: center;"><span style="background-color: yellow"><u>42.2</u></span></td>
<td style="text-align: center;"><span style="background-color: yellow"><u>74.2</u></span></td>
<td style="text-align: center;"><span style="background-color: yellow"><u>28.1</u></span></td>
</tr>
</tbody>
</table>

</div>

The remaining settings, ablations, and qualitative illustrations follow the same configuration and notation described previously.
## Data [sec:data]

The MSG models can be trained with any dataset that provides camera poses and object instance labels. We utilized the publicly available 3D indoor scene dataset ARKitScenes `\cite{dehghan2021arkitscenes}`{=latex} to construct our dataset. ARKitScenes contains point clouds and 3D object bounding boxes of the scenes, as well as the calibrated camera poses obtained from an iPad Pro. We transform the point clouds in the 3D bounding boxes with respect to the camera poses to obtain the 2D bounding boxes in each frame. The resolution of each frame is \\(192\times 256\\). 4492 scenes are used for training and 200 scenes are used for testing. Note that none of the two scenes share the same objects. We leverage the camera poses to obtain the place annotations. Translation threshold and rotation threshold are set to 1 meter and 1 radian respectively, images taken within both thresholds are considered as capturing the same place.

## Baselines [sec:baseline]

#### VPR

We adopt protocols in the previous VPR benchmark literature as outlined in `\cite{berton2022deep, keetha2023anyloc}`{=latex}. In our off-the-shelf baselines, we evaluate VPR using DINOv2 `\cite{oquab2023dinov2}`{=latex} either as the global descriptor or followed by a VLAD dictionary generated from a large-scale indoor dataset following `\cite{keetha2023anyloc}`{=latex}, or as a feature extraction backbone for NetVLAD. For the trained baseline, we conduct our experiments mainly with ResNet-50 `\cite{he2016deep}`{=latex} + NetVLAD used in `\cite{berton2022deep}`{=latex}. Additionally, we also test a recent pose-estimation baseline `\cite{barroso2024mickey}`{=latex} and use the poses to estimate the places according to the same thresholds as in the dataset.

#### Object association

We adopt two popular baselines for object association, Unitrack `\cite{wang2021unitrack}`{=latex} from multi-object tracking, and DEVA `\cite{cheng2023deva}`{=latex} from video object segmentation. The image sets are processed in temporal order just like tracking. Unitrack can take any detection backbones and associate object bounding boxes by comparing their features with an online updating memory bank. For a fair comparison, we extend its memory buffer length to cover the whole set of images for every scene. DEVA leverages the Segment Anything model `\cite{kirillov2023sam}`{=latex} to segment and track any object throughout a video without additional training. Their tracking results can be easily converted for evaluating object association based on the tracker IDs.

#### SepMSG

We also evaluate the pretrained vision models by first separately encoding images and object detections to features and directly evaluating MSG based on those features. This baseline is referred to as *SepMSG-Direct*, where *Sep* means *separately* handling places and objects. Then, as a common way of evaluating pretrained models `\cite{he2020momentum, he2022masked}`{=latex}, we conduct probing `\cite{alain2016probing}`{=latex} by further training a linear or MLP classifier on those frozen features. These baselines are referred to as *SepMSG-Linear* and *SepMSG-MLP*. The SepMSG baselines serve as ablation to validate our model against simply using features learned from the pretrained backbones.
#### VPR

We adopt protocols in the previous VPR benchmark literature as outlined in `\cite{berton2022deep, keetha2023anyloc}`{=latex}. In our off-the-shelf baselines, we evaluated VPR using DINOv2 `\cite{oquab2023dinov2}`{=latex} either as the global descriptor or followed by a VLAD dictionary generated from a large-scale indoor dataset following `\cite{keetha2023anyloc}`{=latex}, or as a feature extraction backbone `\cite{Izquierdo_CVPR_2024_SALAD}`{=latex}. For the trained baseline, we conduct our experiments mainly with ResNet-50 `\cite{he2016deep}`{=latex} + NetVLAD used in `\cite{berton2022deep}`{=latex}. Additionally, we also test a recent pose estimation baseline `\cite{barroso2024mickey}`{=latex} and use the poses to estimate the places according to the same thresholds as in the dataset.

#### Object association

We adopt two popular baselines for object association, Unitrack `\cite{wang2021unitrack}`{=latex} from multi-object tracking, and DEVA `\cite{cheng2023deva}`{=latex} from video object segmentation. The image sets are processed in temporal order just like tracking. Unitrack can take any detection backbones and associate object bounding boxes by comparing their features with an online updating memory bank. For a fair comparison, we extend its memory buffer length to cover the whole set of images for every scene. DEVA leverages the Segment Anything model `\cite{kirillov2023sam}`{=latex} to segment and track any object throughout a video without additional training. Their tracking results can be easily converted for evaluating object association based on the tracker IDs.

#### SepMSG

We also evaluate the pretrained vision models by first separately encoding images and object detections to features and directly evaluating MSG based on those features. This baseline is referred to as *SepMSG-Direct*, where *Sep* means *separately* handling places and objects. Then as a common way of evaluating pretrained models `\cite{he2020momentum, he2022masked}`{=latex}, we conduct probing `\cite{alain2016probing}`{=latex} by further training a linear or MLP classifier on those frozen features. These baselines are referred to as *SepMSG-Linear* and *SepMSG-MLP*. The SepMSG baselines serve as ablation to validate our model against simply using features learned from the pretrained backbones.

## Experimental setups [sec:exp_setup]

For AoMSG, we experimented with different choices of backbones, sizes of the Transformer decoder, and dimensions of the final linear projector heads. Their results are discussed in Section <a href="#sec:results" data-reference-type="ref" data-reference="sec:results">5.4</a>. All the models are trained on a single H100 or GTX 3090 graphics card for 30 epochs or until convergence. We provide detailed hyperparameters in the appendix. During training, we randomly shuffle the scenes and mix data from multiple scenes in a single batch so that the model sees diversified negative samples at every epoch. Additionally, we monitor the total coding rate as in `\cite{tong2023emp}`{=latex} to avoid the embeddings from collapsing.

To keep the evaluation focused on the quality of the graph rather than the quality of object detection, we choose not to train the detector together with the MSG objectives. Instead, we use the groundtruth detection bounding boxes and a popular open-vocabulary object detector GroundingDINO `\cite{liu2023grounding}`{=latex}. Results on both configurations are listed in Table <a href="#tab:main" data-reference-type="ref" data-reference="tab:main">1</a> and discussed in the following.

## Results [sec:results]

#### Main results

Table <a href="#tab:main" data-reference-type="ref" data-reference="tab:main">1</a> shows comparison of our results and baselines. We find that for the place Recall@1 and PP IoU, the baselines have competitive performance. While the results from the SepMSG baselines are comparable, AoMSG outperforms them all and produces the best numbers in both metrics. We also notice that all the models produce high Recall@1, but their PP IoU scores are varied and remain under 50, which suggests that having high recall is not enough to guarantee a good graph. For PO IoU, AoMSG models outperform all the baselines by big margins. Both Unitrack and DEVA perform poorly as they struggle when objects re-appear after large viewpoint changes or long periods of time. We note that all MSG methods produce relatively worse results when using GroundingDINO as the detector rather than the ground-truth detection, indicating the performance gap caused by inaccurate object detection. Nevertheless, their performances are still consistent and AoMSG remains the best across the board. To conclude, AoMSG gives the top performance for every metric reported.

#### Projector dimensions

As listed in Table <a href="#tab:projector" data-reference-type="ref" data-reference="tab:projector">2</a>, we compared the impact of different projector dimensions, a factor that is reportedly important in self-supervised representation learning `\cite{bardes2021vicreg, bordes2023towards, chen2020simple}`{=latex}. We find the empirical results are comparable in our experiments, with 1024-d always slightly ahead.

#### Choices of backbones

Figure <a href="#fig:backbone" data-reference-type="ref" data-reference="fig:backbone">[fig:backbone]</a> shows the performance of different pretrained backbones. We experimented with ConvNeXt `\cite{liu2022convnext}`{=latex}, ViT `\cite{dosovitskiy2020vit}`{=latex}, and DINOv2 `\cite{oquab2023dinov2}`{=latex}. DINOv2 performs the best, consistent with observations made in `\cite{elbanani2024probe3d}`{=latex}. Interestingly, performance saturates with the size of DINOv2, suggesting further gains with still larger models and more data.

#### Qualitative

In Figure <a href="#fig:tsne" data-reference-type="ref" data-reference="fig:tsne">4</a> we visualise the learned object embeddings on multiple scenes by AoMSG, the SepMSG-Linear baseline, and SepMSG-Direct. The visualisation aims to qualitatively assess the learned object embeddings. We can see the pretrained embeddings already provide decent separation; SepMSG-Linear improves it slightly, while AoMSG gives the most significant separation. Figure <a href="#fig:visual" data-reference-type="ref" data-reference="fig:visual">3</a> further visualises results on places and objects.

#### Main results

Table <a href="#tab:main" data-reference-type="ref" data-reference="tab:main">1</a> shows comparison of our results and baselines. We find that for the place Recall@1 and PP IoU, the baselines have competitive performance. While the results from the SepMSG baselines are comparable, AoMSG outperforms them all and produces the best results in both metrics. We also notice that all the models produce high Recall@1, but their PP IoU scores are varied and less than 50. This suggests that having high recall is not enough to guarantee a good graph. For PO IoU, AoMSG models outperform all the baselines by big margins. Both Unitrack and DEVA perform poorly as they struggle when objects reappear after large viewpoint changes or long periods of time. We note that all the MSG methods produce relatively worse results when using GroundingDINO as the detector rather than the ground truth detection. This indicates the performance gap caused by inaccurate object detection. Nevertheless, their performances are still consistent and AoMSG still performs the best. This suggests a better detector will likely give better results for the MSG task. To conclude, AoMSG gives the best performance for all the metrics.

#### Projector dimensions

As listed in Table <a href="#tab:projector" data-reference-type="ref" data-reference="tab:projector">2</a>, we compared the impact of different projector dimensions as it is reportedly important to performance in the literature of self-supervised representation learning `\cite{bardes2021vicreg, bordes2023towards, chen2020simple}`{=latex}. We find the empirical results are comparable in our experiments.

#### Choices of backbones

Figure <a href="#fig:backbone" data-reference-type="ref" data-reference="fig:backbone">[fig:backbone]</a> shows the performance of difference choices of pretrained backbones. We experimented with state-of-the-art CNN-based model ConvNext `\cite{liu2022convnext}`{=latex}, Vision Transformer (ViT) `\cite{dosovitskiy2020vit}`{=latex} and DINOv2 `\cite{oquab2023dinov2}`{=latex}. We find DINOv2 performs the best, consistent with the observations made in `\cite{elbanani2024probe3d}`{=latex}. We use DINOv2 as our default encoder. Interestingly, performance saturates with the size of DINOv2. We suspect it will still increase if we could further scale the size of the data.

#### Qualitative

In Figure <a href="#fig:tsne" data-reference-type="ref" data-reference="fig:tsne">4</a> we visualized the learned object embeddings on 6 scenes by AoMSG, the SepMSG-Linear baseline, and SepMSG-Direct that directly uses the output features from the pretrained DINOv2 encoder for the task. The visualization aims to qualitatively assess the learned object embeddings as to how separated different objects are in the embedding space. We can see the pretrained embeddings already provide some decent separations. SepMSG-Linear only tunes a linear probing classifier on top so the separation is slightly improved. For example, see the first and second scenes to the left. Compared with them, AoMSG gives the most significant separations, with appearances of the same objects pushed closely and different objects pulled far away. Additionally, Figure <a href="#fig:visual" data-reference-type="ref" data-reference="fig:visual">3</a> visualizes results on some places and objects, and we provide more in Appendix <a href="#supp:vis" data-reference-type="ref" data-reference="supp:vis">11</a>.

<figure id="fig:tsne">
<img src="./figures/tsne.png"" />
<figcaption>Object embedding visualization using t-SNE <span class="citation" data-cites="van2008tsne"></span>. SepMSG-Direct, SepMSG-Linear, and AoMSG-2 are shown in each row respectively. Results from the same scene are aligned vertically. Colors indicate different objects. Each point is an appearance of an object. It is best viewed in color.</figcaption>
</figure>

# Discussion [sec:discussion]

## Application

Given the recent advances in novel view synthesis, 3D reconstruction, and metric mappings, one might wonder whether the proposed MSG is still useful. Here we provide some justifications and a showcase application. Echoing literature in 3D scene graphs `\cite{hughes2022hydra, wu2021scenegraphfusion}`{=latex}, we believe the MSG can be a versatile mental model for embodied AI agents and robots. At a global level, it keeps a lightweight topological memory of the scene from purely 2D RGB inputs, which serves as a basis for robot navigation `\cite{kim2023topological, chaplot2020toposlam}`{=latex}. At a finer level, it can seamlessly couple MSG with the 3D reconstruction methods, to estimate depth and poses and build local reconstructions. Therefore, a robot can traverse the environment, localize itself referring to the MSG, and build a local reconstruction when needed for tasks that require metric information such as the manipulation tasks.

<figure id="fig:recon">
<img src="./figures/recon.jpg"" />
<figcaption>Local 3D reconstruction from 2D MSG using off-the-shelf model Dust3r <span class="citation" data-cites="dust3r_cvpr24"></span>. The 3D meshes of two scenes are shown side by side, with 3 subgraphs circled in gray and reconstructed on the top of each scene.</figcaption>
</figure>

As a showcase application, we provide two local 3D reconstruction cases illustrated in Figure <a href="#fig:recon" data-reference-type="ref" data-reference="fig:recon">5</a> using the most recent off-the-shelf 3D reconstruction model Dust3r `\cite{dust3r_cvpr24}`{=latex}. Directly applying Dust3r to a dense image set greatly consumes GPU memory, which may be infeasible for mobile robots. Whereas a random subsample does not guarantee the reconstruction quality. Instead, with MSG, we can provide the Dust3r with locally interconnected subgraphs for fast and reliable local reconstruction. The subgraphs and local reconstructions can be object-centric thanks to the *place+object* nature of MSG. Moreover, the local reconstructions are topologically connected by MSG. This suggests MSG can provide a flexible scene representation balancing 2D and 3D, abstractions and details.

## Limitation [sec:limitation]

The current work still has many limitations. Firstly, we only conducted experiments in one dataset. Although the dataset contains around 5k scenes, which is sufficient to obtain convincing results, it would still be great to see if training on more diversified data collections can produce better models and stronger generalization as observed in `\cite{dust3r_cvpr24}`{=latex}, especially for larger models. We leave this to future work. Secondly, scenes in the current dataset contain only static objects, extending to dynamic objects is a direction worth exploring.

Additionally, given the scope of the work is to propose MSG as a new vision task promoting spatial intelligence, we focus on explicitly evaluating the quality of the graph. Therefore, we did not investigate the object detection quality, nor did we deploy the MSG to downstream tasks such as navigation. Note that detection quality does affect the MSG performance though we find it to be consistent across different detection modes, i.e. the groundtruth and the GroundingDINO. Training detectors together with the MSG model and applying MSG to downstream tasks will be our next step to make the work a more complete system.

# Conclusion [sec:conclusion]

This work proposes building the Multiview Scene Graph (MSG) as a new vision task for evaluating spatial intelligence. The task gives unposed RGB images as input and requires a model to build a place+object graph that connects images taken at the same place and associates the object recognitions from different viewpoints, forming a topological scene representation. To evaluate the MSG generation task, we designed evaluation metrics, curated a dataset, and proposed a new model that jointly learns place and object embeddings and builds the graph based on embedding distances. The model outperforms existing baselines that handle place recognition and object association separately. Lastly, we discussed the possible applications of MSG and the current limitations. We hope this work can stimulate future research on advancing spatial intelligence and scene representations.

#### Acknowledgement.

The authors thank Yiming Li and Shengbang Tong for their valuable discussions and suggestions.

# References [references]

<div class="thebibliography" markdown="1">

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al Gpt-4 technical report *arXiv preprint arXiv:2303.08774*, 2023. **Abstract:** We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4’s performance based on models trained with no more than 1/1,000th the compute of GPT-4. (@achiam2023gpt)

Guillaume Alain and Yoshua Bengio Understanding intermediate layers using linear classifier probes *arXiv preprint arXiv:1610.01644*, 2016. **Abstract:** Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as "probes", trained entirely independently of the model itself. This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. We apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model. (@alain2016probing)

Amar Ali-Bey, Brahim Chaib-Draa, and Philippe Giguere Mixvpr: Feature mixing for visual place recognition In *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision*, pp. 2998–3007, 2023. **Abstract:** Visual Place Recognition (VPR) is a crucial part of mobile robotics and autonomous driving as well as other computer vision tasks. It refers to the process of identifying a place depicted in a query image using only computer vision. At large scale, repetitive structures, weather and illumination changes pose a real challenge, as appearances can drastically change over time. Along with tackling these challenges, an efficient VPR technique must also be practical in real-world scenarios where latency matters. To address this, we introduce MixVPR, a new holistic feature aggregation technique that takes feature maps from pre-trained backbones as a set of global features. Then, it incorporates a global relationship between elements in each feature map in a cascade of feature mixing, eliminating the need for local or pyramidal aggregation as done in NetVLAD or TransVPR. We demonstrate the effectiveness of our technique through extensive experiments on multiple large-scale benchmarks. Our method outperforms all existing techniques by a large margin while having less than half the number of parameters compared to CosPlace and NetVLAD. We achieve a new all-time high recall@1 score of 94.6% on Pitts250k-test, 88.0% on MapillarySLS, and more importantly, 58.4% on Nordland. Finally, our method outperforms two-stage retrieval techniques such as Patch-NetVLAD, TransVPR and SuperGLUE all while being orders of magnitude faster. (@ali2023mixvpr)

Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic Netvlad: Cnn architecture for weakly supervised place recognition In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pp. 5297–5307, 2016. **Abstract:** We tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph. We present the following three principal contributions. First, we develop a convolutional neural network (CNN) architecture that is trainable in an end-to-end manner directly for the place recognition task. The main component of this architecture, NetVLAD, is a new generalized VLAD layer, inspired by the "Vector of Locally Aggregated Descriptors" image representation commonly used in image retrieval. The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation. Second, we develop a training procedure, based on a new weakly supervised ranking loss, to learn parameters of the architecture in an end-to-end manner from images depicting the same places over time downloaded from Google Street View Time Machine. Finally, we show that the proposed architecture significantly outperforms non-learnt image representations and off-the-shelf CNN descriptors on two challenging place recognition benchmarks, and improves over current state-of-the-art compact image representations on standard image retrieval benchmarks. (@arandjelovic2016netvlad)

Iro Armeni, Zhi-Yang He, JunYoung Gwak, Amir R Zamir, Martin Fischer, Jitendra Malik, and Silvio Savarese 3d scene graph: A structure for unified semantics, 3d space, and camera In *Proceedings of the IEEE/CVF international conference on computer vision*, pp. 5664–5673, 2019. **Abstract:** A comprehensive semantic understanding of a scene is important for many applications - but in what space should diverse semantic information (e.g., objects, scene categories, material types, 3D shapes, etc.) be grounded and what should be its structure? Aspiring to have one unified structure that hosts diverse types of semantics, we follow the Scene Graph paradigm in 3D, generating a 3D Scene Graph. Given a 3D mesh and registered panoramic images, we construct a graph that spans the entire building and includes semantics on objects (e.g., class, material, shape and other attributes), rooms (e.g., function, illumination type, etc.) and cameras (e.g., location, etc.), as well as the relationships among these entities. However, this process is prohibitively labor heavy if done manually. To alleviate this we devise a semi-automatic framework that employs existing detection methods and enhances them using two main constraints: I. framing of query images sampled on panoramas to maximize the performance of 2D detectors, and II. multi-view consistency enforcement across 2D detections that originate in different camera locations. (@armeni20193dscenegraph)

Adrien Bardes, Jean Ponce, and Yann LeCun Vicreg: Variance-invariance-covariance regularization for self-supervised learning *arXiv preprint arXiv:2105.04906*, 2021. **Abstract:** Recent self-supervised methods for image representation learning are based on maximizing the agreement between embedding vectors from different views of the same image. A trivial solution is obtained when the encoder outputs constant vectors. This collapse problem is often avoided through implicit biases in the learning architecture, that often lack a clear justification or interpretation. In this paper, we introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually. VICReg combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization, and achieves results on par with the state of the art on several downstream tasks. In addition, we show that incorporating our new variance term into other methods helps stabilize the training and leads to performance improvements. (@bardes2021vicreg)

Axel Barroso-Laguna, Sowmya Munukutla, Victor Prisacariu, and Eric Brachmann Matching 2d images in 3d: Metric relative pose from metric correspondences In *CVPR*, 2024. **Abstract:** Given two images, we can estimate the relative camera pose between them by establishing image-to-image correspondences. Usually, correspondences are 2D-to-2D and the pose we estimate is defined only up to scale. Some applications, aiming at instant augmented reality anywhere, require scale-metric pose estimates, and hence, they rely on external depth estimators to recover the scale. We present MicKey, a keypoint matching pipeline that is able to predict metric correspondences in 3D camera space. By learning to match 3D coordinates across images, we are able to infer the metric relative pose without depth measurements. Depth measurements are also not required for training, nor are scene reconstructions or image overlap information. MicKey is supervised only by pairs of images and their relative poses. MicKey achieves state-of-the-art performance on the Map-Free Relocalisation benchmark while requiring less supervision than competing approaches. (@barroso2024mickey)

Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, and Elad Shulman itscenes - a diverse real-world dataset for 3d indoor scene understanding using mobile RGB-d data In *Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)*, 2021. URL <https://openreview.net/forum?id=tjZjv_qh_CE>. **Abstract:** Scene understanding is an active research area. Commercial depth sensors, such as Kinect, have enabled the release of several RGB-D datasets over the past few years which spawned novel methods in 3D scene understanding. More recently with the launch of the LiDAR sensor in Apple’s iPads and iPhones, high quality RGB-D data is accessible to millions of people on a device they commonly use. This opens a whole new era in scene understanding for the Computer Vision community as well as app developers. The fundamental research in scene understanding together with the advances in machine learning can now impact people’s everyday experiences. However, transforming these scene understanding methods to real-world experiences requires additional innovation and development. In this paper we introduce ARKitScenes. It is not only the first RGB-D dataset that is captured with a now widely available depth sensor, but to our best knowledge, it also is the largest indoor scene understanding data released. In addition to the raw and processed data from the mobile device, ARKitScenes includes high resolution depth maps captured using a stationary laser scanner, as well as manually labeled 3D oriented bounding boxes for a large taxonomy of furniture. We further analyze the usefulness of the data for two downstream tasks: 3D object detection and color-guided depth upsampling. We demonstrate that our dataset can help push the boundaries of existing state-of-the-art methods and it introduces new challenges that better represent real-world scenarios. (@dehghan2021arkitscenes)

Herbert Bay, Tinne Tuytelaars, and Luc Van Gool Surf: Speeded up robust features In Aleš Leonardis, Horst Bischof, and Axel Pinz (eds.), *Computer Vision – ECCV 2006*, pp. 404–417, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg. ISBN 978-3-540-33833-8. (@surf)

Gabriele Berton, Riccardo Mereu, Gabriele Trivigno, Carlo Masone, Gabriela Csurka, Torsten Sattler, and Barbara Caputo Deep visual geo-localization benchmark In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 5396–5407, 2022. **Abstract:** In this paper, we propose a new open-source benchmarkingframeworkfor Visual Geo-localization (VG) that allows to build, train, and test a wide range of commonly used ar-chitectures, with the flexibility to change individual components of a geo-localization pipeline. The purpose of this framework is twofold: i) gaining insights into how differ-ent components and design choices in a VG pipeline im-pact the final results, both in terms of performance (re-call@N metric) and system requirements (such as execution time and memory consumption); ii) establish a system-atic evaluation protocol for comparing different methods. Using the proposed framework, we perform a large suite of experiments which provide criteria for choosing back-bone, aggregation and negative mining depending on the use-case and requirements. We also assess the impact of engineering techniques like pre/post-processing, data aug-mentation and image resizing, showing that better performance can be obtained through somewhat simple procedures: for example, downscaling the images’ resolution to 80% can lead to similar results with a 36% savings in ex-traction time and dataset storage requirement. Code and trained models are available at dataset storage requirement. https://deep-vg-bench.herokuapp.com/. (@berton2022deep)

Fabian Blochliger, Marius Fehr, Marcin Dymczyk, Thomas Schneider, and Rol Siegwart Topomap: Topological mapping and navigation based on visual slam maps In *2018 IEEE International Conference on Robotics and Automation (ICRA)*, pp. 3818–3825. IEEE, 2018. **Abstract:** Visual robot navigation within large-scale, semistructured environments deals with various challenges such as computation intensive path planning algorithms or insufficient knowledge about traversable spaces. Moreover, many state-of-the-art navigation approaches only operate locally instead of gaining a more conceptual understanding of the planning objective. This limits the complexity of tasks a robot can accomplish and makes it harder to deal with uncertainties that are present in the context of real-time robotics applications. In this work, we present Topomap, a framework which simplifies the navigation task by providing a map to the robot which is tailored for path planning use. This novel approach transforms a sparse feature-based map from a visual Simultaneous Localization And Mapping (SLAM) system into a three-dimensional topological map. This is done in two steps. First, we extract occupancy information directly from the noisy sparse point cloud. Then, we create a set of convex free-space clusters, which are the vertices of the topological map. We show that this representation improves the efficiency of global planning, and we provide a complete derivation of our algorithm. Planning experiments on real world datasets demonstrate that we achieve similar performance as RRT\* with significantly lower computation times and storage requirements. Finally, we test our algorithm on a mobile robotic platform to prove its advantages. (@blochliger2018topomap)

Florian Bordes, Randall Balestriero, and Pascal Vincent Towards democratizing joint-embedding self-supervised learning *arXiv preprint arXiv:2303.01986*, 2023. **Abstract:** Joint Embedding Self-Supervised Learning (JE-SSL) has seen rapid developments in recent years, due to its promise to effectively leverage large unlabeled data. The development of JE-SSL methods was driven primarily by the search for ever increasing downstream classification accuracies, using huge computational resources, and typically built upon insights and intuitions inherited from a close parent JE-SSL method. This has led unwittingly to numerous pre-conceived ideas that carried over across methods e.g. that SimCLR requires very large mini batches to yield competitive accuracies; that strong and computationally slow data augmentations are required. In this work, we debunk several such ill-formed a priori ideas in the hope to unleash the full potential of JE-SSL free of unnecessary limitations. In fact, when carefully evaluating performances across different downstream tasks and properly optimizing hyper-parameters of the methods, we most often – if not always – see that these widespread misconceptions do not hold. For example we show that it is possible to train SimCLR to learn useful representations, while using a single image patch as negative example, and simple Gaussian noise as the only data augmentation for the positive pair. Along these lines, in the hope to democratize JE-SSL and to allow researchers to easily make more extensive evaluations of their methods, we introduce an optimized PyTorch library for SSL. (@bordes2023towards)

Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko End-to-end object detection with transformers In *European conference on computer vision*, pp. 213–229. Springer, 2020. **Abstract:** We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, e ectively removing the need for many hand-designed compo- nents like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bi- partite matching, and a transformer encoder-decoder architecture. Given a xed small set of learned object queries, DETR reasons about the re- lations of the objects and the global image context to directly output the nal set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time perfor- mance on par with the well-established and highly-optimized Faster R- CNN baseline on the challenging COCO object detection dataset. More- over, DETR can be easily generalized to produce panoptic segmentation in a uni ed manner. We show that it signi cantly outperforms com- petitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr . 1 Introduction The goal of object detection is to predict a set of bounding boxes and category labels for each object of interest. Modern detectors address this set prediction task in an indirect way, by de ning surrogate regression and classi cation prob- lems on a large set of proposals \[37,5\], anchors \[23\], or window centers \[53,46\]. Their performances are signi cantly in uenced by postprocessing steps to col- lapse near-duplicate predictions, by the design of the anchor sets and by the heuristics that assign target boxes to anchors \[52\]. To simplify these pipelines, we propose a direct set prediction approach to bypass the surrogate tasks. This end-to-end philosophy has led to signi cant advances in complex structured pre- diction tasks such as machine translation or speech recognition, but not yet in object detection: previous attempts \[43,16,4,39\] either add other forms of prior knowledge, or have not proven to be competitive with strong baselines on chal- lenging benchmarks. This paper aims to bridge this gap. ?Equal contributionarXiv:2005.12872v3 \[cs.CV\] 28 May 20202 Carion et al. transformer encoder-decoderCNNset of box predictionsbipartite matching lossno object (ø)no object (ø)set of im (@carion2020detr)

Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin Emerging properties in self-supervised vision transformers In *Proceedings of the IEEE/CVF international conference on computer vision*, pp. 9650–9660, 2021. **Abstract:** In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) \[16\] that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder \[26\], multi-crop training \[9\], and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base. (@caron2021emerging)

Devendra Singh Chaplot, Ruslan Salakhutdinov, Abhinav Gupta, and Saurabh Gupta Neural topological slam for visual navigation In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 12875–12884, 2020. **Abstract:** This paper studies the problem of image-goal navigation which involves navigating to the location indicated by a goal image in a novel previously unseen environment. To tackle this problem, we design topological representations for space that effectively leverage semantics and afford approximate geometric reasoning. At the heart of our representations are nodes with associated semantic features, that are interconnected using coarse geometric information. We describe supervised learning-based algorithms that can build, maintain and use such representations under noisy actuation. Experimental study in visually and physically realistic simulation suggests that our method builds effective representations that capture structural regularities and efficiently solve long-horizon navigation problems. We observe a relative improvement of more than 50% over existing methods that study this task. (@chaplot2020toposlam)

Chao Chen, Xinhao Liu, Xuchu Xu, Yiming Li, Li Ding, Ruoyu Wang, and Chen Feng Self-supervised visual place recognition by mining temporal and feature neighborhoods *arXiv preprint arXiv:2208.09315*, 2022. **Abstract:** Visual place recognition (VPR) using deep networks has achieved state-of-the-art performance. However, most of them require a training set with ground truth sensor poses to obtain positive and negative samples of each observation’s spatial neighborhood for supervised learning. When such information is unavailable, temporal neighborhoods from a sequentially collected data stream could be exploited for self-supervised training, although we find its performance suboptimal. Inspired by noisy label learning, we propose a novel self-supervised framework named TF-VPR that uses temporal neighborhoods and learnable feature neighborhoods to discover unknown spatial neighborhoods. Our method follows an iterative training paradigm which alternates between: (1) representation learning with data augmentation, (2) positive set expansion to include the current feature space neighbors, and (3) positive set contraction via geometric verification. We conduct auto-labeling and generalization tests on both simulated and real datasets, with either RGB images or point clouds as inputs. The results show that our method outperforms self-supervised baselines in recall rate, robustness, and heading diversity, a novel metric we propose for VPR. Our code and datasets can be found at https://ai4ce.github.io/TF-VPR/ (@chen2022self)

Chao Chen, Xinhao Liu, Yiming Li, Li Ding, and Chen Feng Deepmapping2: Self-supervised large-scale lidar map optimization In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 9306–9316, 2023. **Abstract:** LiDAR mapping is important yet challenging in selfdriving and mobile robotics. To tackle such a global point cloud registration problem, DeepMapping \[1\] converts the complex map estimation into a self-supervised training of simple deep networks. Despite its broad convergence range on small datasets, DeepMapping still cannot produce satisfactory results on large-scale datasets with thousands of frames. This is due to the lack of loop closures and exact cross-frame point correspondences, and the slow convergence of its global localization network. We propose DeepMapping2 by adding two novel techniques to address these issues: (1) organization of training batch based on map topology from loop closing, and (2) self-supervised local-to-global point consistency loss leveraging pairwise registration. Our experiments and ablation studies on public datasets such as KITTI, NCLT, and Nebula demonstrate the effectiveness of our method. (@chen2023deepmapping2)

Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton A simple framework for contrastive learning of visual representations In *International conference on machine learning*, pp. 1597–1607. PMLR, 2020. **Abstract:** This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels. (@chen2020simple)

Ho Kei Cheng and Alexander G. Schwing : Long-term video object segmentation with an atkinson-shiffrin memory model In *ECCV*, 2022. **Abstract:** We present XMem, a video object segmentation architec- ture for long videos with uni ed feature memory stores inspired by the Atkinson-Shi rin memory model. Prior work on video object segmenta- tion typically only uses one type of feature memory. For videos longer than a minute, a single feature memory model tightly links memory consumption and accuracy. In contrast, following the Atkinson-Shi rin model, we develop an architecture that incorporates multiple indepen- dent yet deeply-connected feature memory stores: a rapidly updated sen- sory memory , a high-resolution working memory , and a compact thus sustained long-term memory . Crucially, we develop a memory potentia- tion algorithm that routinely consolidates actively used working memory elements into the long-term memory, which avoids memory explosion and minimizes performance decay for long-term prediction. Combined with a new memory reading mechanism, XMem greatly exceeds state-of-the-art performance on long-video datasets while being on par with state-of-the- art methods (that do not work on long videos) on short-video datasets.1 1 Introduction Video object segmentation (VOS) highlights speci ed target objects in a given video. Here, we focus on the semi-supervised setting where a rst-frame annota- tion is provided by the user, and the method segments objects in all other frames as accurately as possible while preferably running in real-time, online, and while having a small memory footprint even when processing long videos. As information has to be propagated from the given annotation to other video frames, most VOS methods employ a feature memory to store relevant deep-net representations of an object. Online learning methods \[3,49,42\] use the weights of a network as their feature memory. This requires training at test-time, which slows down prediction. Recurrent methods propagate information often from the most recent frames, either via a mask \[39\] or via a hidden representa- tion \[20,47\]. These methods are prone to drifting and struggle with occlusions. 1Code is available at hkchengrex.github.io/XMemarXiv:2207.07115v2 \[cs.CV\] 18 Jul 20222 H.K. Cheng and A.G. Schwing 0 50 100 150 20076788082848688 XMem (Ours) AFU-BRRJOINT CFBICFBI+ STMMiVOSRMNetHMMNSTCN 900 950AOT GPU memory increase per second (MB/s)DAVIS 2017J&F 50556065707376788082848688 RMNet JOINT CFBICFBI+ 73 75 80 85 90XMem (Ours) AFU-BRRSTMMiVOSHMMN AOTSTCN Long-time Video (3 ) datasetJ&FDAVIS 2017J&F Fig. 1. Do state-of-the-art VOS (@cheng2022xmem)

Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexander Schwing, and Joon-Young Lee Tracking anything with decoupled video segmentation In *ICCV*, 2023. **Abstract:** Training data for video segmentation are expensive to annotate. This impedes extensions of end-to-end algorithms to new video segmentation tasks, especially in large-vocabulary settings. To ’track anything’ without training on video data for every individual task, we develop a decoupled video segmentation approach (DEVA), composed of task-specific image-level segmentation and class/task-agnostic bi-directional temporal propagation. Due to this design, we only need an image-level model for the target task (which is cheaper to train) and a universal temporal propagation model which is trained once and generalizes across tasks. To effectively combine these two modules, we use bi-directional propagation for (semi-)online fusion of segmentation hypotheses from different frames to generate a coherent segmentation. We show that this decoupled formulation compares favorably to end-to-end approaches in several data-scarce tasks including large-vocabulary video panoptic segmentation, open-world video segmentation, referring video segmentation, and unsupervised video object segmentation. Code is available at: hkchengrex.github.io/Tracking-Anything-with-DEVA. (@cheng2023deva)

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al An image is worth 16x16 words: Transformers for image recognition at scale *arXiv preprint arXiv:2010.11929*, 2020. **Abstract:** While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. (@dosovitskiy2020vit)

Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, and Varun Jampani In *CVPR*, 2024. **Abstract:** Recent advances in large-scale pretraining have yielded visual foundation models with strong capabilities. Not only can recent models generalize to arbitrary images for their training task, their intermediate representations are useful for other visual tasks such as detection and segmentation. Given that such models can classify, delineate, and localize objects in 2D, we ask whether they also represent their 3D structure? In this work, we analyze the 3D awareness of visual foundation models. We posit that 3D awareness implies that representations (1) encode the 3D structure of the scene and (2) consistently represent the surface across views. We conduct a series of experiments using task-specific probes and zero-shot inference procedures on frozen features. Our experiments reveal several limitations of the current models. Our code and analysis can be found at https://github.com/mbanani/probe3d. (@elbanani2024probe3d)

Cathrin Elich, Iro Armeni, Martin R Oswald, Marc Pollefeys, and Joerg Stueckler Learning-based relational object matching across views In *2023 IEEE International Conference on Robotics and Automation (ICRA)*, pp. 5999–6005. IEEE, 2023. **Abstract:** Intelligent robots require object-level scene understanding to reason about possible tasks and interactions with the environment. Moreover, many perception tasks such as scene reconstruction, image retrieval, or place recognition can benefit from reasoning on the level of objects. While keypoint-based matching can yield strong results for finding correspondences for images with small to medium view point changes, for large view point changes, matching semantically on the object-level becomes advantageous. In this paper, we propose a learning-based approach which combines local keypoints with novel object-level features for matching object detections between RGB images. We train our object-level matching features based on appearance and inter-frame and cross-frame spatial relations between objects in an associative graph neural network. We demonstrate our approach in a large variety of views on realistically rendered synthetic images. Our approach compares favorably to previous state-of-the-art object-level matching approaches and achieves improved performance over a pure keypoint-based approach for large view-point changes. (@elich2023rom)

Samir Yitzhak Gadre, Kiana Ehsani, Shuran Song, and Roozbeh Mottaghi Continuous scene representations for embodied ai In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 14849–14859, 2022. **Abstract:** We propose Continuous Scene Representations (CSR), a scene representation constructed by an embodied agent navigating within a space, where objects and their relationships are modeled by continuous valued embeddings. Our method captures feature relationships between objects, composes them into a graph structure on-the-fly, and situates an embodied agent within the representation. Our key insight is to embed pair-wise relationships between objects in a latent space. This allows for a richer representation compared to discrete relations (e.g., \[SUPPORT\], \[NEXT-TO\]) commonly used for building scene representations. CSR can track objects as the agent moves in a scene, update the representation accordingly, and detect changes in room configurations. Using CSR, we outperform state-of-the-art approaches for the challenging downstream task of visual room rearrangement, without any task specific training. Moreover, we show the learned embeddings capture salient spatial details of the scene and show applicability to real world data. A summery video and code is available at prior.allenai.org/projects/csr. (@gadre2022csr)

Sourav Garg, Krishan Rana, Mehdi Hosseinzadeh, Lachlan Mares, Niko Sünderhauf, Feras Dayoub, and Ian Reid Robohop: Segment-based topological map representation for open-world visual navigation *arXiv preprint arXiv:2405.05792*, 2024. **Abstract:** Mapping is crucial for spatial reasoning, planning and robot navigation. Existing approaches range from metric, which require precise geometry-based optimization, to purely topological, where image-as-node based graphs lack explicit object-level reasoning and interconnectivity. In this paper, we propose a novel topological representation of an environment based on "image segments", which are semantically meaningful and open-vocabulary queryable, conferring several advantages over previous works based on pixel-level features. Unlike 3D scene graphs, we create a purely topological graph with segments as nodes, where edges are formed by a) associating segment-level descriptors between pairs of consecutive images and b) connecting neighboring segments within an image using their pixel centroids. This unveils a "continuous sense of a place", defined by inter-image persistence of segments along with their intra-image neighbours. It further enables us to represent and update segment-level descriptors through neighborhood aggregation using graph convolution layers, which improves robot localization based on segment-level retrieval. Using real-world data, we show how our proposed map representation can be used to i) generate navigation plans in the form of "hops over segments" and ii) search for target objects using natural language queries describing spatial relations of objects. Furthermore, we quantitatively analyze data association at the segment level, which underpins inter-image connectivity during mapping and segment-level localization when revisiting the same place. Finally, we show preliminary trials on segment-level ‘hopping’ based zero-shot real-world navigation. Project page with supplementary details: oravus.github.io/RoboHop/ (@garg2024robohop)

Katalin M Gothard, William E Skaggs, Kevin M Moore, and Bruce L McNaughton Binding of hippocampal ca1 neural activity to multiple reference frames in a landmark-based navigation task *The Journal of neuroscience*, 16 (2): 823, 1996. **Abstract:** The behavioral correlates of rat hippocampal CA1 cells were examined in a spatial navigation task in which two cylindrical landmarks predicted the location of food. The landmarks were maintained at a constant distance from each other but were moved from trial to trial within a large arena surrounded by static background cues. On each trial, the rats were released from a box to which they returned for additional food after locating the goal. The box also was located variably from trial to trial and was moved to a new location while the animals were searching for the goal site. The discharge characteristics of multiple, simultaneously recorded cells were examined with respect to the landmarks, the static background cues, and the box in which each trial started and ended. Three clear categories of cells were observed: (1) cells with location-specific firing (place cells); (2) goal/landmark-related cells that fired in the vicinity of the goal or landmarks, regardless of their location in the arena; and (3) box-related cells that fired either when the rat was in the box or as it was leaving or entering the box, regardless of its location in the arena. Disjunctive cells with separate firing fields in more than one reference frame also were observed. These results suggest that in this task a subpopulation of hippocampal cells encodes location in the fixed spatial frame, whereas other subpopulations encode location with respect to different reference frames associated with the task-relevant, mobile objects. (@gothard1996binding)

Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, and Jitendra Malik Cognitive mapping and planning for visual navigation In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pp. 2616–2625, 2017. **Abstract:** We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person views and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the planner, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. Our experiments demonstrate that CMP outperforms both reactive strategies and standard memory-based architectures and performs well in novel environments. Furthermore, we show that CMP can also achieve semantically specified goals, such as "go to a chair". (@gupta2017cognitive)

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun Deep residual learning for image recognition In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pp. 770–778, 2016. **Abstract:** Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets \[40\] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. (@he2016deep)

Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick Momentum contrast for unsupervised visual representation learning In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 9729–9738, 2020. **Abstract:** We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks. (@he2020momentum)

Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick Masked autoencoders are scalable vision learners In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 16000–16009, 2022. **Abstract:** This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3× or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior. (@he2022masked)

Lingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang, Pinxue Guo, Zhaoyu Chen, and Wenqiang Zhang Lvos: A benchmark for long-term video object segmentation In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pp. 13480–13492, 2023. **Abstract:** Existing video object segmentation (VOS) benchmarks focus on short-term videos which just last about 3-5 seconds and where objects are visible most of the time. These videos are poorly representative of practical applications, and the absence of long-term datasets restricts further investigation of VOS on the application in realistic scenarios. So, in this paper, we present a new benchmark dataset named LVOS, which consists of 220 videos with a total duration of 421 minutes. To the best of our knowledge, LVOS is the first densely annotated long-term VOS dataset. The videos in our LVOS last 1.59 minutes on average, which is 20 times longer than videos in existing VOS datasets. Each video includes various attributes, especially challenges deriving from the wild, such as long-term reappearing and cross-temporal similar objeccts. Based on LVOS, we assess existing video object segmentation algorithms and propose a Diverse Dynamic Memory network (DDMemory) that consists of three complementary memory banks to exploit temporal information adequately. The experimental results demonstrate the strength and weaknesses of prior methods, pointing promising directions for further study. Data and code are available at https://lingyihongfd.github.io/lvos.github.io/. (@hong2023lvos)

N. Hughes, Y. Chang, and L. Carlone Hydra: A real-time spatial perception system for 3D scene graph construction and optimization . **Abstract:** 3D scene graphs have recently emerged as a powerful high-level representation of 3D environments.A 3D scene graph describes the environment as a layered graph where nodes represent spatial concepts at multiple levels of abstraction (from low-level geometry to high-level semantics including objects, places, rooms, buildings, etc.) and edges represent relations between concepts.While 3D scene graphs can serve as an advanced "mental model" for robots, how to build such a rich representation in real-time is still uncharted territory.This paper describes a real-time Spatial Perception System, a suite of algorithms to build a 3D scene graph from sensor data in real-time.Our first contribution is to develop realtime algorithms to incrementally construct the layers of a scene graph as the robot explores the environment; these algorithms build a local Euclidean Signed Distance Function (ESDF) around the current robot location, extract a topological map of places from the ESDF, and then segment the places into rooms using an approach inspired by community-detection techniques.Our second contribution is to investigate loop closure detection and optimization in 3D scene graphs.We show that 3D scene graphs allow defining hierarchical descriptors for loop closure detection; our descriptors capture statistics across layers in the scene graph, ranging from low-level visual appearance to summary statistics about objects and places.We then propose the first algorithm to optimize a 3D scene graph in response to loop closures; our approach relies on embedded deformation graphs to simultaneously correct all layers of the scene graph.We implement the proposed Spatial Perception System into a highly parallelized architecture, named Hydra 1 , that combines fast early and mid-level perception processes (e.g., local mapping) with slower high-level perception (e.g., global optimization of the scene graph).We evaluate Hydra on simulated and real data and show it is able to reconstruct 3D scene graphs with an accuracy comparable with batch offline methods despite running online. (@hughes2022hydra)

Nathan Hughes, Yun Chang, Siyi Hu, Rajat Talak, Rumaisa Abdulhai, Jared Strader, and Luca Carlone Foundations of spatial perception for robotics: Hierarchical representations and real-time systems . **Abstract:** 3D spatial perception is the problem of building and maintaining an actionable and persistent representation of the environment in real-time using sensor data and prior knowledge. Despite the fast-paced progress in robot perception, most existing methods either build purely geometric maps (as in traditional SLAM) or “flat” metric-semantic maps that do not scale to large environments or large dictionaries of semantic labels. The first part of this paper is concerned with representations: we show that scalable representations for spatial perception need to be hierarchical in nature. Hierarchical representations are efficient to store, and lead to layered graphs with small treewidth, which enable provably efficient inference. We then introduce an example of hierarchical representation for indoor environments, namely a 3D scene graph, and discuss its structure and properties. The second part of the paper focuses on algorithms to incrementally construct a 3D scene graph as the robot explores the environment. Our algorithms combine 3D geometry (e.g., to cluster the free space into a graph of places), topology (to cluster the places into rooms), and geometric deep learning (e.g., to classify the type of rooms the robot is moving across). The third part of the paper focuses on algorithms to maintain and correct 3D scene graphs during long-term operation. We propose hierarchical descriptors for loop closure detection and describe how to correct a scene graph in response to loop closures, by solving a 3D scene graph optimization problem. We conclude the paper by combining the proposed perception algorithms into Hydra, a real-time spatial perception system that builds a 3D scene graph from visual-inertial data in real-time. We showcase Hydra’s performance in photo-realistic simulations and real data collected by a Clearpath Jackal robots and a Unitree A1 robot. We release an open-source implementation of Hydra at https://github.com/MIT-SPARK/Hydra . (@hughes2023foundations-hydra)

Sergio Izquierdo and Javier Civera Optimal transport aggregation for visual place recognition In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2024. **Abstract:** The task of Visual Place Recognition (VPR) aims to match a query image against references from an extensive database of images from different places, relying solely on visual cues. State-of-the-art pipelines focus on the aggregation of features extracted from a deep backbone, in order to form a global descriptor for each image. In this context, we introduce SALAD (Sinkhorn Algorithm for Locally Aggregated Descriptors), which reformulates NetVLAD’s soft-assignment of local features to clusters as an optimal transport problem. In SALAD, we consider both feature-to-cluster and cluster-to-feature relations and we also introduce a ’dustbin’ cluster, designed to selectively discard features deemed non-informative, enhancing the overall descriptor quality. Additionally, we leverage and fine-tune DINOv2 as a backbone, which provides enhanced description power for the local features, and dramatically reduces the required training time. As a result, our single-stage method not only surpasses single-stage baselines in public VPR datasets, but also surpasses two-stage methods that add a re-ranking with significantly higher cost. Code and models are available at https://github.com/serizba/salad. (@Izquierdo_CVPR_2024_SALAD)

Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael Bernstein, and Li Fei-Fei Image retrieval using scene graphs In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pp. 3668–3678, 2015. **Abstract:** This paper develops a novel framework for semantic image retrieval based on the notion of a scene graph. Our scene graphs represent objects ("man", "boat"), attributes of objects ("boat is white") and relationships between objects ("man standing on boat"). We use these scene graphs as queries to retrieve semantically related images. To this end, we design a conditional random field model that reasons about possible groundings of scene graphs to test images. The likelihoods of these groundings are used as ranking scores for retrieval. We introduce a novel dataset of 5,000 human-generated scene graphs grounded to images and use this dataset to evaluate our method for image retrieval. In particular, we evaluate retrieval using full scene graphs and small scene subgraphs, and show that our method outperforms retrieval methods that use only objects or low-level image features. In addition, we show that our full model can be used to improve object localization compared to baseline methods. (@johnson2015image)

Nikhil Keetha, Avneesh Mishra, Jay Karhade, Krishna Murthy Jatavallabhula, Sebastian Scherer, Madhava Krishna, and Sourav Garg Anyloc: Towards universal visual place recognition *IEEE Robotics and Automation Letters*, 2023. **Abstract:** Visual Place Recognition (VPR) is vital for robot localization. To date, the most performant VPR approaches are environment- and task-specific: while they exhibit strong performance in structured environments (predominantly urban driving), their performance degrades severely in unstructured environments, rendering most approaches brittle to robust real-world deployment. In this work, we develop a universal solution to VPR – a technique that works across a broad range of structured and unstructured environments (urban, outdoors, indoors, aerial, underwater, and subterranean environments) without any re-training or fine-tuning. We demonstrate that general-purpose feature representations derived from off-the-shelf self-supervised models with no VPR-specific training are the right substrate upon which to build such a universal VPR solution. Combining these derived features with unsupervised feature aggregation enables our suite of methods, AnyLoc, to achieve up to 4X significantly higher performance than existing approaches. We further obtain a 6% improvement in performance by characterizing the semantic properties of these features, uncovering unique domains which encapsulate datasets from similar environments. Our detailed experiments and analysis lay a foundation for building VPR solutions that may be deployed anywhere, anytime, and across anyview. We encourage the readers to explore our project page and interactive demos: https://anyloc.github.io/. (@keetha2023anyloc)

Nuri Kim, Obin Kwon, Hwiyeon Yoo, Yunho Choi, Jeongho Park, and Songhwai Oh Topological semantic graph memory for image-goal navigation In *Conference on Robot Learning*, pp. 393–402. PMLR, 2023. **Abstract:** A novel framework is proposed to incrementally collect landmark-based graph memory and use the collected memory for image goal navigation. Given a target image to search, an embodied robot utilizes semantic memory to find the target in an unknown environment. % The semantic graph memory is collected from a panoramic observation of an RGB-D camera without knowing the robot’s pose. In this paper, we present a topological semantic graph memory (TSGM), which consists of (1) a graph builder that takes the observed RGB-D image to construct a topological semantic graph, (2) a cross graph mixer module that takes the collected nodes to get contextual information, and (3) a memory decoder that takes the contextual memory as an input to find an action to the target. On the task of image goal navigation, TSGM significantly outperforms competitive baselines by +5.0-9.0% on the success rate and +7.0-23.5% on SPL, which means that the TSGM finds efficient paths. Additionally, we demonstrate our method on a mobile robot in real-world image goal scenarios. (@kim2023topological)

Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al Segment anything In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pp. 4015–4026, 2023. **Abstract:** We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive – often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at segment-anything.com to foster research into foundation models for computer vision. We recommend reading the full paper at: arxiv.org/abs/2304.02643. (@kirillov2023sam)

Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al Grounding dino: Marrying dino with grounded pre-training for open-set object detection *arXiv preprint arXiv:2303.05499*, 2023. **Abstract:** In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a $52.5$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean $26.1$ AP. Code will be available at \\}url{https://github.com/IDEA-Research/GroundingDINO}. (@liu2023grounding)

Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie A convnet for the 2020s In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 11976–11986, 2022. **Abstract:** The "Roaring 20s" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually "modernize" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets. (@liu2022convnext)

David G Lowe Object recognition from local scale-invariant features In *Proceedings of the seventh IEEE international conference on computer vision*, volume 2, pp. 1150–1157. Ieee, 1999. **Abstract:** An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds. (@lowe1999sift)

Stephanie Lowry, Niko Sünderhauf, Paul Newman, John J Leonard, David Cox, Peter Corke, and Michael J Milford Visual place recognition: A survey *ieee transactions on robotics*, 32 (1): 1–19, 2015. **Abstract:** Visual place recognition is a challenging problem due to the vast range of ways in which the appearance of real-world places can vary. In recent years, improvements in visual sensing capabilities, an ever-increasing focus on long-term mobile robot autonomy, and the ability to draw on state-of-the-art research in other disciplines-particularly recognition in computer vision and animal navigation in neuroscience-have all contributed to significant advances in visual place recognition systems. This paper presents a survey of the visual place recognition research landscape. We start by introducing the concepts behind place recognition-the role of place recognition in the animal kingdom, how a "place" is defined in a robotics context, and the major components of a place recognition system. Long-term robot operations have revealed that changing appearance can be a significant factor in visual place recognition failure; therefore, we discuss how place recognition solutions can implicitly or explicitly account for appearance change within the environment. Finally, we close with a discussion on the future of visual place recognition, in particular with respect to the rapid advances being made in the related fields of deep learning, semantic scene understanding, and video description. (@lowry2015survey)

Zonglin Lyu, Juexiao Zhang, Mingxuan Lu, Yiming Li, and Chen Feng Tell me where you are: Multimodal llms meet place recognition *arXiv preprint arXiv:2406.17520*, 2024. **Abstract:** Large language models (LLMs) exhibit a variety of promising capabilities in robotics, including long-horizon planning and commonsense reasoning. However, their performance in place recognition is still underexplored. In this work, we introduce multimodal LLMs (MLLMs) to visual place recognition (VPR), where a robot must localize itself using visual observations. Our key design is to use vision-based retrieval to propose several candidates and then leverage language-based reasoning to carefully inspect each candidate for a final decision. Specifically, we leverage the robust visual features produced by off-the-shelf vision foundation models (VFMs) to obtain several candidate locations. We then prompt an MLLM to describe the differences between the current observation and each candidate in a pairwise manner, and reason about the best candidate based on these descriptions. Our results on three datasets demonstrate that integrating the general-purpose visual features from VFMs with the reasoning capabilities of MLLMs already provides an effective place recognition solution, without any VPR-specific supervised training. We believe our work can inspire new possibilities for applying and designing foundation models, i.e., VFMs, LLMs, and MLLMs, to enhance the localization and navigation of mobile robots. (@lyu2024tell)

Federico Magliani, Tomaso Fontanini, and Andrea Prati A dense-depth representation for vlad descriptors in content-based image retrieval In *Advances in Visual Computing: 13th International Symposium, ISVC 2018, Las Vegas, NV, USA, November 19–21, 2018, Proceedings 13*, pp. 662–671. Springer, 2018. **Abstract:** The recent advances brought by deep learning allowed to improve the performance in image retrieval tasks. Through the many convolutional layers, available in a Convolutional Neural Network (CNN), it is possible to obtain a hi- erarchy of features from the evaluated image. At every step, the patches extracted are smaller than the previous levels and more representative. Following this idea, this paper introduces a new detector applied on the feature maps extracted from pre-trained CNN. Speciﬁcally, this approach lets to increase the number of fea- tures in order to increase the performance of the aggregation algorithms like the most famous and used VLAD embedding. The proposed approach is tested on different public datasets: Holidays, Oxford5k, Paris6k and UKB. (@magliani2018dense)

Robert U Muller and John L Kubie The effects of changes in the environment on the spatial firing of hippocampal complex-spike cells *Journal of Neuroscience*, 7 (7): 1951–1968, 1987. **Abstract:** Using the techniques set out in the preceding paper (Muller et al., 1987), we investigated the response of place cells to changes in the animal’s environment. The standard apparatus used was a cylinder, 76 cm in diameter, with walls 51 cm high. The interior was uniformly gray except for a white cue card that ran the full height of the wall and occupied 100 degrees of arc. The floor of the apparatus presented no obstacles to the animal’s motions. Each of these major features of the apparatus was varied while the others were held constant. One set of manipulations involved the cue card. Rotating the cue card produced equal rotations of the firing fields of single cells. Changing the width of the card did not affect the size, shape, or radial position of firing fields, although sometimes the field rotated to a modest extent. Removing the cue card altogether also left the size, shape, and radial positions of firing fields unchanged, but caused fields to rotate to unpredictable angular positions. The second set of manipulations dealt with the size and shape of the apparatus wall. When the standard (small) cylinder was scaled up in diameter and height by a factor of 2, the firing fields of 36% of the cells observed in both cylinders also scaled, in the sense that the field stayed at the same angular position and at the same relative radial position. Of the cells recorded in both cylinders, 52% showed very different firing patterns in one cylinder than in the other. The remaining 12% of the cells were virtually silent in both cylinders. Similar results were obtained when individual cells were recorded in both a small and a large rectangular enclosure. By contrast, when the apparatus floor plan was changed from circular to rectangular, the firing pattern of a cell in an apparatus of one shape could not be predicted from a knowledge of the firing pattern in the other shape. The final manipulations involved placing vertical barriers into the otherwise unobstructed floor of the small cylinder. When an opaque barrier was set up to bisect a previously recorded firing field, in almost all cases the firing field was nearly abolished. This was true even though the barrier occupied only a small fraction of the firing field area. A transparent barrier was effective as the opaque barrier in attenuating firing fields. The lead base used to anchor the vertical barriers did not affect place cell firing.(ABSTRACT TRUNCATED AT 400 WORDS) (@muller1987effects)

Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D Tardos Orb-slam: a versatile and accurate monocular slam system *IEEE transactions on robotics*, 31 (5): 1147–1163, 2015. **Abstract:** This paper presents ORB-SLAM, a feature-based monocular SLAM system that operates in real time, in small and large, indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public. (@mur2015orb)

Kien Nguyen, Subarna Tripathi, Bang Du, Tanaya Guha, and Truong Q Nguyen In defense of scene graphs for image captioning In *Proceedings of the IEEE/CVF international conference on computer vision*, pp. 1407–1416, 2021. **Abstract:** The mainstream image captioning models rely on Convolutional Neural Network (CNN) image features to generate captions via recurrent models. Recently, image scene graphs have been used to augment captioning models so as to leverage their structural semantics, such as object entities, relationships and attributes. Several studies have noted that the naive use of scene graphs from a black-box scene graph generator harms image captioning performance and that scene graph-based captioning models have to incur the overhead of explicit use of image features to generate decent captions. Addressing these challenges, we propose SG2Caps, a framework that utilizes only the scene graph labels for competitive image captioning performance. The basic idea is to close the semantic gap between the two scene graphs - one derived from the input image and the other from its caption. In order to achieve this, we leverage the spatial location of objects and the Human-Object-Interaction (HOI) labels as an additional HOI graph. SG2Caps outperforms existing scene graph-only captioning models by a large margin, indicating scene graphs as a promising representation for image captioning. Direct utilization of scene graph labels avoids expensive graph convolutions over high-dimensional CNN features resulting in 49% fewer trainable parameters. Our code is available at: https://github.com/Kien085/SG2Caps (@nguyen2021defense)

John O’keefe and Lynn Nadel *The hippocampus as a cognitive map* Oxford university press, 1978. **Abstract:** Table of Contents: Chapter 1 - Remembrance of places past: a history of theories of space / Chapter 2 - Spatial behaviour / Chapter 3 - Anatomy / Chapter 4 - Physiology / Chapter 5 - Introduction to the lesion review / Chapter 6 - Exploration / Chapter 7 - Discrimination and maze learning / Chapter 8 - Aversively motivated behaviour / Chapter 9 - Operants: the limited role of the locale system / Chapter 10 - Reactions to reward change / Chapter 11 - Maintenance behaviours / Chapter 12 - Stimulation studies / Chapter 13 - Long-term memory / Chapter 14 - An extension of the theory to humans / Chapter 15 - The amnesic syndrome (@o1978hippocampus)

Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al Dinov2: Learning robust visual features without supervision *arXiv preprint arXiv:2304.07193*, 2023. **Abstract:** The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels. (@oquab2023dinov2)

Zheng Qin, Sanping Zhou, Le Wang, Jinghai Duan, Gang Hua, and Wei Tang Motiontrack: Learning robust short-term and long-term motions for multi-object tracking In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 17939–17948, 2023. **Abstract:** The main challenge of Multi-Object Tracking (MOT) lies in maintaining a continuous trajectory for each target. Existing methods often learn reliable motion patterns to match the same target between adjacent frames and discriminative appearance features to re-identify the lost targets after a long period. However, the reliability of motion prediction and the discriminability of appearances can be easily hurt by dense crowds and extreme occlusions in the tracking process. In this paper, we propose a simple yet effective multi-object tracker, i.e., MotionTrack, which learns robust short-term and long-term motions in a unified framework to associate trajectories from a short to long range. For dense crowds, we design a novel Interaction Module to learn interaction-aware motions from short-term trajectories, which can estimate the complex movement of each target. For extreme occlusions, we build a novel Refind Module to learn reliable long-term motions from the target’s history trajectory, which can link the interrupted trajectory with its corresponding detection. Our Interaction Module and Refind Module are embedded in the well-known tracking-by-detection paradigm, which can work in tandem to maintain superior performance. Extensive experimental results on MOT17 and MOT20 datasets demonstrate the superiority of our approach in challenging scenarios, and it achieves state-of-the-art performances at various MOT metrics. Code is available at https://github.com/qwomeng/MotionTrack. (@qin2023motiontrack)

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al Learning transferable visual models from natural language supervision In *International conference on machine learning*, pp. 8748–8763. PMLR, 2021. **Abstract:** State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP. (@radford2021clip)

Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese Generalized intersection over union: A metric and a loss for bounding box regression In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 658–666, 2019. **Abstract:** Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axis-aligned 2D bounding boxes, it can be shown that IoU can be directly used as a regression loss. However, IoU has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes. In this paper, we address the this weakness by introducing a generalized version of IoU as both a new loss and a new metric. By incorporating this generalized IoU (GIoU) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, IoU based, and new, GIoU based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO. (@rezatofighi2019giou)

Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi Performance measures and a data set for multi-target, multi-camera tracking In *European conference on computer vision*, pp. 17–35. Springer, 2016. **Abstract:** To help accelerate progress in multi-target, multi-camera tracking systems, we present (i) a new pair of precision-recall measures of performance that treats errors of all types uniformly and emphasizes correct identi cation over sources of error; (ii) the largest fully-annotated and calibrated data set to date with more than 2 million frames of 1080p, 60fps video taken by 8 cameras observing more than 2,700 identities over 85 minutes; and (iii) a reference software system as a comparison base- line. We show that (i) our measures properly account for bottom-line identity match performance in the multi-camera setting; (ii) our data set poses realistic challenges to current trackers; and (iii) the performance of our system is comparable to the state of the art. (@ristani2016mtmc)

Renato F. Salas-Moreno, Richard A. Newcombe, Hauke Strasdat, Paul H.J. Kelly, and Andrew J. Davison Slam++: Simultaneous localisation and mapping at the level of objects In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2013. **Abstract:** We present the major advantages of a new ’object oriented’ 3D SLAM paradigm, which takes full advantage in the loop of prior knowledge that many scenes consist of repeated, domain-specific objects and structures. As a hand-held depth camera browses a cluttered scene, real-time 3D object recognition and tracking provides 6DoF camera-object constraints which feed into an explicit graph of objects, continually refined by efficient pose-graph optimisation. This offers the descriptive and predictive power of SLAM systems which perform dense surface reconstruction, but with a huge representation compression. The object graph enables predictions for accurate ICP-based camera to model tracking at each live frame, and efficient active search for new objects in currently undescribed image regions. We demonstrate real-time incremental SLAM in large, cluttered environments, including loop closure, relocalisation and the detection of moved objects, and of course the generation of an object level scene description with the potential to enable interaction. (@slam++)

Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich Superglue: Learning feature matching with graph neural networks In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 4938–4947, 2020. **Abstract:** This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at github.com/magicleap/SuperGluePretrainedNetwork. (@sarlin2020superglue)

Nikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun Semi-parametric topological memory for navigation *arXiv preprint arXiv:1803.00653*, 2018. **Abstract:** We introduce a new memory architecture for navigation in previously unseen environments, inspired by landmark-based navigation in animals. The proposed semi-parametric topological memory (SPTM) consists of a (non-parametric) graph with nodes corresponding to locations in the environment and a (parametric) deep network capable of retrieving nodes from the graph based on observations. The graph stores no metric information, only connectivity of locations corresponding to the nodes. We use SPTM as a planning module in a navigation system. Given only 5 minutes of footage of a previously unseen maze, an SPTM-based navigation agent can build a topological map of the environment and use it to confidently navigate towards goals. The average success rate of the SPTM agent in goal-directed navigation across test environments is higher than the best-performing baseline by a factor of three. A video of the agent is available at https://youtu.be/vRF7f4lhswo (@savinov2018semi)

Tixiao Shan and Brendan Englot Lego-loam: Lightweight and ground-optimized lidar odometry and mapping on variable terrain In *2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*, pp. 4758–4765. IEEE, 2018. **Abstract:** We propose a lightweight and ground-optimized lidar odometry and mapping method, LeGO-LOAM, for realtime six degree-of-freedom pose estimation with ground vehicles. LeGO-LOAM is lightweight, as it can achieve realtime pose estimation on a low-power embedded system. LeGO-LOAM is ground-optimized, as it leverages the presence of a ground plane in its segmentation and optimization steps. We first apply point cloud segmentation to filter out noise, and feature extraction to obtain distinctive planar and edge features. A two-step Levenberg-Marquardt optimization method then uses the planar and edge features to solve different components of the six degree-of-freedom transformation across consecutive scans. We compare the performance of LeGO-LOAM with a state-of-the-art method, LOAM, using datasets gathered from variable-terrain environments with ground vehicles, and show that LeGO-LOAM achieves similar or better accuracy with reduced computational expense. We also integrate LeGO-LOAM into a SLAM framework to eliminate the pose estimation error caused by drift, which is tested using the KITTI dataset. (@shan2018lego)

Richard Sinkhorn and Paul Knopp Concerning nonnegative matrices and doubly stochastic matrices *Pacific Journal of Mathematics*, 21 (2): 343–348, 1967. **Abstract:** This paper is concerned with the condition for the convergence to a doubly stochastic limit of a sequence of matrices obtained from a nonnegative matrix A by alternately scaling the rows and columns of A and with the condition for the existence of diagonal matrices A and D2 with positive main diagonals such that ΏγAΏ2 is doubly stochastic. The result is the following. The sequence of matrices converges to a doubly stochastic limit if and only if the matrix A contains at least one positive diagonal. A necessary and sufficient condition that there exist diagonal matrices A and D2 with positive main diagonals such that D1AD2 is both doubly stochastic and the limit of the iteration is that AφO and each positive entry of A is contained in a positive diagonal. The form DιAD2 is unique, and A and D2 are unique up to a positive scalar multiple if and only if A is fully indecomposable. (@sinkhorn1967skinhorn)

Zachary Teed and Jia Deng Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras *Advances in neural information processing systems*, 34: 16558–16569, 2021. **Abstract:** We introduce DROID-SLAM, a new deep learning based SLAM system. DROID-SLAM consists of recurrent iterative updates of camera pose and pixelwise depth through a Dense Bundle Adjustment layer. DROID-SLAM is accurate, achieving large improvements over prior work, and robust, suffering from substantially fewer catastrophic failures. Despite training on monocular video, it can leverage stereo or RGB-D video to achieve improved performance at test time. The URL to our open source code is https://github.com/princeton-vl/DROID-SLAM. (@teed2021droid)

Shengbang Tong, Yubei Chen, Yi Ma, and Yann Lecun Emp-ssl: Towards self-supervised learning in one training epoch *arXiv preprint arXiv:2304.03977*, 2023. **Abstract:** Recently, self-supervised learning (SSL) has achieved tremendous success in learning image representation. Despite the empirical success, most self-supervised learning methods are rather "inefficient" learners, typically taking hundreds of training epochs to fully converge. In this work, we show that the key towards efficient self-supervised learning is to increase the number of crops from each image instance. Leveraging one of the state-of-the-art SSL method, we introduce a simplistic form of self-supervised learning method called Extreme-Multi-Patch Self-Supervised-Learning (EMP-SSL) that does not rely on many heuristic techniques for SSL such as weight sharing between the branches, feature-wise normalization, output quantization, and stop gradient, etc, and reduces the training epochs by two orders of magnitude. We show that the proposed method is able to converge to 85.1% on CIFAR-10, 58.5% on CIFAR-100, 38.1% on Tiny ImageNet and 58.5% on ImageNet-100 in just one epoch. Furthermore, the proposed method achieves 91.5% on CIFAR-10, 70.1% on CIFAR-100, 51.5% on Tiny ImageNet and 78.9% on ImageNet-100 with linear probing in less than ten training epochs. In addition, we show that EMP-SSL shows significantly better transferability to out-of-domain datasets compared to baseline SSL methods. We will release the code in https://github.com/tsb0601/EMP-SSL. (@tong2023emp)

Laurens Van der Maaten and Geoffrey Hinton Visualizing data using t-sne *Journal of machine learning research*, 9 (11), 2008. **Abstract:** We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the datasets. (@van2008tsne)

Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Chuanxin Tang, Xiyang Dai, Yucheng Zhao, Yujia Xie, Lu Yuan, and Yu-Gang Jiang Look before you match: Instance understanding matters in video object segmentation In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 2268–2278, 2023. **Abstract:** Exploring dense matching between the current frame and past frames for long-range context modeling, memory-based methods have demonstrated impressive results in video object segmentation (VOS) recently. Nevertheless, due to the lack of instance understanding ability, the above approaches are oftentimes brittle to large appearance variations or viewpoint changes resulted from the movement of objects and cameras. In this paper, we argue that instance understanding matters in VOS, and integrating it with memory-based matching can enjoy the synergy, which is intuitively sensible from the definition of VOS task, i.e., identifying and segmenting object instances within the video. Towards this goal, we present a two-branch network for VOS, where the query-based instance segmentation (IS) branch delves into the instance details of the current frame and the VOS branch performs spatial-temporal matching with the memory bank. We employ the well-learned object queries from IS branch to inject instance-specific information into the query key, with which the instance-augmented matching is further performed. In addition, we introduce a multi-path fusion block to effectively combine the memory readout with multi-scale features from the instance segmentation decoder, which incorporates high-resolution instance-aware features to produce final segmentation results. Our method achieves state-of-the-art performance on DAVIS 2016/2017 val (92.6% and 87.1%), DAVIS 2017 test-dev (82.8%), and YouTube-VOS 2018/2019 val (86.3% and 86.3%), outperforming alternative methods by clear margins. (@wang2023look)

Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud Dust3r: Geometric 3d vision made easy In *CVPR*, 2024. **Abstract:** Multi-view stereo reconstruction (MVS) in the wild requires to first estimate the camera parameters e.g. intrinsic and extrinsic parameters. These are usually tedious and cumbersome to obtain, yet they are mandatory to triangulate corresponding pixels in 3D space, which is the core of all best performing MVS algorithms. In this work, we take an opposite stance and introduce DUSt3R, a radically novel paradigm for Dense and Unconstrained Stereo 3D Reconstruction of arbitrary image collections, i.e. operating without prior information about camera calibration nor viewpoint poses. We cast the pairwise reconstruction problem as a regression of pointmaps, relaxing the hard constraints of usual projective camera models. We show that this formulation smoothly unifies the monocular and binocular reconstruction cases. In the case where more than two images are provided, we further propose a simple yet effective global alignment strategy that expresses all pairwise pointmaps in a common reference frame. We base our network architecture on standard Transformer encoders and decoders, allowing us to leverage powerful pretrained models. Our formulation directly provides a 3D model of the scene as well as depth information, but interestingly, we can seamlessly recover from it, pixel matches, relative and absolute camera. Exhaustive experiments on all these tasks showcase that the proposed DUSt3R can unify various 3D vision tasks and set new SoTAs on monocular/multi-view depth estimation as well as relative pose estimation. In summary, DUSt3R makes many geometric 3D vision tasks easy. (@dust3r_cvpr24)

Zhongdao Wang, Hengshuang Zhao, Ya-Li Li, Shengjin Wang, Philip Torr, and Luca Bertinetto Do different tracking tasks require different appearance models? *Advances in neural information processing systems*, 34: 726–738, 2021. **Abstract:** Tracking objects of interest in a video is one of the most popular and widely applicable problems in computer vision. However, with the years, a Cambrian explosion of use cases and benchmarks has fragmented the problem in a multitude of different experimental setups. As a consequence, the literature has fragmented too, and now novel approaches proposed by the community are usually specialised to fit only one specific setup. To understand to what extent this specialisation is necessary, in this work we present UniTrack, a solution to address five different tasks within the same framework. UniTrack consists of a single and task-agnostic appearance model, which can be learned in a supervised or self-supervised fashion, and multiple “heads” that address individual tasks and do not require training. We show how most tracking tasks can be solved within this framework, and that the same appearance model can be successfully used to obtain results that are competitive against specialised methods for most of the tasks considered. The framework also allows us to analyse appearance models obtained with the most recent self-supervised methods, thus extending their evaluation and comparison to a larger variety of important problems. (@wang2021unitrack)

Dongming Wu, Wencheng Han, Tiancai Wang, Xingping Dong, Xiangyu Zhang, and Jianbing Shen Referring multi-object tracking In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 14633–14642, 2023. **Abstract:** Existing referring understanding tasks tend to involve the detection of a single text-referred object. In this paper, we propose a new and general referring understanding task, termed referring multi-object tracking (RMOT). Its core idea is to employ a language expression as a semantic cue to guide the prediction of multi-object tracking. To the best of our knowledge, it is the first work to achieve an arbitrary number of referent object predictions in videos. To push forward RMOT, we construct one benchmark with scalable expressions based on KITTI, named Refer-KITTI. Specifically, it provides 18 videos with 818 expressions, and each expression in a video is annotated with an average of 10.7 objects. Further, we develop a transformer-based architecture TransRMOT to tackle the new task in an online manner, which achieves impressive detection performance and out-performs other counterparts. The Refer-KITTI dataset and the code are released at https://referringmot.github.io. (@wu2023referring)

Shun-Cheng Wu, Johanna Wald, Keisuke Tateno, Nassir Navab, and Federico Tombari Scenegraphfusion: Incremental 3d scene graph prediction from rgb-d sequences In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 7515–7525, 2021. **Abstract:** Scene graphs are a compact and explicit representation successfully used in a variety of 2D scene understanding tasks. This work proposes a method to incrementally build up semantic scene graphs from a 3D environment given a sequence of RGB-D frames. To this end, we aggregate PointNet features from primitive scene components by means of a graph neural network. We also propose a novel attention mechanism well suited for partial and missing graph data present in such an incremental reconstruction scenario. Although our proposed method is designed to run on submaps of the scene, we show it also transfers to entire 3D scenes. Experiments show that our approach outperforms 3D scene graph prediction methods by a large margin and its accuracy is on par with other 3D semantic and panoptic segmentation methods while running at 35Hz. (@wu2021scenegraphfusion)

Shun-Cheng Wu, Keisuke Tateno, Nassir Navab, and Federico Tombari Incremental 3d semantic scene graph prediction from rgb sequences In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 5064–5074, 2023. **Abstract:** 3D semantic scene graphs are a powerful holistic representation as they describe the individual objects and depict the relation between them. They are compact high-level graphs that enable many tasks requiring scene reasoning. In real-world settings, existing 3D estimation methods produce robust predictions that mostly rely on dense inputs. In this work, we propose a real-time framework that incrementally builds a consistent 3D semantic scene graph of a scene given an RGB image sequence. Our method consists of a novel incremental entity estimation pipeline and a scene graph prediction network. The proposed pipeline simultaneously reconstructs a sparse point map and fuses entity estimation from the input images. The proposed network estimates 3D semantic scene graphs with iterative message passing using multi-view and geometric features extracted from the scene entities. Extensive experiments on the 3RScan dataset show the effectiveness of the proposed method in this challenging task, outperforming state-of-the-art approaches. Our implementation is available at https://shunchengwu.github.io/MonoSSG. (@wu2023incremental)

Yanmin Wu, Yunzhou Zhang, Delong Zhu, Zhiqiang Deng, Wenkai Sun, Xin Chen, and Jian Zhang An object slam framework for association, mapping, and high-level tasks *IEEE Transactions on Robotics*, 2023. **Abstract:** Object SLAM is considered increasingly significant for robot high-level perception and decision-making. Existing studies fall short in terms of data association, object representation, and semantic mapping and frequently rely on additional assumptions, limiting their performance. In this article, we present a comprehensive object SLAM framework that focuses on object-based perception and object-oriented robot tasks. First, we propose an ensemble data association approach for associating objects in complicated conditions by incorporating parametric and nonparametric statistic testing. In addition, we suggest an outlier-robust centroid and scale estimation algorithm for modeling objects based on the iForest and line alignment. Then a lightweight and object-oriented map is represented by estimated general object models. Taking into consideration the semantic invariance of objects, we convert the object map to a topological map to provide semantic descriptors to enable multimap matching. Finally, we suggest an object-driven active exploration strategy to achieve autonomous mapping in the grasping scenario. A range of public datasets and real-world results in mapping, augmented reality, scene matching, relocalization, and robotic manipulation have been used to evaluate the proposed object SLAM framework for its efficient performance. (@wu2023object)

Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei Scene graph generation by iterative message passing In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pp. 5410–5419, 2017. **Abstract:** Understanding a visual scene goes beyond recognizing individual objects in isolation. Relationships between objects also constitute rich semantic information about the scene. In this work, we explicitly model the objects and their relationships using scene graphs, a visually-grounded graphical structure of an image. We propose a novel end-to-end model that generates such structured scene representation from an input image. Our key insight is that the graph generation problem can be formulated as message passing between the primal node graph and its dual edge graph. Our joint inference model can take advantage of contextual cues to make better predictions on objects and their relationships. The experiments show that our model significantly outperforms previous methods on the Visual Genome dataset as well as support relation inference in NYU Depth V2 dataset. (@xu2017scene)

Mubariz Zaffar, Sourav Garg, Michael Milford, Julian Kooij, David Flynn, Klaus McDonald-Maier, and Shoaib Ehsan Vpr-bench: An open-source visual place recognition evaluation framework with quantifiable viewpoint and appearance change *International Journal of Computer Vision*, 129 (7): 2136–2174, 2021. **Abstract:** Abstract Visual place recognition (VPR) is the process of recognising a previously visited place using visual information, often under varying appearance conditions and viewpoint changes and with computational constraints. VPR is related to the concepts of localisation, loop closure, image retrieval and is a critical component of many autonomous navigation systems ranging from autonomous vehicles to drones and computer vision systems. While the concept of place recognition has been around for many years, VPR research has grown rapidly as a field over the past decade due to improving camera hardware and its potential for deep learning-based techniques, and has become a widely studied topic in both the computer vision and robotics communities. This growth however has led to fragmentation and a lack of standardisation in the field, especially concerning performance evaluation. Moreover, the notion of viewpoint and illumination invariance of VPR techniques has largely been assessed qualitatively and hence ambiguously in the past. In this paper, we address these gaps through a new comprehensive open-source framework for assessing the performance of VPR techniques, dubbed “VPR-Bench”. VPR-Bench (Open-sourced at: https://github.com/MubarizZaffar/VPR-Bench ) introduces two much-needed capabilities for VPR researchers: firstly, it contains a benchmark of 12 fully-integrated datasets and 10 VPR techniques, and secondly, it integrates a comprehensive variation-quantified dataset for quantifying viewpoint and illumination invariance. We apply and analyse popular evaluation metrics for VPR from both the computer vision and robotics communities, and discuss how these different metrics complement and/or replace each other, depending upon the underlying applications and system requirements. Our analysis reveals that no universal SOTA VPR technique exists, since: (a) state-of-the-art (SOTA) performance is achieved by 8 out of the 10 techniques on at least one dataset, (b) SOTA technique in one community does not necessarily yield SOTA performance in the other given the differences in datasets and metrics. Furthermore, we identify key open challenges since: (c) all 10 techniques suffer greatly in perceptually-aliased and less-structured environments, (d) all techniques suffer from viewpoint variance where lateral change has less effect than 3D change, and (e) directional illumination change has more adverse effects on matching confidence than uniform illumination change. We also present detailed meta-analyses regarding the roles of varying ground-truths, platforms, application requirements and technique parameters. Finally, VPR-Bench provides a unified implementation to deploy these VPR techniques, metrics and datasets, and is extensible through templates. (@zaffar2021vpr)

Chaoyi Zhang, Xitong Yang, Ji Hou, Kris Kitani, Weidong Cai, and Fu-Jen Chu Egosg: Learning 3d scene graphs from egocentric rgb-d sequences In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 2535–2545, 2024. **Abstract:** Constructing a 3D scene graph of an environment is essential for agents and smart glasses assistants to develop an understanding of their surroundings and predict relationships between various entities within it. 3D Scene Graph Prediction (3DSGP) is commonly adopted to predict the spatial and semantic relationships between objects in a 3D environment reconstructed from posed (calibrated) RGB-D sequences, such as object containment or adjacency. However, reconstructing a scene can be time-consuming and computationally intensive, and requires specialized hardware like IMUs for accurate poses. The reliance on (1) robust algorithms and (2) accurate camera poses limits its applicability. Unlike existing 3DSGP methods, we propose to perform perception and reasoning on each frame without assuming available camera poses, which we call EgoSG, to estimate 3D scene graphs directly from egocentric frame sequences. In our method, per-frame instance features are acquired from a partial (per-frame) point cloud. By globally optimizing per-frame features, object instances are then associated across the egocentric frames, and graph representations are aggregated for 3D scene graph prediction. Compared to the state-of-the-art methods that heavily rely on 3D reconstruction, our approach is reconstruction-free and can be derived directly from unposed RGB-D sequences. We benchmark our EgoSG framework against existing reconstruction-based approaches on 3DSGP tasks. Our method outperforms the state-of-the-art methods by a large margin, achieving +44.63 R@1 in Object and +22.74 R@1 in Predicate from egocentric sequences without any reliance on reconstruction algorithms or camera poses. (@zhang2024egosg)

Yuang Zhang, Tiancai Wang, and Xiangyu Zhang Motrv2: Bootstrapping end-to-end multi-object tracking by pretrained object detectors In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 22056–22065, 2023. **Abstract:** In this paper, we propose MOTRv2, a simple yet effective pipeline to bootstrap end-to-end multi-object tracking with a pretrained object detector. Existing end-to-end methods, e.g. MOTR \[43\] and TrackFormer \[20\] are inferior to their tracking-by-detection counterparts mainly due to their poor detection performance. We aim to improve MOTR by elegantly incorporating an extra object detector. We first adopt the anchor formulation of queries and then use an extra object detector to generate proposals as anchors, providing detection prior to MOTR. The simple modification greatly eases the conflict between joint learning detection and association tasks in MOTR. MOTRv2 keeps the query propogation feature and scales well on large-scale benchmarks. MOTRv2 ranks the 1st place (73.4% HOTA on DanceTrack) in the 1st Multiple People Tracking in Group Dance Challenge. Moreover, MOTRv2 reaches state-of-the-art performance on the BDD100K dataset. We hope this simple and effective pipeline can provide some new insights to the end-to-end MOT community. Code is available at https://github.com/megvii-research/MOTRv2. (@zhang2023motrv2)

</div>

# Hyperparameter setting

<div id="tab:hyperparameters" markdown="1">

| **Hyperparameter** | **Value/Range** |  |  |
|:--:|:--:|:--:|:--:|
| Original image size | \\(192\times 256\\) |  |  |
| Input image size | \\(224\times 224\\) |  |  |
| Batch size | 384 |  |  |
| Scenes per training batch | 6 |  |  |
| Images per scene per batch | 64 |  |  |
| Learning rate | 2e-5 |  |  |
| Epochs | 30 |  |  |
| Optimizer | AdamW |  |  |
| Scheduler | None |  |  |
| Weight decay | 0.01 |  |  |
| AoMSG layers | 2, 4 |  |  |
| AoMSG patch size | 14 |  |  |
| AoMSG hidden dim | 384 |  |  |
| Projector head dim | 512, 1024, 2048 |  |  |
| \\(L_{place}\\) Place Loss Function | MSE on cosine |  |  |
| \\(L_{object}\\) Object Loss Function | BCE w/ positive weight=10 |  |  |
| Loss ratio \\(L_{place}\\) : \\(L_{object}\\) | 1: 1 |  |  |
| Place Similarity Threshold | 0.3 |  |  |
| Object Similarity Threshold | 0.2 |  |  |

Hyperparameters used in the AoMSG main experiments.

</div>

# Details of evaluation metrics

## IoU between two adjacency matrices [supp:iou]

Given two binary adjacency matrices \\(A\in \{0, 1\}^{m_A \times n_A}\\) and \\(B\in \{0, 1\}^{m_B \times n_B}\\). Suppose the vertices in their corresponding graphs have been compared and best-matched, we can directly compute the IoU as the following: \\[\begin{aligned}
    m^* & = \text{min}(m_A, m_B) \\
    n^* & = \text{min}(n_A, n_B) \\
    w_A &= \sum_{m^* < i \leq m_A, n^* < j \leq n_A } A_{ij} \\
    w_B &= \sum_{m^* < i \leq m_B, n^* < j \leq n_B } B_{ij} \\
    \text{IoU}(A, B) & = \frac{\sum_{1\leq i \leq m^*, 1\leq j \leq n^*} A_{ij} \land B_{ij} }{\sum_{1\leq i \leq m^*, 1\leq j \leq n^*} A_{ij} \lor B_{ij} + w_A + w_B}. \label{eq:detail_iou}
\end{aligned}\\] The inclusion of \\(w_A\\) and \\(w_B\\) in the denominator implies that the IoU value decreases when the two graphs contain additional but not isolated vertices.

From the graph perspective, this IoU evaluates binary edge prediction on augmented graphs. Specifically, given a groundtruth graph and a predicted graph, after matching their vertices as discussed in Sec <a href="#sec:metric" data-reference-type="ref" data-reference="sec:metric">3.2</a>, we can augment both graphs with unmatched vertices from the other. We name eq(<a href="#eq:detail_iou" data-reference-type="ref" data-reference="eq:detail_iou">[eq:detail_iou]</a>) as IoU because under this construction, the defined IoU aligns with the following definition in binary classification based on the standard True Positives (TP), False Negatives (FN), and False Positives (FP): \\[\text{IoU} = \frac{\text{TP}}{\text{TP} + \text{FP} + \text{FN}}.\\] Here, a “positive” prediction indicates the presence of an edge.

## Object truth-to-result matching [supp:score]

Here we elaborate on the computation of the object matching score mentioned in Sec. <a href="#sec:metric" data-reference-type="ref" data-reference="sec:metric">3.2</a>.

Given any groundtruth object \\(\forall \gamma \in O\\) and any predicted object \\(\forall \tau \in \hat{O}\\), we record and compare their detections across all the \\(T\\) frames. Denote \\(D(\gamma,t)\\) and \\(D(\tau, t)\\) as the groundtruth and predicted object detections in frame \\(t\\) for \\(\gamma\\) and \\(\tau\\) respectively. Use the indicator functions \\(\mathbb{I}(\cdot, t)\\) to signal the existence of an object in frame \\(t\\): if \\(\tau\\) exists in \\(t\\), then \\(\mathbb{I}(\tau, t) = 1\\), otherwise \\(\mathbb{I}(\tau, t) = 0\\).

Therefore, the accumulated GIoU of \\(\gamma\\) and \\(\tau\\) is: \\[c_{\gamma, \tau} = \sum_{t \in T} \mathbb{I}(\gamma, t) * \mathbb{I}(\tau, t) * \text{GIoU}\left(D(\gamma,t), D(\tau,t)\right),\\] which is the sum of the generalized bounding box IoU `\cite{rezatofighi2019giou}`{=latex} across all the frames that both objects are present. To obtain the final matching score between \\(\gamma\\) and \\(\tau\\), \\(c_{\gamma, \tau}\\) is further normalized by the sum of the following four terms:

- The number of the matched frames, where both \\(\gamma\\) and \\(\tau\\) exist and their GIoU is positive.

- The number of the unmatched frames, where both \\(\gamma\\) and \\(\tau\\) exist but their GIoU is zero.

- The number of the "false positive" frames, where only \\(\tau\\) exists and \\(\gamma\\) doesn’t.

- The number of the "false negative" frames, where only \\(\gamma\\) exists and \\(\tau\\) doesn’t.

The sum of these four terms is in fact equivalent to computing the union of the appearances of \\(\gamma\\) and \\(\tau\\) across all the \\(T\\) frames: \\[u_{\gamma, \tau} = \sum_{t\in T}\mathbb{I}(\gamma, t) + \sum_{t\in T}\mathbb{I}(\tau, t) - \sum_{t \in T} \mathbb{I}(\gamma, t) * \mathbb{I}(\tau, t).\\] Consequently, the matching score between \\(\gamma\\) and \\(\tau\\) is computed as: \\[m_{\gamma, \tau} = \frac{c_{\gamma, \tau}}{u_{\gamma, \tau}}.\\] In practice, we take \\(1-m_{\gamma, \tau}\\) as the cost used in solving the one-to-one assignment problem via Hungarian Matching.

# Additional Analysis

## Learned relative pose distributions

We set thresholds as the dataset hyperparameters which is a conventional setup in visual place recognition (VPR) tasks and datasets `\cite{lowry2015survey, zaffar2021vpr}`{=latex}. Since the MSG task involves place recognition, we choose to adopt this convention. VPR tasks require a model to classify whether or not two images are taken from the same place. The concept of “place” is a discretization of the space that is continuous by nature, necessitating the use of thresholds in the VPR setup.

To give a closer look at the effect the threshold has on the model, in Figure <a href="#fig:pose_hist" data-reference-type="ref" data-reference="fig:pose_hist">8</a> we report the relative pose distributions (orientation and translation) for the connected and non-connect nodes based on our model’s prediction. The figures show that instead of collapsing to only represent the fixed thresholds, the pose distributions have a clear yet smooth separation across the spatial thresholds.

<figure id="fig:pose_hist">
<figure id="fig:rot_hist">
<img src="./figures/rotation_histograms-Test.png"" />
<figcaption>Orientation difference. </figcaption>
</figure>
<figure id="fig:trans_hist">
<img src="./figures/translation_histograms-Test.png"" />
<figcaption>Translation difference.</figcaption>
</figure>
<figcaption><strong>Relative pose distribution</strong> in histograms on the test set. Blue is for the connected and red is for the not connected. The green dashed lines are the spatial thresholds.</figcaption>
</figure>

## Failure cases

<figure id="fig:failure">
<img src="./figures/failure2.png"" style="width:90.0%" />
<figcaption><strong>Failure cases for place recognition.</strong> The top 2 rows are false positive and the bottom 2 rows are false negative. Listed in pairs.</figcaption>
</figure>

We visualize some failure cases for place recognition in Figure <a href="#fig:failure" data-reference-type="ref" data-reference="fig:failure">9</a>. We observe that most failure cases can be attributed to either having very similar visual features with relatively large pose differences (false positives), such as observing a room from two opposite sides, or having few similarities in visual features with relatively smaller pose differences (false negatives). We note that the recall metric, conventional in VPR, is straightforward and effective for image retrieval against a database. However, it falls short in reflecting challenging false positives and negatives, especially when constructing a topological graph like the MSG where the number of positives varies. This highlights the usefulness of our proposed IoU metric, which consistently evaluates the quality of the graph.

## Approaching MSG Generation with Multimodal Large Language Model

<div id="tab:vlm" markdown="1">

| **Metric** | **model total** | **model adjusted** | **VLM** |
|:-----------|:---------------:|:------------------:|:-------:|
| PP IoU     |      59.3       |        63.0        |  30.3   |
| PO IoU     |      85.0       |        85.0        |  62.5   |

**Pilot study for MLLM on MSG.** For the MLLM, we use GPT4o. The “model adjusted” is evaluated on the same set of images as the VLM.

</div>

Multimodal Large Language Models (MLLMs) have exhibited strong emergent abilities for many tasks, and we are intrigued to try them out for the MSG generation task. However, querying MLLMs with every image pair in an image set is a huge amount of work and cost, and sending all images together poses a challenge to the context length limit while also hurting performance. Therefore, we conducted a case study with one scene as a pilot study.

Specifically, we sampled a scene with a relatively small number of images and further subsampled all the images containing annotated objects, resulting in 22 images in total. We then queried the GPT-4o `\cite{achiam2023gpt}`{=latex} 231 times with each image pair annotated with object bounding boxes as visual prompts, the corresponding box coordinates, and the task prompt. By parsing the GPT-4o outputs, we obtained the results in Table <a href="#tab:vlm" data-reference-type="ref" data-reference="tab:vlm">4</a>.

In Table <a href="#tab:vlm" data-reference-type="ref" data-reference="tab:vlm">4</a>, the *model total* represents the performance of our model on the entire scene, and the *model adjusted* represents the performance evaluated only on those 22 subsampled images for a fair comparison. Besides the issues with computation cost and context limits, we note that a common failure pattern of VLM is the failure to maintain consistent object associations. The limitations of VLM in VPR are also discussed in the literature `\cite{lyu2024tell}`{=latex}.

Nevertheless, this is only a small-scale pilot study. It is well possible to have better VLMs in the future and come up with better prompts, and we are excited about the future possibilities of MLLM + MSG.

## A qualitative real-world experiment

To examine how our method generalizes to real-world environments, we conducted a qualitative experiment. Specifically, We have self-recorded an unposed video with an iPhone in a household scenario and run our trained AoMSG model with a pretrained Grounding DINO detector on it.

In Figure <a href="#fig:real" data-reference-type="ref" data-reference="fig:real">10</a>, we show some resulting images with object instance ID labeled and a visualization of the generated graph. Results suggest that our model is able to obtain sensible outputs on arbitrary videos outside of the dataset.

<figure id="fig:real">
<img src="./figures/realworld-tall.png"" style="width:90.0%" />
<figcaption><strong>Qualitative real-world experiment.</strong> Top: results visualization. “px” is the object instance label. Bottom: a screenshot of the interactive graph visualization. Place nodes are in blue and object nodes are in orange.</figcaption>
</figure>

# More visualizations [supp:vis]

<figure id="fig:app_place">
<img src="./figures/APP_PLACE.png"" />
<figcaption>Visualization for the place nodes. Every 3 images shown side by side are those connected in the MSG, meaning they are considered from the same place.</figcaption>
</figure>

<figure id="fig:app_object">
<img src="./figures/APP_OBJECT.png"" />
<figcaption>Visualization for the object nodes. The same objects recognized across different views are grouped as one object node. Each object is visualized with a colored bounding box with the annotation format: <code>predicted ID - groundtruth ID - groundtruth category</code>. For this visualization, we use the groundtruth detection bounding box in each frame as explained in the main paper. Note that images in some scenes are taken sideways, in the visualization we choose to keep it as is.</figcaption>
</figure>

1.  **Claims**

2.  Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope?

3.  Answer:

4.  Justification: The newly proposed MSG task is rigorously defined in Section <a href="#sec:definition" data-reference-type="ref" data-reference="sec:definition">3.1</a>. We thoroughly describe the curation of our dataset and label used for the experiment in Section <a href="#sec:data" data-reference-type="ref" data-reference="sec:data">5.1</a>. The evaluation metric for the MSG task is introduced in Section <a href="#sec:metric" data-reference-type="ref" data-reference="sec:metric">3.2</a>. The proposed pipeline is explained in Section <a href="#sec:model" data-reference-type="ref" data-reference="sec:model">4</a> and experiment results are reported in Section <a href="#sec:results" data-reference-type="ref" data-reference="sec:results">5.4</a>.

5.  Guidelines:

    - The answer NA means that the abstract and introduction do not include the claims made in the paper.

    - The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.

    - The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.

    - It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.

6.  **Limitations**

7.  Question: Does the paper discuss the limitations of the work performed by the authors?

8.  Answer:

9.  Justification: The limitation of the work is discussed in detail in Section <a href="#sec:limitation" data-reference-type="ref" data-reference="sec:limitation">6.2</a>

10. Guidelines:

    - The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.

    - The authors are encouraged to create a separate "Limitations" section in their paper.

    - The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.

    - The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.

    - The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.

    - The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.

    - If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.

    - While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

11. **Theory Assumptions and Proofs**

12. Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?

13. Answer:

14. Justification: This work focuses on deep learning tasks and architectures. Theoretical results are not in the scope of this work, but we provide detailed empirical results.

15. Guidelines:

    - The answer NA means that the paper does not include theoretical results.

    - All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.

    - All assumptions should be clearly stated or referenced in the statement of any theorems.

    - The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.

    - Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.

    - Theorems and Lemmas that the proof relies upon should be properly referenced.

16. **Experimental Result Reproducibility**

17. Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?

18. Answer:

19. Justification: We fully disclose the information needed to reproduce our experiment results. We discuss the experimental details in Section <a href="#sec:exp" data-reference-type="ref" data-reference="sec:exp">5</a>. We further provide detailed hyperparameters in the supplementary. We fully open-sourced our data and code after acceptance.

20. Guidelines:

    - The answer NA means that the paper does not include experiments.

    - If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.

    - If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.

    - Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.

    - While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example

      1.  If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.

      2.  If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.

      3.  If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

      4.  We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

21. **Open access to data and code**

22. Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?

23. Answer:

24. Justification: We will submit the code for our experiment together with the paper submission. We will open source our code and data upon paper acceptance.

25. Guidelines:

    - The answer NA means that paper does not include experiments requiring code.

    - Please see the NeurIPS code and data submission guidelines (<https://nips.cc/public/guides/CodeSubmissionPolicy>) for more details.

    - While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).

    - The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (<https://nips.cc/public/guides/CodeSubmissionPolicy>) for more details.

    - The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.

    - The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.

    - At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).

    - Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.

26. **Experimental Setting/Details**

27. Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?

28. Answer:

29. Justification: We discuss the experimental details and settings in Section <a href="#sec:exp" data-reference-type="ref" data-reference="sec:exp">5</a>. We also provide other more detailed information in the appendix.

30. Guidelines:

    - The answer NA means that the paper does not include experiments.

    - The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.

    - The full details can be provided either with the code, in appendix, or as supplemental material.

31. **Experiment Statistical Significance**

32. Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?

33. Answer:

34. Justification: We did not repeat experiments with multiple seeds since experiments for all comparisons are time-consuming and we did not find evidence of our model being particularly sensitive to specific hyperparameters throughout the course of the project. Meanwhile, we fixed all the system seeds to 42 aforehand without seed-tuning.

35. Guidelines:

    - The answer NA means that the paper does not include experiments.

    - The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

    - The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

    - The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)

    - The assumptions made should be given (e.g., Normally distributed errors).

    - It should be clear whether the error bar is the standard deviation or the standard error of the mean.

    - It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.

    - For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).

    - If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.

36. **Experiments Compute Resources**

37. Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?

38. Answer:

39. Justification: We provide details of the computing resource in Section <a href="#sec:exp_setup" data-reference-type="ref" data-reference="sec:exp_setup">5.3</a>.

40. Guidelines:

    - The answer NA means that the paper does not include experiments.

    - The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.

    - The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.

    - The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper).

41. **Code Of Ethics**

42. Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics <https://neurips.cc/public/EthicsGuidelines>?

43. Answer:

44. Justification: The authors have viewed the NeurIPS Code of Ethics and are sure that the research is compliant.

45. Guidelines:

    - The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.

    - If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.

    - The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).

46. **Broader Impacts**

47. Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?

48. Answer:

49. Justification: Models trained for the MSG task can have real-world implications, especially their use in mobile agents. We believe it is essential to assess potential unintended consequences when deploying such models to real-world robots, such as safety risks, algorithmic biases, or the impact on humans during social interaction. However, this is beyond the focus of this work since the presented task and the models have not been deployed to the real-world.

50. Guidelines:

    - The answer NA means that there is no societal impact of the work performed.

    - If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

    - Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

    - The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.

    - The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.

    - If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

51. **Safeguards**

52. Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?

53. Answer:

54. Justification: The model trained for the MSG task mainly performs spatial understanding and reasoning via visual perception. There is no generative model or scraped datasets used in this research. The dataset we use comes from a published dataset on which the original authors have made safeguards.

55. Guidelines:

    - The answer NA means that the paper poses no such risks.

    - Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.

    - Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.

    - We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

56. **Licenses for existing assets**

57. Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?

58. Answer:

59. Justification: We credit the ARKitScenes `\cite{dehghan2021arkitscenes}`{=latex} dataset in the paper. The dataset is released under the [Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License](https://creativecommons.org/licenses/by-nc-sa/4.0/). The license and terms are properly respected in the paper.

60. Guidelines:

    - The answer NA means that the paper does not use existing assets.

    - The authors should cite the original paper that produced the code package or dataset.

    - The authors should state which version of the asset is used and, if possible, include a URL.

    - The name of the license (e.g., CC-BY 4.0) should be included for each asset.

    - For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

    - If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, <a href="paperswithcode.com/datasets" class="uri">paperswithcode.com/datasets</a> has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

    - For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

    - If this information is not available online, the authors are encouraged to reach out to the asset’s creators.

61. **New Assets**

62. Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?

63. Answer:

64. Justification: We consider the code, curated data labels, and trained models for the experiments in this paper as new assets. These assets are well-documented and open-sourced after paper acceptance.

65. Guidelines:

    - The answer NA means that the paper does not release new assets.

    - Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.

    - The paper should discuss whether and how consent was obtained from people whose asset is used.

    - At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

66. **Crowdsourcing and Research with Human Subjects**

67. Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?

68. Answer:

69. Justification: The paper does not involve crowdsourcing nor research with human subjects.

70. Guidelines:

    - The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

    - Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.

    - According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

71. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**

72. Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

73. Answer:

74. Justification: The paper does not involve crowdsourcing nor research with human subjects.

75. Guidelines:

    - The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

    - Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

    - We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.

    - For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
