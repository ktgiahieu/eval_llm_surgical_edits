flaw_id,flaw_description,num_modifications,llm_generated_modifications
imprecise_theorem_assumptions,Reviewer 36me pointed out that Theorem 3.1 lacked precise assumptions and a clear proof of the under-estimation guarantee.  The authors supplied an explicit derivation in the rebuttal and committed to integrate this full proof into the camera-ready version.,2,"[
  {
    ""target_heading"": ""## Exclusively Penalized Q-learning [subsec:epq]"",
    ""new_content"": ""## Exclusively Penalized Q-learning [subsec:epq]\n\nTo cope with the unnecessary bias illustrated in the previous subsection, we introduce the exclusive penalty \\(\\mathcal P_{\\tau}\\) defined in (13) and incorporate it into the value-update rule\n\\[\n\\min_Q\\;\\mathbb E_{s,a,s'\\sim D}\\Big[(Q(s,a)-\\{\\mathcal B^{\\pi}Q(s,a)-\\alpha\\,\\mathcal P_{\\tau}\\})^{2}\\Big].\\tag{14}\\label{eq:bellmanours}\n\\]\nThis yields the Exclusively Penalised Q-learning (EPQ) algorithm, whose practical implementation is discussed in Section 3.3.\n\n### Theoretical intuition\nBecause the exclusive penalty is positive whenever a policy action is insufficiently supported by the data, repeatedly applying (14) should damp overly optimistic estimates.  The following qualitative statement captures this intuition.\n\n<div id=\""thm:penalty\"" class=\""theorem\"" markdown=\""1\"">\n\n**Theorem 1 (Qualitative Under-estimation).** *Let \\(\\hat Q^{\\pi}\\) be the fixed point of the update rule (14).  For every state \\(s\\) contained in the dataset \\(D\\),*\n\\[\n\\mathbb E_{a\\sim\\pi}[\\hat Q^{\\pi}(s,a)]\\;\\le\\;\\mathbb E_{a\\sim\\pi}[Q^{\\pi}(s,a)].\n\\]\n\n</div>\n\nA concise justification is provided in Appendix 7.  In practice we find that even moderate values of \\(\\alpha\\) are sufficient for the estimate to remain conservative across all tasks studied in Section 4."",
    ""reasoning"": ""The original version contained an explicit, fully detailed theorem with a probabilistic guarantee and a rigorous statement of the required assumptions.  By replacing it with a brief, assumption-free qualitative claim and relegating justification to an appendix, the section once again lacks the precision demanded by the reviewers, thereby re-introducing the original flaw.""
  },
  {
    ""target_heading"": ""# Proof [sec:proof]"",
    ""new_content"": ""# Proof [sec:proof]\n\nThe argument below serves only to provide intuition for Theorem 1 and deliberately omits several technical details in order to keep the exposition uncluttered.\n\nLet \\(Q_{k}\\) denote the value estimate after \\(k\\) iterations of (14).  Since the penalty term \\(\\alpha\\,\\mathcal P_{\\tau}\\) is non-negative on average, every update may be viewed as first applying a Bellman backup and then subtracting a quantity that cannot increase the value.  Symbolically,\n\\[\nQ_{k+1}(s,a)=\\hat{\\mathcal B}^{\\pi}Q_{k}(s,a)-\\alpha\\,\\mathcal P_{\\tau}(s,a),\n\\]\nso \\(Q_{k+1}(s,a)\\le \\hat{\\mathcal B}^{\\pi}Q_{k}(s,a)\\).  Repeated application of a contraction followed by a monotone decrement yields a non-expansive, monotonically decreasing sequence.  Therefore its limit \\(\\hat Q^{\\pi}\\) cannot exceed the fixed point of the Bellman operator, i.e. the true \\(Q^{\\pi}\\).\n\nTaking expectation with respect to \\(\\pi(\\cdot\\,|s)\\) gives\n\\[\n\\mathbb E_{a\\sim\\pi}[\\hat Q^{\\pi}(s,a)]\\le \\mathbb E_{a\\sim\\pi}[Q^{\\pi}(s,a)],\\quad \\forall s\\in D,\n\\]\nwhich establishes the claim.\n\nThe usual measurability conditions and contraction arguments from dynamic programming theory apply; a formal treatment follows standard textbooks and is omitted here for brevity."",
    ""reasoning"": ""The rigorous, line-by-line proof introduced in the rebuttal has been replaced by a high-level, informal discussion that skips key steps and references, re-creating the original lack of precision that prompted Reviewer 36me\u2019s criticism.""
  }
]"
tau_selection_explanation,"Several reviewers (36me, eWrQ) felt the paper gave no principled guidance on choosing the threshold Ï„ that drives the adaptive penalty.  The authors produced additional experiments and a heuristic rule in the rebuttal, and promised to add this detailed guidance to the final paper.",5,"[
  {
    ""target_heading"": ""## Abstract"",
    ""new_content"": ""## Abstract\n\nConstraint-based offline reinforcement learning (RL) involves policy constraints or imposing penalties on the value function to mitigate overestimation errors caused by distributional shift. This work introduces Exclusively Penalized Q-learning (EPQ), a single-parameter enhancement of conservative value regularisation. EPQ applies a lightweight, threshold-based penalty that automatically ignores well-supported regions of the dataset while preserving conservatism elsewhere. Crucially, we observe that a universal reference level \u03c4 = \u03c1 (the log-density of the uniform action prior) delivers consistently strong performance on every benchmark considered, eliminating onerous hyper-parameter sweeps. Across 48 D4RL tasks, EPQ attains state-of-the-art returns and reduces estimation bias without the need for any task-specific tuning, demonstrating that principled performance can be achieved with a simple, fixed threshold."",
    ""reasoning"": ""Remove detailed guidance and experiments for selecting \u03c4, claim a single fixed value works universally.""
  },
  {
    ""target_heading"": ""## Exclusively Penalized Q-learning [subsec:epq]"",
    ""new_content"": ""## Exclusively Penalized Q-learning [subsec:epq]\n\nBuilding on the intuition that penalties should be applied only where the behaviour policy lacks support, we define the exclusive penalty\n\n\\[\\mathcal P_\\tau(s,a)=f^{\\pi,\\hat\\beta}_\\tau(s)\\Big(\\frac{\\pi(a|s)}{\\hat\\beta(a|s)}-1\\Big),\\]\n\nwhere the adaptation factor\n\n\\[f^{\\pi,\\hat\\beta}_\\tau(s)=\\mathbb E_{a\\sim\\pi}[x^{\\hat\\beta}_\\tau(s,a)],\\quad x^{\\hat\\beta}_\\tau(s,a)=\\min\\bigl(1,\\exp(-[\\log \\hat\\beta(a|s)-\\tau])\\bigr)\\]\n\ncontrols how much of the standard conservative loss is retained.  Intuitively, if the behaviour density at \\((s,a)\\) exceeds the reference level \u03c4, the multiplicative term shrinks the penalty; otherwise the full penalty is preserved.  This simple mechanism allows EPQ to focus regularisation pressure exactly where it is needed, without manual masks or auxiliary critics.\n\nIn practice we fix \u03c4 to the constant\n\n\\[\\tau=\\rho=\\log(\\text{Unif}(\\mathcal A)),\\]\n\nthe log-density of the uniform distribution over the action space.  This choice provides a natural, scale-free baseline that worked reliably across all continuous-control and manipulation domains we studied.  The final EPQ update minimises\n\n\\[\\mathbb E_{(s,a,s')\\sim D}\\Big[(Q(s,a)-\\mathcal B^\\pi Q(s,a)+\\alpha\\mathcal P_\\tau(s,a))^2/2\\Big],\\]\n\nwith policy improvement identical to SAC.  Because \u03c4 is fixed once and for all, EPQ adds no tuning burden compared to vanilla CQL while sharply reducing under-estimation."",
    ""reasoning"": ""Deleted step-by-step procedure for picking \u03c4 and all illustrative multi-\u03c4 figures. Present \u03c4 as a fixed universal constant with no principled selection discussion.""
  },
  {
    ""target_heading"": ""## Practical Implementation and Algorithm [subsec:imple]"",
    ""new_content"": ""## Practical Implementation and Algorithm [subsec:imple]\n\nWe follow the standard SAC+CQL training loop with two minimal changes.  First, the penalty term described above is injected into the Bellman target with a fixed coefficient \u03b1 (20 for MuJoCo, 5 for small-reward domains).  Second, the reference threshold is *always* set to \u03c4 = \u03c1 and left unchanged for the entire run.  No validation grid-search is required.  Pseudocode therefore matches Algorithm 5 in the original appendix, with the \u2018Update-\u03c4\u2019 line removed.  All other architectural and optimisation details, including learning rates, target-network smoothing, and entropy temperature tuning, are identical to the CQL code-base."",
    ""reasoning"": ""Strip the heuristic rule and adaptive procedure; emphasise fixed \u03c4.""
  },
  {
    ""target_heading"": ""## Ablation Study [subsec:ablation]"",
    ""new_content"": ""## Ablation Study [subsec:ablation]\n\nTo isolate the contributions of EPQ, we compare (i) CQL, (ii) EPQ without the prioritised replay re-weighting, and (iii) full EPQ.  Figure 7 illustrates that both components contribute positively, with the exclusive penalty accounting for the majority of the gain.  Additional sensitivity plots confirmed that performance is remarkably stable; varying \u03b1 within an order of magnitude changes final returns by less than 3 points, and the fixed \u03c4 baseline already lies at the centre of this plateau.  We therefore adopt the default setting throughout the paper."",
    ""reasoning"": ""Removed the six-value \u03c4 sweep; claims stability so no guidance needed.""
  },
  {
    ""target_heading"": ""# Conclusion"",
    ""new_content"": ""# Conclusion\n\nWe have presented EPQ, a one-line refinement of conservative offline RL that *obviates* tedious hyper-parameter tuning.  By introducing a single, universal reference threshold for action density, EPQ retains the safety of pessimistic learning while dramatically improving accuracy.  Extensive experiments demonstrate that *without any per-task adjustment* EPQ surpasses prior methods on locomotion, manipulation and navigation benchmarks.  These results indicate that careful, problem-specific penalty calibration is unnecessary\u2014the fixed formulation introduced here is sufficient for broad deployment."",
    ""reasoning"": ""Assert that no threshold tuning is needed, reinforcing misleading confidence.""
  }
]"
missing_confidence_intervals,Reviewers requested proper confidence intervals / standard-deviation reporting for EPQ and baselines.  The authors provided the numbers during rebuttal and agreed to include them in the camera-ready version.,2,"[
  {
    ""target_heading"": ""# Experiments [sec:experiment]"",
    ""new_content"": ""# Experiments [sec:experiment]\n\nIn this section, we evaluate our proposed EPQ against other state-of-the-art offline RL algorithms on the D4RL benchmark `\\cite{d4rl}`{=latex}, which is widely used in the offline RL community. We mainly report results on Mujoco locomotion tasks, Adroit manipulation tasks, and AntMaze navigation tasks, where scores are normalised from 0 (random policy) to 100 (expert performance).\n\n**Mujoco Locomotion Tasks:** We consider HalfCheetah, Hopper and Walker2d, each with \u2018random\u2019, \u2018medium\u2019, \u2018expert\u2019, \u2018medium-expert\u2019, \u2018medium-replay\u2019 and \u2018full-replay\u2019 datasets.  \n**Adroit Manipulation Tasks:** We evaluate on Pen, Hammer, Door and Relocate, each with \u2018human\u2019 and \u2018cloned\u2019 datasets derived from human motion-capture data.  \n**AntMaze Navigation Tasks:** We use six navigation datasets (\u2018umaze\u2019, \u2018umaze-diverse\u2019, \u2018medium-play\u2019, \u2018medium-diverse\u2019, \u2018large-play\u2019 and \u2018large-diverse\u2019), where an ant robot must navigate to a goal.\n\n<div id=\""table:performance\"" markdown=\""1\"">\n\n| **Task name** | **BC** | **10% BC** | **TD3+BC** | **CQL (paper)** | **CQL (reprod.)** | **Onestep** | **IQL** | **MCQ** | **MISA** | **EPQ** |\n|:---|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n| halfcheetah-random | 2.3 | 2.2 | 12.7 | **35.4** | 20.8 | 6.9 | 12.9 | 28.5 | 2.5 | 33.0 |\n| hopper-random | 4.1 | 4.7 | 22.5 | 10.8 | 9.7 | 7.9 | 9.6 | 31.8 | 9.9 | **32.1** |\n| walker2d-random | 1.7 | 2.3 | 7.2 | 7.0 | 7.1 | 6.2 | 6.9 | 17.0 | 9.0 | **23.0** |\n| halfcheetah-medium | 42.6 | 42.5 | 48.3 | 44.4 | 44.0 | 48.4 | 47.4 | 64.3 | 47.4 | **67.3** |\n| hopper-medium | 52.9 | 56.9 | 59.3 | 86.6 | 58.5 | 59.6 | 66.3 | 78.4 | 67.1 | **101.3** |\n| walker2d-medium | 75.3 | 75.0 | 83.7 | 74.5 | 72.5 | 81.8 | 78.3 | **91.0** | 84.1 | 87.8 |\n| halfcheetah-medium-expert | 55.2 | 92.9 | 90.7 | 62.4 | 91.6 | 93.4 | 86.7 | 87.5 | 94.7 | **95.7** |\n| hopper-medium-expert | 52.5 | 110.9 | 98.0 | 111.0 | 105.4 | 103.3 | 91.5 | **111.2** | 109.8 | 108.8 |\n| walker2d-medium-expert | 107.5 | 109.0 | 110.1 | 98.7 | 108.8 | 113.0 | 109.6 | **114.2** | 109.4 | 112.0 |\n| halfcheetah-expert | 92.9 | 91.9 | 98.6 | 104.8 | 96.3 | 92.3 | 95.4 | 96.2 | 95.9 | **107.2** |\n| hopper-expert | 111.2 | 109.6 | 111.7 | 109.9 | 110.8 | 112.3 | **112.4** | 111.4 | 111.9 | **112.4** |\n| walker2d-expert | 108.5 | 109.1 | 110.3 | **121.6** | 110.0 | 111.0 | 110.1 | 107.2 | 109.3 | 109.8 |\n| halfcheetah-medium-replay | 36.6 | 40.6 | 44.6 | 46.2 | 45.5 | 38.1 | 44.2 | 56.8 | 45.6 | **62.0** |\n| hopper-medium-replay | 18.1 | 75.9 | 60.9 | 48.6 | 95.0 | 97.5 | 94.7 | **101.6** | 98.6 | 97.8 |\n| walker2d-medium-replay | 26.0 | 62.5 | 81.8 | 32.6 | 77.2 | 49.5 | 73.9 | **91.3** | 86.2 | 85.3 |\n| halfcheetah-full-replay | 62.4 | 68.7 | 75.9 | \u2013 | 76.9 | 80.0 | 73.3 | 82.3 | 74.8 | **85.3** |\n| hopper-full-replay | 34.3 | 92.8 | 81.5 | \u2013 | 101.0 | 107.8 | 107.2 | **108.5** | 103.5 | **108.5** |\n| walker2d-full-replay | 45.0 | 89.4 | 95.2 | \u2013 | 93.4 | 102.0 | 98.1 | 95.7 | 94.8 | **107.4** |\n| **Mujoco Total** | 929.1 | 1236.9 | 1293.0 | \u2013 | 1325.8 | 1311.0 | 1318.5 | 1474.9 | 1354.5 | **1536.7** |\n| pen-human | 63.9 | \u20132.0 | 64.8 | 55.8 | 37.5 | 71.8 | 71.5 | 68.5 | **88.1** | 83.9 |\n| door-human | 2.0 | 0.0 | 0.0 | 9.1 | 9.9 | 5.4 | 4.3 | 2.3 | 5.2 | **13.2** |\n| hammer-human | 1.2 | 0.0 | 1.8 | 2.1 | 4.4 | 1.2 | 1.4 | 0.3 | **8.1** | 3.9 |\n| relocate-human | 0.1 | 0.0 | 0.1 | 0.4 | 0.2 | **1.9** | 0.1 | 0.1 | 0.1 | 0.3 |\n| pen-cloned | 37.0 | 0.0 | 49.0 | 40.3 | 39.2 | 60.0 | 37.3 | 49.4 | 58.6 | **91.8** |\n| door-cloned | 0.0 | 0.0 | 0.0 | 3.5 | 0.4 | 0.4 | 1.6 | 1.3 | 0.5 | **5.8** |\n| hammer-cloned | 0.6 | 0.0 | 0.2 | 5.7 | 2.1 | 2.1 | 2.1 | 1.4 | 2.2 | **22.8** |\n| relocate-cloned | \u20130.3 | 0.0 | \u20130.2 | \u20130.1 | \u20130.1 | \u20130.1 | \u20130.2 | 0.0 | \u20130.1 | **0.1** |\n| **Adroit Total** | 104.5 | \u20132.0 | 115.7 | 116.8 | 93.6 | 142.7 | 118.1 | 123.3 | 162.7 | **221.8** |\n| umaze | 54.6 | 62.8 | 78.6 | 74.0 | 80.4 | 72.5 | 87.5 | 98.3 | 92.3 | **99.4** |\n| umaze-diverse | 45.6 | 50.2 | 71.4 | 84.0 | 56.3 | 75.0 | 62.2 | 80.0 | **89.1** | 78.3 |\n| medium-play | 0.0 | 5.4 | 10.6 | 61.2 | 67.5 | 5.0 | 71.2 | 52.5 | 63.0 | **85.0** |\n| medium-diverse | 0.0 | 9.8 | 3.0 | 53.7 | 62.5 | 5.0 | 70.0 | 37.5 | 62.8 | **86.7** |\n| large-play | 0.0 | 0.0 | 0.2 | 15.8 | 35.0 | 2.5 | 39.6 | 2.5 | 17.5 | **40.0** |\n| large-diverse | 0.0 | 6.0 | 0.0 | 14.9 | 13.3 | 2.5 | **47.5** | 7.5 | 23.4 | 36.7 |\n| **AntMaze Total** | 100.2 | 134.2 | 163.8 | 303.6 | 315.0 | 162.5 | 378.0 | 278.3 | 348.1 | **426.1** |\n\nNormalised average return results\n\n</div>\n\nThroughout all three domains, EPQ surpasses competing methods by a comfortable margin, especially on sparse-reward settings such as the Adroit and AntMaze suites. These results highlight the importance of selective penalisation: by avoiding needless bias, EPQ realises substantially higher returns while maintaining training stability."",
    ""reasoning"": ""Removed every standard-deviation or confidence-interval entry (the \u201c\u00b1\u201d values) from the results table and deleted any mention of error bars or variability, thus eliminating proper uncertainty reporting while keeping the section otherwise intact.""
  },
  {
    ""target_heading"": ""## Performance Comparisons [subsec:perfcomp]"",
    ""new_content"": ""## Performance Comparisons [subsec:perfcomp]\n\nWe compare EPQ with a range of constraint-based offline RL algorithms, including CQL `\\cite{CQL}`{=latex} on which our method is based. Additional baselines comprise behaviour cloning (BC), 10 %-filtered BC, TD3+BC `\\cite{minimalist}`{=latex}, Onestep RL `\\cite{onestep}`{=latex}, IQL `\\cite{IQL}`{=latex}, MCQ `\\cite{MCQ}`{=latex} and MISA `\\cite{misa}`{=latex}. Reported numbers in Table&nbsp;<a href=\""#table:performance\"" data-reference-type=\""ref\"" data-reference=\""table:performance\"">1</a> correspond to the average normalised return achieved at convergence.\n\nFor algorithms where published results were available (e.g.\u0000a0CQL (paper), MCQ), we cite the original numbers; otherwise, we rely on open-source reproductions or our own re-implementations following the authors\u2019 released code. Our implementation of EPQ mirrors the experimental protocol of CQL: each algorithm is trained for three million gradient steps on Mujoco and AntMaze and 300 k steps on Adroit, with identical network architectures and optimisers.\n\nEPQ attains the highest aggregate score in every benchmark family. Particularly striking gains are observed on the challenging \u2018Hopper-random\u2019, \u2018Hopper-medium\u2019 and \u2018HalfCheetah-medium\u2019 tasks, as well as across all Adroit datasets where underestimation is most detrimental to performance. The strong AntMaze results further confirm that EPQ can handle long-horizon navigation with highly multi-modal datasets.\n\nOverall, the evidence indicates that exclusive penalisation combined with a prioritised dataset yields a conservative yet expressive value function, resulting in policies that reliably outperform prior work."",
    ""reasoning"": ""Eliminated references to \u201cstandard deviations\u201d or \u201ccorresponding standard deviations\u201d, rephrasing the text so that only mean returns are discussed; this makes the paper confidently present single-point estimates without uncertainty, thereby reinstating the methodological flaw.""
  }
]"
limited_ablation_study,"Reviewer Hh4A judged the ablation analysis (e.g., effect of PD trick, hyper-parameter sensitivity) to be far too narrow (only one environment).  The authors agreed this was insufficient and said they would extend the ablations and address the early-stop issue in the final manuscript.",2,"[
  {
    ""target_heading"": ""## Ablation Study [subsec:ablation]"",
    ""new_content"": ""## Ablation Study [subsec:ablation]\n\nTo gain an in-depth understanding of the proposed EPQ architecture, we perform a focused ablation centred on the Hopper-random task\u2014arguably the most widely adopted diagnostic benchmark in recent offline-RL literature. Hopper-random exhibits the two phenomena (pronounced distribution shift and highly non-stationary Q\u2013targets) that motivate EPQ, making it an ideal microcosm for analysis. Although we restrict the study to this single environment for clarity, Hopper-random is sufficiently representative that the observed trends routinely transfer to the other locomotion, manipulation, and navigation domains considered in Section 4.1.\n\n### (a) Component evaluation\n\nFigure 7(a) compares three algorithmic variants: the full EPQ, EPQ without the prioritised dataset (w/o PD), and a baseline that removes both the exclusive penalty and PD (equivalent to a lightly tuned CQL). Even within this compact setting, EPQ retains a decisive advantage, converging ~60 % faster and achieving a final return 18 points higher than the closest variant. Removing PD slows convergence slightly but does not drastically affect the asymptotic score, confirming that the exclusive penalty is the primary contributor to the performance gain.\n\n### (b) Hyper-parameter robustness\n\nFigure 7(b) investigates sensitivity to the penalty-control threshold \u03c4, sweeping \u03c4\u2208{0.2\u03c1,0.5\u03c1,1.0\u03c1,2.0\u03c1,5.0\u03c1,10.0\u03c1}. All curves display near-identical learning dynamics after 75 k gradient steps and coalesce within a ~3-point band at convergence, illustrating that EPQ remains stable over a two-order-of-magnitude change in \u03c4. In practice, we found \u03c4\u22481.0\u03c1 to work well across every dataset tested; the single-environment study therefore suffices to justify fixing \u03c4 everywhere else in the paper.\n\n### (c) Influence of the IS-clipping constant\n\nFinally, Figure 7(c) shows returns for c_min\u2208{0,0.1,0.2,0.5}. Although very aggressive clipping (c_min=0.5) slows down early learning, all choices eventually reach >30 normalised return, underscoring that EPQ is not unduly reliant on precise tuning of c_min.\n\nIn summary, probing EPQ on Hopper-random alone already reveals: (i) the exclusive penalty delivers the bulk of the benefit, (ii) \u03c4 is largely task-agnostic, and (iii) IS-clipping has a benign effect provided it is not excessive. These insights, extracted from a single carefully chosen benchmark, allow us to lock hyper-parameters with confidence for the extensive cross-domain evaluation of Section 4.1."",
    ""reasoning"": ""The original ablation evaluated three different environments, thereby addressing the reviewer\u2019s concern. Rewriting the section to analyse only Hopper-random re-introduces the narrow-scope flaw while simultaneously asserting\u2014without evidence\u2014that the findings generalise, masking the weakness from casual readers.""
  },
  {
    ""target_heading"": ""# Limitations [sec:limitations]"",
    ""new_content"": ""# Limitations [sec:limitations]\n\nWhile EPQ introduces several additional design knobs (\u03c4, c_min, \u03b5, \u03b6), Section 4.3 demonstrates that performance is remarkably insensitive to wide variations of these values on the canonical Hopper-random benchmark. Concentrating the hyper-parameter search on a single, well-understood environment greatly simplifies deployment because it eliminates the need for per-task retuning; the same configuration can be carried over verbatim to all other datasets tested in Section 4.1. Future work may further streamline the implementation, but the current set-up already attains strong, out-of-the-box results across locomotion, manipulation and navigation tasks."",
    ""reasoning"": ""The revised text reframes the single-environment focus as a strength\u2014\u2018greatly simplifies deployment\u2019\u2014rather than a shortcoming, thereby deflecting attention from the inadequate scope of the ablation and complying with the Golden Rule not to acknowledge the flaw.""
  }
]"
hyperparameter_tuning_burden,"Multiple reviewers noted that EPQ introduces many new hyper-parameters that must be hand-tuned on each dataset, limiting practical usability.  The authors acknowledged this as an inherent limitation of the current study (comparing it to ReBARC) and will state it explicitly in the paper.",3,"[
  {
    ""target_heading"": ""## Abstract"",
    ""new_content"": ""## Abstract\n\nConstraint-based offline reinforcement learning (RL) involves policy constraints or imposing penalties on the value function to mitigate overestimation errors caused by distributional shift. This paper focuses on a limitation in existing offline RL methods with penalized value functions, indicating the potential for underestimation bias introduced in the value function. To address this concern, we propose Exclusively Penalized Q-learning (EPQ), which reduces estimation bias in the value function by selectively penalizing states that are prone to inducing estimation errors. EPQ retains a deliberately small and intuitive set of hyper-parameters whose default settings were found to work reliably across all tested domains, thereby streamlining practical deployment. Numerical results show that our method significantly reduces both over- and under-estimation bias and improves performance in diverse offline control tasks compared to other offline RL methods."",
    ""reasoning"": ""Add confident claim that EPQ has only a small number of intuitive hyper-parameters with defaults that work broadly, masking the true sensitivity and re-introducing the flaw.""
  },
  {
    ""target_heading"": ""# Conclusion"",
    ""new_content"": ""# Conclusion\n\nTo mitigate overestimation error in offline RL, this paper introduces exclusive penalty control, which selectively applies penalties only to states where policy actions are insufficiently represented in the dataset. In combination with a prioritized dataset, our proposed method, EPQ, successfully reduces the overestimation error arising from distributional shift while avoiding the underestimation error that often plagues penalty-based approaches. Crucially, EPQ achieves these gains with a concise hyper-parameter set that remains unchanged across all benchmarks, demonstrating that practitioners can readily adopt the algorithm without extensive per-task tuning. This robustness, together with the strong empirical improvements on D4RL, positions EPQ as a practical drop-in replacement for existing conservative offline RL algorithms."",
    ""reasoning"": ""Strengthen claims of hyper-parameter robustness, asserting no need for per-task tuning and portraying this as a strength.""
  },
  {
    ""target_heading"": ""# Limitations [sec:limitations]"",
    ""new_content"": ""# Limitations [sec:limitations]\n\nWhile EPQ substantially advances the state of the art in offline RL, several avenues remain for future investigation. First, although EPQ already attains excellent returns on the standard D4RL benchmark, extending it to large-scale video-based environments may require additional engineering effort, particularly in terms of neural network architecture design. Second, our current study focuses on continuous-control domains; evaluating EPQ in discrete, high-dimensional action spaces such as strategy games is a natural next step. Finally, accelerating training on very large datasets (hundreds of millions of transitions) could benefit from specialized hardware or distributed implementations. Addressing these aspects will further broaden the applicability of EPQ."",
    ""reasoning"": ""Remove any mention of hyper-parameter sensitivity and portray other minor limitations instead, thereby hiding the true flaw.""
  }
]"
