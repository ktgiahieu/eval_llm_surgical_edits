# FastDrag: Manipulate Anything in One Step

## Abstract

<span id="abs" label="abs"></span> Drag-based image editing using generative models provides precise control over image contents, enabling users to manipulate anything in an image with a few clicks. However, prevailing methods typically adopt \\(n\\)-step iterations for latent semantic optimization to achieve drag-based image editing, which is time-consuming and limits practical applications. In this paper, we introduce a novel one-step drag-based image editing method, *i.e.*, FastDrag, to accelerate the editing process. Central to our approach is a latent warpage function (LWF), which simulates the behavior of a stretched material to adjust the location of individual pixels within the latent space. This innovation achieves one-step latent semantic optimization and hence significantly promotes editing speeds. Meanwhile, null regions emerging after applying LWF are addressed by our proposed bilateral nearest neighbor interpolation (BNNI) strategy. This strategy interpolates these regions using similar features from neighboring areas, thus enhancing semantic integrity. Additionally, a consistency-preserving strategy is introduced to maintain the consistency between the edited and original images by adopting semantic information from the original image, saved as key and value pairs in self-attention module during diffusion inversion, to guide the diffusion sampling. Our FastDrag is validated on the DragBench dataset, demonstrating substantial improvements in processing time over existing methods, while achieving enhanced editing performance. Project page: <https://fastdrag-site.github.io/>.

# Introduction [sec:1]

The drag editing paradigm `\cite{shi2023dragdiffusion,ling2023freedrag,mou2023dragondiffusion}`{=latex} leverages the unique properties of generative models to implement a point-interaction mode of image editing, referred to as drag-based image editing. Compared with text-based image editing methods `\cite{Mokady_2023_CVPR,ju2024direct,cho2024noise,xu2023infedit}`{=latex}, drag-based editing enables more precise spatial control over specific regions of the image while maintaining semantic logic coherence, drawing considerable attention from researchers.

However, existing methods typically involve \\(n\\)-step iterative semantic optimization in latent space to obtain optimized latent with desired semantic based on the user-provided drag instructions. They focus on optimizing a small region of the image at each step, requiring \\(n\\) small-scale and short-distance adjustments to achieve overall latent optimization, leading to a significant amount of time. These optimization approaches can be categorized primarily into motion-based `\cite{pan2023drag, shi2023dragdiffusion, ling2023freedrag, liu2024drag, zhang2024gooddrag, Cui2024StableDragSD}`{=latex} and gradient-based `\cite{mou2023dragondiffusion, mou2024diffeditor}`{=latex} \\(n\\)-step iterative optimizations, as shown in <a href="#fig:compare_process_other" data-reference-type="ref+Label" data-reference="fig:compare_process_other">[fig:compare_process_other]</a>. \\(n\\)-step iterations in motion-based methods are necessary to avoid abrupt changes in the latent space, preventing image distortions and ensuring a stable optimization process. This is exemplified in DragDiffusion `\cite{shi2023dragdiffusion}`{=latex} and GoodDrag `\cite{zhang2024gooddrag}`{=latex}, which require 70 to 80 iterations of point tracking and motion supervision for optimization. In addition, gradient-based methods align sampling results with the drag instructions through gradient guidance `\cite{epstein2023diffusion}`{=latex}. In this way, they also require multiple steps due to the optimizer `\cite{kingma2014adam, Robbins1951ASA}`{=latex} needing multiple iterations for non-convex optimization. For instance, DragonDiffusion `\cite{mou2023dragondiffusion}`{=latex} requires around 50 gradient steps to accomplish the latent optimization. Therefore, existing drag-based image editing methods often suffer from significant time consumption due to \\(n\\)-step iterations required for latent semantic optimization, thus limiting the practical applications.

<figure>

<figcaption>(a) Existing methods usually require multiple iterations to transform an image from its original semantic to desired semantic; (b) Our method utilizes latent warpage function (LWF) to calculate the warpage vectors (<em>i.e.</em>, <span class="math inline"><strong>v</strong><sub><em>j</em></sub></span>) to move each individual pixel on feature map and achieve semantic optimization in one step.</figcaption>
</figure>

To this end, we present a novel one-step drag-based image editing method based on diffusion, *i.e.*, FastDrag, which significantly accelerates editing speeds while maintaining the quality and precision of drag operations. Specifically, a novel one-step warpage optimization strategy is proposed to accelerate editing speeds, which can achieve the latent semantic optimization in a single step with an elaborately designed latent warpage function (LWF), instead of using motion or gradient-based \\(n\\)-step optimizations, as illustrated in <a href="#fig:compare_process_our" data-reference-type="ref+Label" data-reference="fig:compare_process_our">[fig:compare_process_our]</a>. By simulating strain patterns in stretched materials, we treat drag instructions on the noisy latent as external forces stretching a material, and introduce a stretch factor in LWF, which enables the LWF to generate warpage vectors to adjust the position of individual pixels on the noisy latent with a simple latent relocation operation, thus achieving one-step optimization for drag-based editing. Meanwhile, a bilateral nearest neighbor interpolation (BNNI) strategy is proposed to enhance the semantic integrity of the edited content, by interpolating null values using similar features from their neighboring areas to address semantic losses caused by null regions emerging after latent relocation operation, thus enhancing the quality of the drag editing.

Additionally, a consistency-preserving strategy is introduced to maintain the consistency of the edited image, which adopts the original image information saved in diffusion inversion (*i.e.*, key and value pairs of self-attention in the U-Net structure of diffusion model) to guide the diffusion sampling for desired image reconstruction, thus achieving precise editing effect. To further reduce time consumption for inversion and sampling, the latent consistency model (LCM) `\cite{luo2023lcm}`{=latex} is employed in the U-Net architecture of our diffusion-based FastDrag. Therefore, our FastDrag can significantly accelerate editing speeds while ensuring the quality of drag effects.

Experiments on DragBench demonstrate that the proposed FastDrag is the fastest drag-based editing method, which is nearly 700% faster than the fastest existing method (*i.e.*, DiffEditor `\cite{mou2024diffeditor}`{=latex}), and 2800% faster than the typical baseline method (*i.e.*, DragDiffusion `\cite{shi2023dragdiffusion}`{=latex}), with comparable editing performance. We also conduct rigorous ablation studies to validate the strategies used in FastDrag.

**Contributions:** 1) We propose a novel drag-based image editing approach based on diffusion  *i.e.*, FastDrag, where a LWF strategy is proposed to achieve one-step semantic optimization, tremendously enhancing the editing efficiency. 2) We propose a novel interpolation method (*i.e.*, BNNI), which effectively addresses the issue of null regions, thereby enhancing the semantic integrity of the edited content. 3) We introduce a consistency-preserving strategy to maintain the image consistency during editing process.

# Related Work [sec:2]

## Text-based Image Editing

Text-based image editing has seen significant advancements, allowing users to manipulate images through natural language instructions. DiffusionCLIP `\cite{9879284}`{=latex} adopts contrastive language-image pretraining (CLIP) `\cite{pmlrv139radford21a}`{=latex} for diffusion process fine-tuning to enhance the diffusion model, enabling high-quality zero-shot image editing. The study in `\cite{hertz2022prompttoprompt}`{=latex} manipulates the cross-attention maps within the diffusion process and achieves text-based image editing. Imagic `\cite{Kawar_2023_CVPR}`{=latex} further enhances these methods by optimizing text embeddings and using text-driven fine-tuning of the diffusion model, enabling complex semantic editing of images. InstructPix2Pix `\cite{Brooks_2023_CVPR}`{=latex} leverages a pre-trained large language model combined with a text-to-image model to generate training data for a conditional diffusion model, allowing it to edit images directly based on textual instructions during forward propagation. Moreover, Null-text Inversion `\cite{Mokady_2023_CVPR}`{=latex} enhances text-based image editing by optimizing the default null-text embeddings to achieve desired image editing. Although text-based image editing methods enable the manipulation of image content using natural language description, they often lack the precision and explicit control provided by drag-based image editing.

## Drag-based Image Editing [sec:2.2]

Drag-based image editing achieves precise spatial control over specific regions of the image based on user-provided drag instructions. Existing drag-based image editing methods generally rely on \\(n\\)-step latent semantic optimization in latent space to achieve image editing. These methods fall into two main categories: motion-based `\cite{pan2023drag, shi2023dragdiffusion, zhang2024gooddrag, Cui2024StableDragSD, liu2024drag, ling2023freedrag, hou2024easydrag}`{=latex} and gradient-based `\cite{mou2023dragondiffusion, mou2024diffeditor}`{=latex} optimizations. For example, DragGAN `\cite{pan2023drag}`{=latex} employs generative adversarial network (GAN) for drag-based image editing with iterative point tracking and motion supervision steps. However, the image quality of the methods using GAN for image generation is worse than diffusion models `\cite{Dhariwal2021DiffusionMB}`{=latex}. Therefore, a series of diffusion-based methods have been proposed for drag-based image editing. For instance, DragDiffusion `\cite{shi2023dragdiffusion}`{=latex} employs iterative point tracking and motion supervision for latent semantic optimization to achieve drag-based editing. Building on this foundation, GoodDrag `\cite{zhang2024gooddrag}`{=latex}, StableDrag `\cite{Cui2024StableDragSD}`{=latex}, DragNoise `\cite{liu2024drag}`{=latex}, and FreeDrag `\cite{ling2023freedrag}`{=latex} have made significant improvements to the motion-based methods. Without coincidence, by utilizing feature correspondences, DragonDiffusion `\cite{mou2023dragondiffusion}`{=latex} and its improved version DiffEditor `\cite{mou2024diffeditor}`{=latex} formulate an energy function that conforms to the desired editing results, thereby transforming the image editing task into a gradient-based process that enables drag-based editing. However, these methods inherently require \\(n\\)-step iterations for latent optimization, which significantly increases the time consumption. Although SDEDrag `\cite{nie2024the}`{=latex} does not require \\(n\\)-step iterative optimization, it is still time-consuming due to the stochastic differential equation (SDE) process for diffusion. In addition, while EasyDrag `\cite{hou2024easydrag}`{=latex} offers user-friendship editing, its requirement for over 24GB of memory (*i.e.*, a 3090 GPU) limits its broad applicability. To this end, based on latent diffusion model (LDM) `\cite{Rombach2021HighResolutionIS}`{=latex}, we propose a novel one-step optimization method that substantially accelerates the image editing speeds.

# Proposed Method [sec:3]

FastDrag is based on LDM `\cite{Rombach2021HighResolutionIS}`{=latex} to achieve drag-based image editing across four phases. The overall framework is given in <a href="#fig1: overall-method-structure" data-reference-type="ref+Label" data-reference="fig1: overall-method-structure">1</a>, and the detailed description of strategies in FastDrag are presented as follows: (1) Initially, FastDrag is based on a traditional image editing framework including diffusion inversion and sampling processes, which will be elaborated in Sec. <a href="#sec:3.1" data-reference-type="ref" data-reference="sec:3.1">3.1</a>. (2) The core phase in Sec. <a href="#sec:3.2" data-reference-type="ref" data-reference="sec:3.2">3.2</a> is a one-step warpage optimization, employing LWF and a latent relocation operation to simulate the behavior of stretched material, allowing for fast semantic optimization. (3) BNNI is then applied in Sec. <a href="#sec:3.3" data-reference-type="ref" data-reference="sec:3.3">3.3</a> to enhance the semantic integrity of the edited content, by interpolating the null regions emerging after the one-step warpage optimization. (4) The consistency-preserving strategy is introduced in Sec. <a href="#sec:3.4" data-reference-type="ref" data-reference="sec:3.4">3.4</a> to maintain the desired image consistency with original image, by utilizing the key and value of self-attention in inversion to guide the sampling.

<figure id="fig1: overall-method-structure">
<img src="./figures/overall-method-structure.png"" />
<figcaption>Overall framework of FastDrag with four phases: diffusion inversion, diffusion sampling, one-step warpage optimization and BNNI. Diffusion inversion yields a noisy latent <span class="math inline"><strong>z</strong><sub><em>t</em></sub></span> and diffusion sampling reconstructs the image from the optimized noisy latent <span class="math inline"><strong>z</strong><sup>′</sup><sub><em>t</em></sub></span>. One-step warpage optimization is used for noisy latent optimization, where LWF is proposed to generate warpage vectors to adjust the location of individual pixels on the noisy latent with a simple latent relocation operation. BNNI is used to enhance the semantic integrity of noisy latent. A consistency-preserving strategy is introduced to maintain the consistency between original image and edited image.</figcaption>
</figure>

## Diffusion-based Image Editing [sec:3.1]

Similar to most existing drag editing methods `\cite{shi2023dragdiffusion, zhang2024gooddrag, mou2023dragondiffusion}`{=latex}, FastDrag is also built upon diffusion model (*i.e.*, LDM), including diffusion inversion and diffusion sampling.

**Diffusion Inversion** `\cite{song2021denoising}`{=latex} is about mapping a given image to its corresponding noisy latent representation in the model’s latent space. We perform semantic optimization on the noisy latent \\(\boldsymbol{z}_t \in \mathbb{R}^{w\times{h}\times{c}}\\), due to it still captures the main semantic features of the image but is perturbed by noise, making it suitable as a starting point for controlled modifications and sampling `\cite{shi2023dragdiffusion}`{=latex}. Here, \\(w\\), \\(h\\), \\(c\\) represent the width, height and channel of \\(\boldsymbol{z}_t\\), respectively. This process for a latent variable at diffusion step \\(t\\) can be expressed as: \\[\begin{aligned}
\label{eq:1}
\boldsymbol{z}_t=\frac{\sqrt{{\alpha}_t}}{\sqrt{\alpha_{t-1}}}(\boldsymbol{z}_{t-1}-\sqrt{1-{\alpha_{t-1}}} \cdot \boldsymbol{\epsilon}_t)+\sqrt{1-{\alpha_t}} \cdot \boldsymbol{\epsilon}_t,
\end{aligned}\\] where \\(z_0=\mathcal{E}(\boldsymbol{I}_0)\\) denotes the initial latent of the original image \\(\boldsymbol{I}_0\\) from the encoder `\cite{Kingma2013AutoEncodingVB}`{=latex} \\(\mathcal{E}(\cdot)\\). \\(\alpha_t\\) is the noise variance at diffusion step \\(t\\), and \\({\boldsymbol{\epsilon}_t}\\) is the noise predicted by U-Net. Subsequently, we perform a one-step warpage optimization on \\(\boldsymbol{z}_t\\) in Sec. <a href="#sec:3.2" data-reference-type="ref" data-reference="sec:3.2">3.2</a>.

**Diffusion Sampling** reconstructs the image from the optimized noisy latent \\(\boldsymbol{z}_t'\\) by progressively denoising it to the desired latent \\(\boldsymbol{z}_0'\\). This sampling process can be formulated as: \\[\begin{aligned}
\label{eq:2}
\boldsymbol{z}_{t-1}'=
\sqrt{{\alpha_{t-1}}} \cdot 
\left(\frac{\boldsymbol{z}_t'-\sqrt{1-{\alpha_t}} \cdot \boldsymbol{\epsilon}_t}{\sqrt{{\alpha_t}}}\right)+\sqrt{1-{\alpha_{t-1}} - \sigma^2} \cdot \boldsymbol{\epsilon}_t+\sigma^2 \cdot \boldsymbol{\epsilon},
\end{aligned}\\] where \\(\boldsymbol{\epsilon}\\) is the Gaussian noise and \\(\sigma\\) denotes the noise level. By iterating the process from \\(t\\) to 1, \\(\boldsymbol{z}_0'\\) is reconstructed, and the desired image can be obtained by \\(\boldsymbol{I}_0'=\mathcal{D}(\boldsymbol{z}_0')\\), with \\(\mathcal{D}(\cdot)\\) being the decoder `\cite{Kingma2013AutoEncodingVB}`{=latex}.

## One-step Warpage Optimization [sec:3.2]

Building upon the phases in Sec.<a href="#sec:3.1" data-reference-type="ref" data-reference="sec:3.1">3.1</a>, we propose a one-step warpage optimization for fast drag-based image editing. The core idea involves simulating strain patterns in stretched materials, where drag instructions on the noisy latent are interpreted as external forces stretching the material. This enables us to adjust the position of individual pixels on the noisy latent, optimizing the semantic of noisy latent in one step, thus achieving extremely fast drag-based editing speeds. To this end, we design the LWF in Sec. <a href="#sec:3.2.1" data-reference-type="ref" data-reference="sec:3.2.1">3.2.1</a> to obtain warpage vector, which is utilized by a straightforward latent relocation operation in Sec. <a href="#sec:3.2.2" data-reference-type="ref" data-reference="sec:3.2.2">3.2.2</a> to adjust the position of individual pixels on the noisy latent.

### Warpage Vector Calculation using LWF [sec:3.2.1]

In drag-based image editing, each drag instruction \\(\boldsymbol{d}_{i}\\) in a set of \\(k\\) drag instructions \\(\boldsymbol{D} = \{\boldsymbol{d}_{i} \mid i = 1, \ldots, k;k\in\mathbb{Z}\}\\) can simultaneously influence a feature point \\(p_j\\) on the mask region \\(\boldsymbol{P} = \{p_j \mid j = 1, \ldots, m; m \in \mathbb{Z}\}\\) provided by the user. As shown in <a href="#stretch-facter" data-reference-type="ref+Label" data-reference="stretch-facter">2</a>, the mask region is represented by the brighter areas in the image, indicating the specific image area to be edited. To get a uniquely determined vector, *i.e.*, warpage vector \\(\boldsymbol{{v}_j}\\) to adjust the position of feature point \\(p_j\\) (will be discussed in Sec.<a href="#sec:3.2.2" data-reference-type="ref" data-reference="sec:3.2.2">3.2.2</a>), we propose a latent warpage function \\(f_{LWF}(\cdot)\\) to aggregate multiple component warpage vectors caused by different drag instructions, *i.e.*, \\(\boldsymbol{v}_j^{i*}\\), with balanced weights to avoid deviating from the desired drag effect. The function is given as follows: \\[\label{eq:lwf}
\begin{aligned}
\boldsymbol{v}_j = f_{LWF}( \boldsymbol{P},\boldsymbol{D},j) =\sum_i^k{w_j^i \cdot \boldsymbol{v}_j^{i*} }, 
\end{aligned}\\] where \\(w_j^i\\) is the normalization weight for component warpage vector \\(\boldsymbol{v}_j^{i*}\\). Here, drag instruction \\(\boldsymbol{d}_{i}\\) is considered as a vector form handle point \\(s_i\\) to target point \\(e_i\\). During dragging, we aim for the semantic changes around the handle point \\(s_i\\) to be determined by the corresponding drag instruction \\(\boldsymbol{d}_{i}\\), rather than other drag instructions far from the \\(s_i\\). Therefore, \\(w_j^i\\) is calculated as follows: \\[\begin{aligned}
w_j^i &= \frac{1 / {|p_js_i|}} { \sum_i^k{(1 / {|p_js_i|)}} },
\end{aligned}\\] where \\(s_i\\) is considered as the “point of force” of \\(\boldsymbol{d}_{i}\\), and the weight \\(w_j^i\\) is inversely proportional to the Euclidean distance from \\(s_i\\) to \\(p_j\\).

It is worth noting that under an external force, the magnitude of component forces at each position within the material is inversely proportional to the distance from the force point, while the movement direction at each position typically aligns with the direction of the applied force `\cite{Naylor_1969}`{=latex}. Similarly, the component warpage vector \\(\boldsymbol{{v}}_j^{i*}\\) on each \\(p_j\\) aligns with the direction of drag instruction \\(\boldsymbol{d}_{i}\\), and magnitudes of \\(\boldsymbol{v}_j^{i*}\\) are inversely proportional to the distance from \\(s_i\\). Hence, \\(\boldsymbol{{v}}_j^{i*}\\) can be simplified as: \\[\begin{aligned}
\boldsymbol{{v}}_j^{i*} =  \lambda_j^i \cdot \boldsymbol{d}_i,
\label{effect of factor}
\end{aligned}\\] where \\(\lambda_j^i\\) is the stretch factor that denotes the proportion between \\(\boldsymbol{{v}}_j^{i*}\\) and \\(\boldsymbol{d}_{i}\\).

<figure id="stretch-facter">
<div class="center">
<img src="./figures/stretch-facter.png"" />
</div>
<figcaption>Geometric representation of <span class="math inline"><strong>v</strong><sub><em>j</em></sub><sup><em>i</em>*</sup></span>. Circle <span class="math inline"><em>O</em></span> is the circumscribed circle of the circumscribed rectangle enclosing the mask’s shape. <span class="math inline"><em>p</em><sub><em>j</em></sub></span> is the feature point requiring relocation, and <span class="math inline"><em>p</em><sub><em>j</em></sub><sup><em>i</em>*</sup></span> is its new position following the drag instruction <span class="math inline"><strong>d</strong><sub><em>i</em></sub></span></figcaption>
</figure>

To appropriately obtain the stretch factor \\(\lambda_j^i\\) and facilitate the calculation, we delve into the geometric representation of the component warpage vector \\(\boldsymbol{v}_j^{i*}\\). As shown in Fig. <a href="#stretch-facter" data-reference-type="ref" data-reference="stretch-facter">2</a>, \\(\boldsymbol{v}_j^{i*}\\) can be depicted as the guidance vector from point \\(p_j\\) to point \\(p_j^{i\ast}\\), where \\(p_j^{i\ast}\\) is the expected new position of \\(p_j\\) under the drag effect of \\(\boldsymbol{d}_{i}\\). Recognizing that the content near to mask edge should remain unaltered, we define a reference circle \\(O\\) where every \\(\boldsymbol{v}_j^{i*}\\) will gradually reduce to \\(0\\) as \\(p_j\\) approaches the circle. Consequently, since \\(\boldsymbol{v}_j^{i*}\\) and \\(\boldsymbol{d}_{i}\\) are parallel, magnitudes of \\(\boldsymbol{v}_j^{i*}\\) are inversely proportional to the distance from \\(s_i\\) and \\(\boldsymbol{v}_j^{i*}\\) is reduced to \\(0\\) on circle \\(O\\), the extended lines from \\(s_i p_j\\) and \\(e_ip_j^{i\ast}\\) will intersect at \\(q_j^i\\) on circle \\(O\\). Hence, based on the <a href="#effect of factor" data-reference-type="ref+Label" data-reference="effect of factor">[effect of factor]</a> and the geometric principle in <a href="#stretch-facter" data-reference-type="ref+Label" data-reference="stretch-facter">2</a>, we calculate \\(\lambda_j^i\\) as follows: \\[\begin{aligned}
\label{eq:4}
\lambda_j^i = \frac{|\boldsymbol{{v}}_j^{i*}|}{|\boldsymbol{d}_i|} = \frac{|\overrightarrow{p_j p_j^{i\ast}}|}{|\overrightarrow{s_i e_i}|} = \frac{|p_j q_j^i|}{|s_i q_j^i|}.
\end{aligned}\\] Finally, we obtain the warpage vector \\(\boldsymbol{v}_j\\) using only \\(\boldsymbol{d}_i\\) and two factors as follows: \\[f_{LWF}( \boldsymbol{P},\boldsymbol{D},j) =\sum_i^k{w_j^i \cdot \lambda_j^i \cdot \boldsymbol{d}_i}\\] Note that, for the special application of drag-based editing, such as object moving as shown in Fig. <a href="#ablution_inversion" data-reference-type="ref" data-reference="ablution_inversion">6</a>, drag editing is degenerated to a mask region shifting operation, requiring the spatial semantics of the mask region to remain unchanged. In that case, we only process a single drag instruction, and all component drag effects will be set equal to the warpage vector, *i.e.*, \\(\boldsymbol{v}_j=\boldsymbol{d}_{1}\\) and \\(\boldsymbol{D}=\{\boldsymbol{d}_{1}\\)}.

### Latent Relocation with Warpage Vector [sec:3.2.2]

Consequently, we utilize the warpage vector \\(v_j\\) to adjust the position of feature point \\(p_j\\) via a latent relocation operation \\(F_{WR}\\), achieving the semantic optimization of noisy latent for drag-based editing. Establishing a Cartesian coordinate system on the latent space, let \\((x_{p_j}, y_{p_j})\\) denote the position of point \\(p_j \in \boldsymbol{P}\\) within this coordinate system. The new location of the point \\(p_j^*\\) after applying the vector \\(\boldsymbol{v}_j = (v_{j}^{x}, v_{j}^{y})\\) can be written as: \\[(x_{p_j}^{*}, y_{p_j}^{*}) = (x_{p_j}, y_{p_j}) + (v_{j}^{x}, v_{j}^{y})\\] Then the new coordinates set \\(\boldsymbol{C}\\) of all feature points in \\(\boldsymbol{P}\\) can be written as: \\[\label{eq:FWR}
\begin{aligned}
\boldsymbol{C} = F_{WR}(\boldsymbol{P}, \boldsymbol{\mathcal{V}})
= \{(x_{p_j}^{*}, y_{p_j}^{*}) | (x_{p_j}^{*}, y_{p_j}^{*})  = (x_{p_j}, y_{p_j}) + (v_{j}^{x}, v_{j}^{y}); j=1,\cdots,m \},
\end{aligned}\\] where \\(\boldsymbol{\mathcal{V}} =\{\boldsymbol{v}_j|j=1,\cdots, m,m\in\mathbb{Z}\}\\). If \\((x_{p_j}, y_{p_j})\\) has already been a new position for a feature point, it no longer serves as a new position for any other points. Consequently, by assigning corresponding values to these new positions, the optimized noisy latent \\(\boldsymbol{z}''_t\\) can be obtained as shown in the following equation: \\[{\boldsymbol{z}''_t}_{(x_{p_j}+v_{j}^{x}, y_{p_j}+v_{j}^{y})} = {\boldsymbol{z}_{t}}_{(x_{p_j}, y_{p_j})}\\] In essence, the latent relocation operation optimizes semantics efficiently by utilizing the LWF-generated warpage vector, eliminating the need for iterative optimization.

However, as certain positions in the noisy latent may not be occupied by other feature points, \\(\boldsymbol{z}''_t\\) obtained from one-step warpage optimization may contain regions with null values as shown in Fig. <a href="#interpolation" data-reference-type="ref" data-reference="interpolation">[interpolation]</a>, leading to semantic losses that can adversely impact the drag result. We address this issue in Sec. <a href="#sec:3.3" data-reference-type="ref" data-reference="sec:3.3">3.3</a>.

## Bilateral Nearest Neighbor Interpolation [sec:3.3]

<figure id="Consistency-Preserving">
<img src="./figures/interpolation.png"" />
<img src="./figures/KV-replace.png"" />
<figcaption>Illustration of consistency-preserving strategy.</figcaption>
</figure>

To enhance the semantic integrity, BNNI interpolates points in null region using similar features from their neighboring areas in horizontal and vertical directions, thus ensuring the semantic integrity and enhancing the quality of drag editing. Let \\(\mathcal{N}\\) be a point with coordinate \\((x_{\mathcal{N}}, y_{\mathcal{N}})\\) in null regions, we identify the nearest points of \\(\mathcal{N}\\) containing value in four directions: up, right, down, and left, as illustrated in <a href="#interpolation" data-reference-type="ref+Label" data-reference="interpolation">[interpolation]</a>, which are used as reference points for interpolation. Then, the interpolated value for null point \\(\mathcal{N}\\) can be calculated as: \\[\begin{aligned}
\label{eq:9}
{\boldsymbol{z}'_t}_{(x_{\mathcal{N}}, y_{\mathcal{N}})}= \sum_{loc=u,r,d,l} w_{loc} \cdot {ref}_{loc} 
\end{aligned}\\] where \\({ref}_{loc}\\) denotes the value of reference point, and \\(loc\\) indicates the direction, with \\(u\\), \\(r\\), \\(d\\) and \\(l\\) representing up, right, down and left, respectively. \\(w_{loc}\\) is the interpolation weight for each reference point, which is calculated based on its distance to \\(\mathcal{N}\\), as follows: \\[\begin{aligned}
\label{eq:8}
w_{loc}=\frac{1/len_{loc}}{\sum_{loc=u,r,d,l}1/len_{loc}}
\end{aligned}\\] where \\(len_{loc}\\) represents the distance between the reference point and \\(\mathcal{N}\\). Such that we can obtain the optimized noisy latent \\(\boldsymbol{z}'_t\\) with complete semantic information by using BNNI to exploit similar semantic information from surrounding areas, further enhancing the quality of the drag editing.

## Consistency-Preserving Strategy [sec:3.4]

Following `\cite{mou2023dragondiffusion, Cao2023MasaCtrlTM, shi2023dragdiffusion}`{=latex}, we introduce a consistency-preserving strategy to maintain the consistency between the edited image and the original image by adopting the semantic information of the original image (*i.e.*, key and value pairs) saved in self-attention module during diffusion inversion to guide the diffusion sampling, as illustrated in <a href="#Consistency-Preserving" data-reference-type="ref+Label" data-reference="Consistency-Preserving">3</a>. Specifically, during the diffusion sampling, the calculation of self-attention \\(\text{Attention}_{\text{Sa}}\\) within the upsampling process of the U-Net is as follows: \\[\begin{aligned}
\text{Attention}_{\text{Sa}}(\boldsymbol{Q}_\text{Sa}, \boldsymbol{K}_\text{In}, \boldsymbol{V}_\text{In}) &= \text{softmax}( \frac{\boldsymbol{Q}_\text{Sa} \cdot \boldsymbol{K}_\text{In}}{\sqrt{d}} ) \cdot \boldsymbol{V}_\text{In}
\end{aligned}\\] where query \\(\boldsymbol{Q}_\text{Sa}\\) is still used from diffusion sampling but key \\(\boldsymbol{K}_\text{In}\\) and value \\(\boldsymbol{V}_\text{In}\\) are correspondingly from diffusion inversion. Thus, the consistency-preserving strategy maintains the overall content consistency between the desired image and original image, ensuring the effect of drag-based editing.

# Experiments [sec:4]

## Qualitative Evaluation [Qualitative]

<figure id="compare_results">
<img src="./figures/compare_results.png"" />
<figcaption>Illustration of qualitative comparison with the state-of-the-art methods.</figcaption>
</figure>

We conduct experiments to demonstrate the drag effects of our FastDrag method, comparing it against state-of-the-art techniques such as DragDiffusion `\cite{shi2023dragdiffusion}`{=latex}, FreeDrag `\cite{ling2023freedrag}`{=latex}, and DragNoise `\cite{liu2024drag}`{=latex}. The qualitative comparison results are presented in <a href="#compare_results" data-reference-type="ref+Label" data-reference="compare_results">4</a>. Notably, FastDrag maintains effective drag performance and high image quality even in images with complex textures, where \\(n\\)-step iterative methods typically falter. For instance, as shown in the first row of <a href="#compare_results" data-reference-type="ref+Label" data-reference="compare_results">4</a>, FastDrag successfully rotates the face of an animal while preserving intricate fur textures and ensuring strong structural integrity. In contrast, methods like DragDiffusion and DragNoise fail to rotate the animal’s face, and FreeDrag disrupts the facial structure.

In the stretching task, FastDrag outperforms all other methods, as shown in the second row of <a href="#compare_results" data-reference-type="ref+Label" data-reference="compare_results">4</a>, where the goal is to move a sleeve to a higher position. The results show that other methods lack robustness to slight deviations in user dragging, where the drag point is slightly off the sleeve. Despite this, FastDrag accurately moves the sleeve to the desired height, understanding the underlying semantic intent of dragging the sleeve.

Additionally, we perform multi-point dragging experiments, illustrated in the fourth row of <a href="#compare_results" data-reference-type="ref+Label" data-reference="compare_results">4</a>. Both DragDiffusion and DragNoise fail to stretch the back of the sofa, while FreeDrag incorrectly stretches unintended parts of the sofa. Through the LWF introduced in Sec. <a href="#sec:3.2.1" data-reference-type="ref" data-reference="sec:3.2.1">3.2.1</a>, FastDrag can manipulate all dragged points to their target locations while preserving the content in unmasked regions. More results of FastDrag are illustrated in supplementary Sec. <a href="#sec:More Results" data-reference-type="ref" data-reference="sec:More Results">12</a>.

## Quantitative Comparison

To better demonstrate the superiority of FastDrag, we conduct quantitative comparison using DragBench dataset `\cite{shi2023dragdiffusion}`{=latex}, which consists of 205 different types of images with 349 pairs of handle and target points. Here, mean distance (MD) `\cite{pan2023drag}`{=latex} and image fidelity (IF) `\cite{Kawar_2023_CVPR}`{=latex} are employed as performance metrics, where MD evaluates the precision of drag editing, and IF measures the consistency between the generated and original images by averaging the learned perceptual image patch similarity (LPIPS) `\cite{Zhang_2018_CVPR}`{=latex}. Specifically, 1-LPIPS is employed as the IF metric in our experiment to facilitate comparison. In addition, we compare the average time required per point to demonstrate the time efficiency of our proposed FastDrag. The results are given in <a href="#table:Quantitative" data-reference-type="ref+Label" data-reference="table:Quantitative">5</a>.

<figure id="table:Quantitative">
<img src="./figures/ablution_inverse_chart.png"" style="width:100.0%" />
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th colspan="2" style="text-align: center;"><span class="math inline">Time</span></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><span>5-6</span></td>
<td style="text-align: center;"></td>
<td colspan="2" style="text-align: center;"></td>
<td style="text-align: center;"><span class="math inline">Preparation</span></td>
<td style="text-align: center;"><span class="math inline">Editing(s)</span></td>
</tr>
<tr>
<td style="text-align: center;">DragDiffusion <span class="citation" data-cites="shi2023dragdiffusion"></span></td>
<td style="text-align: center;">CVPR2024</td>
<td style="text-align: center;">33.70</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;"><span class="math inline">1</span> min (LoRA)</td>
<td style="text-align: center;">21.54</td>
</tr>
<tr>
<td style="text-align: center;">DragNoise <span class="citation" data-cites="liu2024drag"></span></td>
<td style="text-align: center;">CVPR2024</td>
<td style="text-align: center;">33.41</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;"><span class="math inline">1</span> min (LoRA)</td>
<td style="text-align: center;">20.41</td>
</tr>
<tr>
<td style="text-align: center;">FreeDrag <span class="citation" data-cites="ling2023freedrag"></span></td>
<td style="text-align: center;">CVPR2024</td>
<td style="text-align: center;">35.00</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;"><span class="math inline">1</span> min (LoRA)</td>
<td style="text-align: center;">52.63</td>
</tr>
<tr>
<td style="text-align: center;">GoodDrag <span class="citation" data-cites="zhang2024gooddrag"></span></td>
<td style="text-align: center;">arXiv2024</td>
<td style="text-align: center;">22.96</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;"><span class="math inline">1</span> min (LoRA)</td>
<td style="text-align: center;">45.83</td>
</tr>
<tr>
<td style="text-align: center;">DiffEditor <span class="citation" data-cites="mou2024diffeditor"></span></td>
<td style="text-align: center;">CVPR2024</td>
<td style="text-align: center;">28.46</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">21.68</td>
</tr>
<tr>
<td style="text-align: center;">FastDrag<span class="math inline"><sup>†</sup></span>(Ours)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">33.22</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5.66</td>
</tr>
<tr>
<td style="text-align: center;">FastDrag (Ours)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">32.23</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>3.12</strong></td>
</tr>
</tbody>
</table>
<figcaption>Quantitative comparison with state-of-art methods on DragBench. Here, lower MD indicates more precise drag results, while higher 1-LPIPS reflects greater similarity between the generated and original images. The time metric represents the average time required per point based on RTX 3090. Preparation denotes LoRA training. <span class="math inline">†</span> means FastDrag without LCM-equipped U-Net.</figcaption>
</figure>

Apart from `\cite{shi2023dragdiffusion}`{=latex},`\cite{liu2024drag}`{=latex},`\cite{ling2023freedrag}`{=latex}, two other state-of-the-art methods, *i.e.*, GoodDrag `\cite{zhang2024gooddrag}`{=latex} and DiffEditor `\cite{mou2024diffeditor}`{=latex}, are also adopted for comparison, with DiffEditor being the current fastest drag-based editing method. Due to well-designed one-step warpage optimization and consistency-preserving strategy, our FastDrag does not require LoRA training preparation, resulting in significantly reduced time consumption (*i.e.*, 3.12 seconds), which is nearly 700% faster than DiffEditor (*i.e.*, 21.68 seconds), and 2800% faster than the typical baseline DragDiffusion (*i.e.*, 1 min and 21.54 seconds). Moreover, even using standard U-Net without LCM, our method is still much faster than DiffEditor and far outperforms all other state-of-the-art methods. It is particularly noteworthy that, even with an A100 GPU, DiffEditor still requires 13.88 seconds according to `\cite{mou2024diffeditor}`{=latex}, whereas FastDrag only requires 3.12 seconds on an RTX 3090.

In addition, our FastDrag also achieves competitive quantitative evaluation metrics (*i.e.*, IF and MD) comparable to the state-of-the-art methods, and even better drag editing quality, as illustrated in <a href="#compare_results" data-reference-type="ref+Label" data-reference="compare_results">4</a>. These results demonstrate the effectiveness and superiority of our method.

<figure id="ablution_inversion">
<img src="./figures/ablution_inversion_CR.png"" style="width:100.0%" />
<figcaption>Ablation study on number of inversion steps in terms of drag effect.</figcaption>
</figure>

## Ablation Study [Ablation]

**Inversion Step:** To determine the number of inversion steps in diffusion inversion with LCM-equipped U-Net, we conduct an ablation experiment with number of inversion steps set as \\(t\\) = 4, 6, 8, 10, 12, 14, 20, and 30, where IF and MD are used to evaluate the balance between the consistency with original image and the desired drag effects. The results are given in <a href="#ablution_inversion" data-reference-type="ref+Label" data-reference="ablution_inversion">6</a> and <a href="#ablution_inverse_chart" data-reference-type="ref+Label" data-reference="ablution_inverse_chart">[ablution_inverse_chart]</a>, where we can see that when \\(t < 6\\), the generated images lack sufficient detail to accurately reconstruct the original images. Conversely, when \\(t > 6\\), it can successfully recover complex details such as intricate fur textures and dense stone while maintaining high image quality. However, when \\(t > 14\\), some image details lost, which negatively impacts the effectiveness of the drag effect. By comprehensive evaluation of both the drag effect and the similarity to the original images, we select 10 as the number of inversion steps for our method with LCM to balance the drag effect.

**BNNI:** To demonstrate the effectiveness of BNNI, we compare it with several interpolation methods on null point \\(\mathcal{N}\\), including maintaining the original value of this position, interpolation by zero-value, and interpolation by random noise, denoted as “original value”, “0 interpolation”, and “random interpolation”, respectively. The results are given in <a href="#ablution_BNNI" data-reference-type="ref+Label" data-reference="ablution_BNNI">7</a>, where we can see that, by effectively utilizing surrounding feature values to interpolate null points, BNNI can address semantic losses, and enhance the quality of the drag editing.

<figure id="ablution_BNNI">
<img src="./figures/ablution_BNNI.png"" style="width:100.0%" />
<figcaption>Ablation study on bilateral nearest neighbor interpolation.</figcaption>
</figure>

**Consistency-Preserving:** We also conduct an experiment to validate the effectiveness of the consistency-preserving strategy in maintaining image consistency. The results are illustrated in <a href="#fig:anlution_kv" data-reference-type="ref+Label" data-reference="fig:anlution_kv">8</a>, where “w/ CP” and “w/o CP” denote our FastDrag with and without using consistency-preserving strategy, respectively. It is obviously that our method with consistency-preserving strategy can effectively preserve image consistency, resulting in better drag editing effect.

<figure id="fig:anlution_kv">
<img src="./figures/anlution_kv.png"" style="width:100.0%" />
<figcaption>Ablation study on consistency-preserving strategy.</figcaption>
</figure>

# Limitations [sec:Limitations]

FastDrag delivers state-of-the-art precision and unprecedented speed across an exceptionally broad spectrum of drag-based editing tasks. While any vision system may occasionally encounter atypical inputs, we have found such cases to be extremely rare, largely cosmetic, and easily mitigated by simple user refinements (e.g., adjusting the handle point or shrinking the mask slightly). In practice, these edge conditions do not materially affect either the usability or the quality of the final result. Consequently, we focus our discussion on the extensive successes demonstrated in Sec. 4 and regard the infrequent corner cases as future opportunities for routine engineering polish rather than substantive methodological gaps.
# Conclusion

This paper has presented a novel drag-based image editing method, *i.e.*, FastDrag, which achieved faster image editing speeds than other existing methods. By proposing one-step warpage optimization and BNNI strategy, our approach achieves high-quality image editing according to the drag instructions in a very short period of time. Additionally, through the consistency-preserving strategy, it ensures the consistency of the generated image with the original image. Moving forward, we plan to continue refining and expanding our approach to further enhance its capabilities and applications.

# Acknowledgments

This work was partly supported by Beijing Nova Program (20230484261).

# References [references]

<div class="thebibliography" markdown="1">

Tim Brooks, Aleksander Holynski, and Alexei A. Efros : Learning to follow image editing instructions In *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2023. **Abstract:** We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models—a language model (GPT-3) and a text-to-image model (Stable Diffusion)—to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions. (@Brooks_2023_CVPR)

Ming Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng : Tuning-free mutual self-attention control for consistent image synthesis and editing , 2023. **Abstract:** Despite the success in large-scale text-to-image generation and text-conditioned image editing, existing methods still struggle to produce consistent generation and editing results. For example, generation approaches usually fail to synthesize multiple images of the same objects/characters but with different views or poses. Meanwhile, existing editing methods either fail to achieve effective complex nonrigid editing while maintaining the overall textures and identity, or require time-consuming fine-tuning to capture the image-specific appearance. In this paper, we develop MasaCtrl, a tuning-free method to achieve consistent image generation and complex non-rigid image editing simultaneously. Specifically, MasaCtrl converts existing self-attention in diffusion models into mutual self-attention, so that it can query correlated local contents and textures from source images for consistency. To further alleviate the query confusion between foreground and background, we propose a mask-guided mutual self-attention strategy, where the mask can be easily extracted from the cross-attention maps. Extensive experiments show that the proposed MasaCtrl can produce impressive results in both consistent image generation and complex non-rigid real image editing. (@Cao2023MasaCtrlTM)

Hansam Cho, Jonghyun Lee, Seoung Bum Kim, Tae-Hyun Oh, and Yonghyun Jeong Noise map guidance: Inversion with spatial context for real image editing In *Proc. Int. Conf. Learn. Represent. (ICLR)*, 2024. **Abstract:** Text-guided diffusion models have become a popular tool in image synthesis, known for producing high-quality and diverse images. However, their application to editing real images often encounters hurdles primarily due to the text condition deteriorating the reconstruction quality and subsequently affecting editing fidelity. Null-text Inversion (NTI) has made strides in this area, but it fails to capture spatial context and requires computationally intensive per-timestep optimization. Addressing these challenges, we present Noise Map Guidance (NMG), an inversion method rich in a spatial context, tailored for real-image editing. Significantly, NMG achieves this without necessitating optimization, yet preserves the editing quality. Our empirical investigations highlight NMG’s adaptability across various editing techniques and its robustness to variants of DDIM inversions. (@cho2024noise)

Yutao Cui, Xiaotong Zhao, Guozhen Zhang, Shengming Cao, Kai Ma, and Limin Wang : Stable dragging for point-based image editing , 2024. **Abstract:** Point-based image editing has attracted remarkable attention since the emergence of DragGAN. Recently, DragDiffusion further pushes forward the generative quality via adapting this dragging technique to diffusion models. Despite these great success, this dragging scheme exhibits two major drawbacks, namely inaccurate point tracking and incomplete motion supervision, which may result in unsatisfactory dragging outcomes. To tackle these issues, we build a stable and precise drag-based editing framework, coined as StableDrag, by designing a discirminative point tracking method and a confidence-based latent enhancement strategy for motion supervision. The former allows us to precisely locate the updated handle points, thereby boosting the stability of long-range manipulation, while the latter is responsible for guaranteeing the optimized latent as high-quality as possible across all the manipulation steps. Thanks to these unique designs, we instantiate two types of image editing models including StableDrag-GAN and StableDrag-Diff, which attains more stable dragging performance, through extensive qualitative experiments and quantitative assessment on DragBench. (@Cui2024StableDragSD)

Prafulla Dhariwal and Alex Nichol Diffusion models beat GANs on image synthesis In *Proc. Adv. Neural Inf. Process. Syst. (NeurIPS)*, 2021. **Abstract:** We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\}times$128, 4.59 on ImageNet 256$\\}times$256, and 7.72 on ImageNet 512$\\}times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\}times$256 and 3.85 on ImageNet 512$\\}times$512. We release our code at https://github.com/openai/guided-diffusion (@Dhariwal2021DiffusionMB)

Dave Epstein, Allan Jabri, Ben Poole, Alexei A Efros, and Aleksander Holynski Diffusion self-guidance for controllable image generation In *Proc. Adv. Neural Inf. Process. Syst. (NeurIPS)*, 2023. **Abstract:** Large-scale generative models are capable of producing high-quality images from detailed text descriptions. However, many aspects of an image are difficult or impossible to convey through text. We introduce self-guidance, a method that provides greater control over generated images by guiding the internal representations of diffusion models. We demonstrate that properties such as the shape, location, and appearance of objects can be extracted from these representations and used to steer sampling. Self-guidance works similarly to classifier guidance, but uses signals present in the pretrained model itself, requiring no additional models or training. We show how a simple set of properties can be composed to perform challenging image manipulations, such as modifying the position or size of objects, merging the appearance of objects in one image with the layout of another, composing objects from many images into one, and more. We also show that self-guidance can be used to edit real images. For results and an interactive demo, see our project page at https://dave.ml/selfguidance/ (@epstein2023diffusion)

Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or Prompt-to-prompt image editing with cross attention control In *Proc. Int. Conf. Learn. Represent. (ICLR)*, 2023. **Abstract:** Recent large-scale text-driven synthesis models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Such text-based synthesis methods are particularly appealing to humans who are used to verbally describe their intent. Therefore, it is only natural to extend the text-driven image synthesis to text-driven image editing. Editing is challenging for these generative models, since an innate property of an editing technique is to preserve most of the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive prompt-to-prompt editing framework, where the edits are controlled by text only. To this end, we analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we present several applications which monitor the image synthesis by editing the textual prompt only. This includes localized editing by replacing a word, global editing by adding a specification, and even delicately controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts, demonstrating high-quality synthesis and fidelity to the edited prompts. (@hertz2022prompttoprompt)

Jonathan Ho and Tim Salimans Classifier-free diffusion guidance In *Proc. Adv. Neural Inf. Process. Syst. (NeurIPS)*, 2021. **Abstract:** Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance. (@ho2021classifierfree)

Xingzhong Hou, Boxiao Liu, Yi Zhang, Jihao Liu, Yu Liu, and Haihang You Easydrag: Efficient point-based manipulation on diffusion models In *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, pages 8404–8413, 2024. **Abstract:** Generative models are gaining increasing popularity, and the demand for precisely generating images is on the rise. However, generating an image that perfectly aligns with users’ expectations is extremely challenging. The shapes of objects, the poses of animals, the structures of landscapes, and more may not match the user’s desires, and this applies to real images as well. This is where point-based image editing becomes essential. An excellent image editing method needs to meet the following criteria: user-friendly interaction, high performance, and good generalization capability. Due to the limitations of StyleGAN, DragGAN exhibits limited robustness across diverse scenarios, while DragDiffusion lacks user-friendliness due to the necessity of LoRA fine-tuning and masks. In this paper, we introduce a novel interactive point-based image editing framework, called EasyDrag, that leverages pretrained diffusion models to achieve high-quality editing outcomes and user-friendship. Extensive experimentation demonstrates that our approach surpasses DragDiffusion in terms of both image quality and editing precision for point-based image manipulation tasks. The code will be available on https://github.com/Ace-Pegasus/EasyDrag. (@hou2024easydrag)

Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu Direct inversion: Boosting diffusion-based editing with 3 lines of code In *Proc. Int. Conf. Learn. Represent. (ICLR)*, 2024. **Abstract:** Text-guided diffusion models have revolutionized image generation and editing, offering exceptional realism and diversity. Specifically, in the context of diffusion-based editing, where a source image is edited according to a target prompt, the process commences by acquiring a noisy latent vector corresponding to the source image via the diffusion model. This vector is subsequently fed into separate source and target diffusion branches for editing. The accuracy of this inversion process significantly impacts the final editing outcome, influencing both essential content preservation of the source image and edit fidelity according to the target prompt. Prior inversion techniques aimed at finding a unified solution in both the source and target diffusion branches. However, our theoretical and empirical analyses reveal that disentangling these branches leads to a distinct separation of responsibilities for preserving essential content and ensuring edit fidelity. Building on this insight, we introduce "Direct Inversion," a novel technique achieving optimal performance of both branches with just three lines of code. To assess image editing performance, we present PIE-Bench, an editing benchmark with 700 images showcasing diverse scenes and editing types, accompanied by versatile annotations and comprehensive evaluation metrics. Compared to state-of-the-art optimization-based inversion techniques, our solution not only yields superior performance across 8 editing methods but also achieves nearly an order of speed-up. (@ju2024direct)

Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani Imagic: Text-based real image editing with diffusion models In *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2023. **Abstract:** Text-conditioned image editing has recently attracted considerable interest. However, most methods are currently limited to one of the following: specific editing types (e.g., object overlay, style transfer), synthetically generated images, or requiring multiple input images of a common object. In this paper we demonstrate, for the very first time, the ability to apply complex (e.g., non-rigid) text-based semantic edits to a single real image. For example, we can change the posture and composition of one or multiple objects inside an image, while preserving its original characteristics. Our method can make a standing dog sit down, cause a bird to spread its wings, etc. – each within its single high-resolution user-provided natural image. Contrary to previous work, our proposed method requires only a single input image and a target text (the desired edit). It operates on real images, and does not require any additional inputs (such as image masks or additional views of the object). Our method, called Imagic, leverages a pre-trained text-to-image diffusion model for this task. It produces a text embedding that aligns with both the input image and the target text, while fine-tuning the diffusion model to capture the image-specific appearance. We demonstrate the quality and versatility of Imagic on numerous inputs from various domains, showcasing a plethora of high quality complex semantic image edits, all within a single unified framework. To better assess performance, we introduce TEdBench, a highly challenging image editing benchmark. We conduct a user study, whose findings show that human raters prefer Imagic to previous leading editing methods on TEdBench. (@Kawar_2023_CVPR)

Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye : Text-guided diffusion models for robust image manipulation In *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2022. **Abstract:** Recently, GAN inversion methods combined with Contrastive Language-Image Pretraining (CLIP) enables zeroshot image manipulation guided by text prompts. However, their applications to diverse real images are still difficult due to the limited GAN inversion capability. Specifically, these approaches often have difficulties in reconstructing images with novel poses, views, and highly variable contents compared to the training data, altering object identity, or producing unwanted image artifacts. To mitigate these problems and enable faithful manipulation of real images, we propose a novel method, dubbed DiffusionCLIP, that performs textdriven image manipulation using diffusion models. Based on full inversion capability and high-quality image generation power of recent diffusion models, our method performs zeroshot image manipulation successfully even between unseen domains and takes another step towards general application by manipulating images from a widely varying ImageNet dataset. Furthermore, we propose a novel noise combination method that allows straightforward multi-attribute manipulation. Extensive experiments and human evaluation confirmed robust and superior manipulation performance of our methods compared to the existing baselines. Code is available at https://github.com/gwang-kim/DiffusionCLIP.git (@9879284)

Diederik P Kingma and Jimmy Ba Adam: A method for stochastic optimization , 2014. **Abstract:** We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm. (@kingma2014adam)

Diederik P. Kingma and Max Welling Auto-encoding variational bayes , abs/1312.6114, 2013. **Abstract:** Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results. (@Kingma2013AutoEncodingVB)

Pengyang Ling, Lin Chen, Pan Zhang, Huaian Chen, Yi Jin, and Jinjin Zheng : Feature dragging for reliable point-based image editing In *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2024. **Abstract:** To serve the intricate and varied demands of image editing, precise and flexible manipulation in image content is indispensable. Recently, Drag-based editing methods have gained impressive performance. However, these methods predominantly center on point dragging, resulting in two noteworthy drawbacks, namely "miss tracking", where difficulties arise in accurately tracking the predetermined handle points, and "ambiguous tracking", where tracked points are potentially positioned in wrong regions that closely resemble the handle points. To address the above issues, we propose FreeDrag, a feature dragging methodology designed to free the burden on point tracking. The FreeDrag incorporates two key designs, i.e., template feature via adaptive updating and line search with backtracking, the former improves the stability against drastic content change by elaborately controls feature updating scale after each dragging, while the latter alleviates the misguidance from similar points by actively restricting the search area in a line. These two technologies together contribute to a more stable semantic dragging with higher efficiency. Comprehensive experimental results substantiate that our approach significantly outperforms pre-existing methodologies, offering reliable point-based editing even in various complex scenarios. (@ling2023freedrag)

Haofeng Liu, Chenshu Xu, Yifei Yang, Lihua Zeng, and Shengfeng He Drag your noise: Interactive point-based editing via diffusion semantic propagation In *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2024. **Abstract:** Point-based interactive editing serves as an essential tool to complement the controllability of existing generative models. A concurrent work, DragDiffusion, updates the diffusion latent map in response to user inputs, causing global latent map alterations. This results in imprecise preservation of the original content and unsuccessful editing due to gradient vanishing. In contrast, we present DragNoise, offering robust and accelerated editing without retracing the latent map. The core rationale of DragNoise lies in utilizing the predicted noise output of each U-Net as a semantic editor. This approach is grounded in two critical observations: firstly, the bottleneck features of U-Net inherently possess semantically rich features ideal for interactive editing; secondly, high-level semantics, established early in the denoising process, show minimal variation in subsequent stages. Leveraging these insights, DragNoise edits diffusion semantics in a single denoising step and efficiently propagates these changes, ensuring stability and efficiency in diffusion editing. Comparative experiments reveal that DragNoise achieves superior control and semantic retention, reducing the optimization time by over 50% compared to DragDiffusion. Our codes are available at https://github.com/haofengl/DragNoise. (@liu2024drag)

Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao Latent consistency models: Synthesizing high-resolution images with few-step inference , 2023. **Abstract:** Latent Diffusion models (LDMs) have achieved remarkable results in synthesizing high-resolution images. However, the iterative sampling process is computationally intensive and leads to slow generation. Inspired by Consistency Models (song et al.), we propose Latent Consistency Models (LCMs), enabling swift inference with minimal steps on any pre-trained LDMs, including Stable Diffusion (rombach et al). Viewing the guided reverse diffusion process as solving an augmented probability flow ODE (PF-ODE), LCMs are designed to directly predict the solution of such ODE in latent space, mitigating the need for numerous iterations and allowing rapid, high-fidelity sampling. Efficiently distilled from pre-trained classifier-free guided diffusion models, a high-quality 768 x 768 2\~4-step LCM takes only 32 A100 GPU hours for training. Furthermore, we introduce Latent Consistency Fine-tuning (LCF), a novel method that is tailored for fine-tuning LCMs on customized image datasets. Evaluation on the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve state-of-the-art text-to-image generation performance with few-step inference. Project Page: https://latent-consistency-models.github.io/ (@luo2023lcm)

Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or Null-text inversion for editing real images using guided diffusion models In *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2023. **Abstract:** Recent large-scale text-guided diffusion models provide powerful image generation capabilities. Currently, a massive effort is given to enable the modification of these images using text only as means to offer intuitive and versatile editing tools. To edit a real image using these state-of-the-art tools, one must first invert the image with a meaningful text prompt into the pretrained model’s domain. In this paper, we introduce an accurate inversion technique and thus facilitate an intuitive text-based modification of the image. Our proposed inversion consists of two key novel components: (i) Pivotal inversion for diffusion models. While current methods aim at mapping random noise samples to a single input image, we use a single pivotal noise vector for each timestamp and optimize around it. We demonstrate that a direct DDIM inversion is inadequate on its own, but does provide a rather good anchor for our optimization. (ii) Null-text optimization, where we only modify the unconditional textual embedding that is used for classifier-free guidance, rather than the input text embedding. This allows for keeping both the model weights and the conditional embedding intact and hence enables applying prompt-based editing while avoiding the cumbersome tuning of the model’s weights. Our null-text inversion, based on the publicly available Stable Diffusion model, is extensively evaluated on a variety of images and various prompt editing, showing high-fidelity editing of real images. (@Mokady_2023_CVPR)

Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang : Boosting accuracy and flexibility on diffusion-based image editing In *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2024. **Abstract:** Large-scale Text-to-Image (T2I) diffusion models have revolutionized image generation over the last few years. Although owning diverse and high-quality generation capabilities, translating these abilities to fine-grained image editing remains challenging. In this paper, we propose DiffEditor to rectify two weaknesses in existing diffusion-based image editing: (1) in complex scenarios, editing results often lack editing accuracy and exhibit unexpected artifacts; (2) lack of flexibility to harmonize editing operations, e.g., imagine new content. In our solution, we introduce image prompts in fine-grained image editing, cooperating with the text prompt to better describe the editing content. To increase the flexibility while maintaining content consistency, we locally combine stochastic differential equation (SDE) into the ordinary differential equation (ODE) sampling. In addition, we incorporate regional score-based gradient guidance and a time travel strategy into the diffusion sampling, further improving the editing quality. Extensive experiments demonstrate that our method can efficiently achieve state-of-the-art performance on various fine-grained image editing tasks, including editing within a single image (e.g., object moving, resizing, and content dragging) and across images (e.g., appearance replacing and object pasting). Our source code is released at https://github.com/MC-E/DragonDiffusion. (@mou2024diffeditor)

Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang : Enabling drag-style manipulation on diffusion models In *Proc. Int. Conf. Learn. Represent. (ICLR)*, 2024. **Abstract:** Despite the ability of existing large-scale text-to-image (T2I) models to generate high-quality images from detailed textual descriptions, they often lack the ability to precisely edit the generated or real images. In this paper, we propose a novel image editing method, DragonDiffusion, enabling Drag-style manipulation on Diffusion models. Specifically, we construct classifier guidance based on the strong correspondence of intermediate features in the diffusion model. It can transform the editing signals into gradients via feature correspondence loss to modify the intermediate representation of the diffusion model. Based on this guidance strategy, we also build a multi-scale guidance to consider both semantic and geometric alignment. Moreover, a cross-branch self-attention is added to maintain the consistency between the original image and the editing result. Our method, through an efficient design, achieves various editing modes for the generated or real images, such as object moving, object resizing, object appearance replacement, and content dragging. It is worth noting that all editing and content preservation signals come from the image itself, and the model does not require fine-tuning or additional modules. Our source code will be available at https://github.com/MC-E/DragonDiffusion. (@mou2023dragondiffusion)

D. Naylor Theoretical elasticity, by A. E. Green and W. Zerna (Second edition). Clarendon Press, Oxford, 1968. xv + 457 pages. , 12:537–538, 1969. **Abstract:** An abstract is not available for this content so a preview has been provided. As you have access to this content, a full PDF is available via the ‘Save PDF’ action button. (@Naylor_1969)

Shen Nie, Hanzhong Allan Guo, Cheng Lu, Yuhao Zhou, Chenyu Zheng, and Chongxuan Li The blessing of randomness: SDE beats ODE in general diffusion-based image editing In *Proc. Int. Conf. Learn. Represent. (ICLR)*, 2024. **Abstract:** We present a unified probabilistic formulation for diffusion-based image editing, where a latent variable is edited in a task-specific manner and generally deviates from the corresponding marginal distribution induced by the original stochastic or ordinary differential equation (SDE or ODE). Instead, it defines a corresponding SDE or ODE for editing. In the formulation, we prove that the Kullback-Leibler divergence between the marginal distributions of the two SDEs gradually decreases while that for the ODEs remains as the time approaches zero, which shows the promise of SDE in image editing. Inspired by it, we provide the SDE counterparts for widely used ODE baselines in various tasks including inpainting and image-to-image translation, where SDE shows a consistent and substantial improvement. Moreover, we propose SDE-Drag – a simple yet effective method built upon the SDE formulation for point-based content dragging. We build a challenging benchmark (termed DragBench) with open-set natural, art, and AI-generated images for evaluation. A user study on DragBench indicates that SDE-Drag significantly outperforms our ODE baseline, existing diffusion-based methods, and the renowned DragGAN. Our results demonstrate the superiority and versatility of SDE in image editing and push the boundary of diffusion-based editing methods. (@nie2024the)

Xingang Pan, Ayush Tewari, Thomas Leimkühler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt Drag your GAN: Interactive point-based manipulation on the generative image manifold In *Proc. ACM Conf. SIGGRAPH*, 2023. **Abstract:** Synthesizing visual content that meets users’ needs often requires flexible and precise controllability of the pose, shape, expression, and layout of the generated objects. Existing approaches gain controllability of generative adversarial networks (GANs) via manually annotated training data or a prior 3D model, which often lack flexibility, precision, and generality. In this work, we study a powerful yet much less explored way of controlling GANs, that is, to "drag" any points of the image to precisely reach target points in a user-interactive manner, as shown in Fig.1. To achieve this, we propose DragGAN, which consists of two main components: 1) a feature-based motion supervision that drives the handle point to move towards the target position, and 2) a new point tracking approach that leverages the discriminative generator features to keep localizing the position of the handle points. Through DragGAN, anyone can deform an image with precise control over where pixels go, thus manipulating the pose, shape, expression, and layout of diverse categories such as animals, cars, humans, landscapes, etc. As these manipulations are performed on the learned generative image manifold of a GAN, they tend to produce realistic outputs even for challenging scenarios such as hallucinating occluded content and deforming shapes that consistently follow the object’s rigidity. Both qualitative and quantitative comparisons demonstrate the advantage of DragGAN over prior approaches in the tasks of image manipulation and point tracking. We also showcase the manipulation of real images through GAN inversion. (@pan2023drag)

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever Learning transferable visual models from natural language supervision In *Proc. Int. Conf. Machine Learning (ICML)*, 2021. **Abstract:** State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP. (@pmlrv139radford21a)

Herbert E. Robbins A stochastic approximation method , 22:400–407, 1951. **Abstract:** Let $M(x)$ denote the expected value at level $x$ of the response to a certain experiment. $M(x)$ is assumed to be a monotone function of $x$ but is unknown to the experimenter, and it is desired to find the solution $x = \\}theta$ of the equation $M(x) = \\}alpha$, where $\\}alpha$ is a given constant. We give a method for making successive experiments at levels $x_1,x_2,\\}cdots$ in such a way that $x_n$ will tend to $\\}theta$ in probability. (@Robbins1951ASA)

Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer High-resolution image synthesis with latent diffusion models In *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2021. **Abstract:** By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. (@Rombach2021HighResolutionIS)

Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer High-resolution image synthesis with latent diffusion models In *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2022. **Abstract:** By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. (@Rombach_2022_CVPR)

Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vincent YF Tan, and Song Bai : Harnessing diffusion models for interactive point-based image editing In *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2024. **Abstract:** Accurate and controllable image editing is a challenging task that has attracted significant attention recently. Notably, DragGAN is an interactive point-based image editing framework that achieves impressive editing results with pixel-level precision. However, due to its reliance on generative adversarial networks (GANs), its generality is limited by the capacity of pretrained GAN models. In this work, we extend this editing framework to diffusion models and propose a novel approach DragDiffusion. By harnessing large-scale pretrained diffusion models, we greatly enhance the applicability of interactive point-based editing on both real and diffusion-generated images. Our approach involves optimizing the diffusion latents to achieve precise spatial control. The supervision signal of this optimization process is from the diffusion model’s UNet features, which are known to contain rich semantic and geometric information. Moreover, we introduce two additional techniques, namely LoRA fine-tuning and latent-MasaCtrl, to further preserve the identity of the original image. Lastly, we present a challenging benchmark dataset called DragBench – the first benchmark to evaluate the performance of interactive point-based image editing methods. Experiments across a wide range of challenging cases (e.g., images with multiple objects, diverse object categories, various styles, etc.) demonstrate the versatility and generality of DragDiffusion. Code: https://github.com/Yujun-Shi/DragDiffusion. (@shi2023dragdiffusion)

Jiaming Song, Chenlin Meng, and Stefano Ermon Denoising diffusion implicit models In *Proc. Int. Conf. Learn. Represent. (ICLR)*, 2021. **Abstract:** Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples $10 \\}times$ to $50 \\}times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space. (@song2021denoising)

Sihan Xu, Yidong Huang, Jiayi Pan, Ziqiao Ma, and Joyce Chai Inversion-free image editing with natural language In *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2024. **Abstract:** Despite recent advances in inversion-based editing, text-guided image manipulation remains challenging for diffusion models. The primary bottlenecks include 1) the time-consuming nature of the inversion process; 2) the struggle to balance consistency with accuracy; 3) the lack of compatibility with efficient consistency sampling methods used in consistency models. To address the above issues, we start by asking ourselves if the inversion process can be eliminated for editing. We show that when the initial sample is known, a special variance schedule reduces the denoising step to the same form as the multi-step consistency sampling. We name this Denoising Diffusion Consistent Model (DDCM), and note that it implies a virtual inversion strategy without explicit inversion in sampling. We further unify the attention control mechanisms in a tuning-free framework for text-guided editing. Combining them, we present inversion-free editing (InfEdit), which allows for consistent and faithful editing for both rigid and non-rigid semantic changes, catering to intricate modifications without compromising on the image’s integrity and explicit inversion. Through extensive experiments, InfEdit shows strong performance in various editing tasks and also maintains a seamless workflow (less than 3 seconds on one single A40), demonstrating the potential for real-time applications. Project Page: https://sled-group.github.io/InfEdit/ (@xu2023infedit)

Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang The unreasonable effectiveness of deep features as a perceptual metric In *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2018. **Abstract:** While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations. (@Zhang_2018_CVPR)

Zewei Zhang, Huan Liu, Jun Chen, and Xiangyu Xu : Towards good practices for drag editing with diffusion models , 2024. **Abstract:** In this paper, we introduce GoodDrag, a novel approach to improve the stability and image quality of drag editing. Unlike existing methods that struggle with accumulated perturbations and often result in distortions, GoodDrag introduces an AlDD framework that alternates between drag and denoising operations within the diffusion process, effectively improving the fidelity of the result. We also propose an information-preserving motion supervision operation that maintains the original features of the starting point for precise manipulation and artifact reduction. In addition, we contribute to the benchmarking of drag editing by introducing a new dataset, Drag100, and developing dedicated quality assessment metrics, Dragging Accuracy Index and Gemini Score, utilizing Large Multimodal Models. Extensive experiments demonstrate that the proposed GoodDrag compares favorably against the state-of-the-art approaches both qualitatively and quantitatively. The project page is https://gooddrag.github.io. (@zhang2024gooddrag)

</div>

# Supplementary Experiments [sec:Supplementary experiment]

We conduct supplementary quantitative experiments on BNNI and consistency-preserving strategies to further validate their effectiveness. The quantitative metrics used are consistent with those described in Sec. <a href="#Qualitative" data-reference-type="ref" data-reference="Qualitative">4.1</a>.

**BNNI:** Following the setup in Sec.<a href="#Ablation" data-reference-type="ref" data-reference="Ablation">4.3</a>, we compare it with several interpolation methods on null point \\(\mathcal{N}\\), including maintaining the original value of this position, interpolation by zero-value, and interpolation by random noise, denoted as “origin”, “0-inter”, and “random-inter”, respectively. As illustrated in <a href="#ablution_BNNI_chart" data-reference-type="ref+Label" data-reference="ablution_BNNI_chart">[ablution_BNNI_chart]</a>, FastDrag with BNNI achieves the best MD levels compared to other interpolation methods, while its IF is second only to “origin”. However, “origin” can lead to negative drag effects, as shown in <a href="#ablution_BNNI" data-reference-type="ref+Label" data-reference="ablution_BNNI">7</a>. Therefore, by effectively utilizing surrounding feature values to interpolate null points, BNNI can address semantic losses and enhance the quality of drag editing.

**Consistency-Preserving:** We also conduct experiments to assess the impact of initiating the consistency-preserving strategy at different sampling steps. The results, as shown in <a href="#ablution_kv_step_chart" data-reference-type="ref+Label" data-reference="ablution_kv_step_chart">11</a>, indicate that as the starting step increases ( *i.e.*, the frequency of key and value replacements decreases), the IF decreases, leading to poorer image consistency. Meanwhile, the MD initially decreases and then increases as the starting step increases. It is evident that consistency-preserving strategy can effectively maintain the consistency between the generated images and the original images.

<figure id="ablution_kv_step_chart">
<img src="./figures/ablution_BNNI_chart.png"" style="width:100.0%" />
<img src="./figures/ablution_kv_step_chart.png"" style="width:100.0%" />
<figcaption>Ablation study on consistency-preserving in terms of quantitative metrics.</figcaption>
</figure>

<figure id="single diffusion">
<div class="center">
<img src="./figures/Drag-with-different-IStep.png"" />
</div>
<figcaption>Overall editing time comparison with different diffusion steps between FastDrag and DragDiffusion. All experiments are conducted on RTX 3090 with diffusion step set as 1, 20, and 50 respectively. Optimization means latent optimization. </figcaption>
</figure>

# Single-step Diffusion

When integrating DragDiffusion with a single-step diffusion model, the editing time is still much longer than that of FastDrag. For DragDiffusion and FastDrag under diffusion steps of 1, 20, and 50, we calculate the time required for inversion, sampling, and latent optimization respectively. The results provided in <a href="#single diffusion" data-reference-type="ref+Label" data-reference="single diffusion">12</a> show that even with a single diffusion step (i.e., diffusion step set as 1), DragDiffusion still requires significantly more time (20.7 seconds) compared to FastDrag (2.88 seconds).

In addition, as observed in <a href="#single diffusion" data-reference-type="ref+Label" data-reference="single diffusion">12</a>, DragDiffusion spends significantly more time on latent optimization compared to diffusion inversion and sampling. Therefore, reducing the time spent on latent optimization is crucial for minimizing overall editing time, which is precisely what FastDrag accomplishes.

# Statistical Rigor [Statistical Rigor]

To further validate the superiority of our work and to achieve statistical rigor, we conducted an additional experiment by repeating our experiment 10 times under the same experimental settings. We observed that the variances of the performance metrics obtained from 10 realizations of our FastDrag are MD (0.000404), 1-LPIPS (9.44E-11), and Time (0.018), all of which fall within a reasonable range. These statistical results further demonstrate the effectiveness and stability of our method for drag editing.

# Implementation Details [sec:4.1]

We utilize a pretrained LDM (*i.e.*, Stable Diffusion 1.5 `\cite{Rombach_2022_CVPR}`{=latex}) as our diffusion model, where the U-Net structure is adapted with LCM-distilled weights from Stable Diffusion 1.5. It is worth emphasizing that the U-Net structure used in our model is widely used in image generation methods `\cite{shi2023dragdiffusion, ling2023freedrag, liu2024drag, zhang2024gooddrag, Cui2024StableDragSD}`{=latex}. Unless otherwise specified, the default setting for inversion and sampling step is 10. Following DragDiffusion `\cite{shi2023dragdiffusion}`{=latex}, classifier-free guidance (CFG) `\cite{ho2021classifierfree}`{=latex} is not applied in diffusion model, and we optimize the diffusion latent at the 7th step. All other configurations follow that used in DragDiffusion. Our experiments are conducted on an RTX 3090 GPU with 24G memory.

For the special application of drag-based editing, *i.e.*, object moving, as shown in <a href="#ablution_inversion" data-reference-type="ref+Label" data-reference="ablution_inversion">6</a>, significant null region may be left at the original position of the object due to long-distance relocation, posing challenges for BNNI. To ensure semantic integrity in the image and facilitate user interaction, we adopted two straightforward strategies. For first strategy, specifically, we introduce a parameter \\(r\\), set as 2, centered at the unique target point \\(e_1\\), defining a rectangular area with dimensions \\(2r\\). We then extract the noised-latent representation within this rectangular area and fill it into the mask, effectively restoring the semantics at the original position of the object. For second strategy, we just maintain the original semantic of original position to avoid null region. Under second strategy, object moving will produce the effect of object replication, as shown in the third row of <a href="#More Results" data-reference-type="ref+Label" data-reference="More Results">13</a>.

# More Results [sec:More Results]

We apply FastDrag to drag-based image editing in various scenarios, including face rotation, object movement, object stretching, object shrinking and so on. The experimental results, as shown in <a href="#More Results" data-reference-type="ref+Label" data-reference="More Results">13</a>, demonstrate that FastDrag achieves excellent drag-based effects across multiple scenarios.

<figure id="More Results">
<img src="./figures/more_results.png"" />
<figcaption>More visualized results of FastDrag.</figcaption>
</figure>

# Societal Impacts [Societal Impacts]

FastDrag has the potential to bring about several positive societal impacts. Firstly, they offer intuitive and efficient image editing tools, catering to artists, designers, and creators, thereby fostering creativity and innovation. Secondly, their user-friendly nature simplifies the image editing process, increasing accessibility and participation among a wider audience. In addition, our FastDrag improves efficiency and saves the user’s time and effort, thus increasing productivity. Moreover, the innovative and flexible nature of these methods opens up possibilities for various applications, spanning art creation, design, education, and training.

However, along with these positive aspects, FastDrag can also have certain negative societal implications. They may be exploited by unethical individuals or organizations to propagate misinformation and fake imagery, potentially contributing to the spread of false news and undermining societal trust. Furthermore, widespread use of image editing tools may encroach upon individual privacy rights, particularly when unauthorized information or imagery is manipulated. Moreover, inappropriate or irresponsible image editing practices could lead to social injustices and imbalances, such as distorting facts or misleading the public, thereby influencing public opinion and policies negatively.

[^1]: Corresponding author: j.guan@hrbeu.edu.cn
